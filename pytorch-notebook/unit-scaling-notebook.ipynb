{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unit Scaling: A How-To Guide\n",
    "\n",
    "In our paper [Unit Scaling: Out-of-the-Box Low-Precision Training](), we describe a scheme for designing neural networks that have approximate unit variance after every operation.\n",
    "\n",
    "This can be seen as an alternative to (static) loss scaling, or its automatic variant, as used in Automatic Mixed Precision. Whereas both of those schemes rely on a single, global scaling factor for all the gradients, unit scaling is more fine-grained.\n",
    "\n",
    "A unit-scaled model adds scaling factors (constant scalar multiplications) to each operation in the computational graph to achieve this unit variance property. The result is a model which naturally produces tensors in the middle of the dynamic range provided by floating-point formats. There's no extra loss-scale hyperparameter—it just works out-of-the-box!\n",
    "\n",
    "## Implementing unit scaling\n",
    "\n",
    "Here we demonstrate how to go about unit scaling in practice. This involves re-implementing common neural network layers to add variance-preserving scaling factors.\n",
    "\n",
    "As explained in the paper, we can sometimes justify using different scaling factors in the forward and backward pass. We introduce a special `scaled()` op which allows us to do just that.\n",
    "\n",
    "Below we implement two models. The first is a simple implementation of a transformer-decoder. The design and hyperparameters are inspired (though some differ) by Andrej Karpathy's [popular NanoGPT implementation](https://github.com/karpathy/nanoGPT). It's also 🤗-compatible!\n",
    "\n",
    "The second is the same, but unit-scaled. Let's get stuck in...\n",
    "\n",
    "## Building a unit-scaled NanoGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/charlieb/Projects/graphcore/unit-scaling-notebook/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from typing import Dict, Optional\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, Tensor\n",
    "from transformers.activations import GELUActivation\n",
    "from transformers.configuration_utils import PretrainedConfig\n",
    "from transformers.modeling_outputs import CausalLMOutputWithCrossAttentions\n",
    "from transformers.modeling_utils import PreTrainedModel"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The MLP layer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by setting up a basic config for a reasonably small transformer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NanoGPTConfig(PretrainedConfig):\n",
    "    model_type = \"nano-gpt\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_size: int = 384,\n",
    "        num_hidden_layers: int = 6,\n",
    "        num_attention_heads: int = 6,\n",
    "        dropout: float = 0.1,\n",
    "        vocab_size: int = 384,\n",
    "        eos_token_id: int = 1,\n",
    "        **kwargs,\n",
    "    ) -> None:\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "        self.dropout = dropout\n",
    "        self.vocab_size = vocab_size\n",
    "        self.eos_token_id = eos_token_id\n",
    "        super().__init__(**kwargs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Along with a standard (pre-norm) MLP module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, config: NanoGPTConfig) -> None:\n",
    "        super().__init__()\n",
    "        self.ln = nn.LayerNorm(config.hidden_size)\n",
    "        self.linear_1 = nn.Linear(config.hidden_size, config.hidden_size * 4)\n",
    "        self.act = GELUActivation()\n",
    "        self.linear_2 = nn.Linear(config.hidden_size * 4, config.hidden_size)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, hidden_states: Tensor) -> Tensor:\n",
    "        hidden_states = self.ln(hidden_states)\n",
    "        hidden_states = self.linear_1(hidden_states)\n",
    "        hidden_states = self.act(hidden_states)\n",
    "        hidden_states = self.linear_2(hidden_states)\n",
    "        return self.dropout(hidden_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: hide\n",
    "def init_weights(init_fn) -> None:\n",
    "    def inner_fn(module: nn.Module) -> None:\n",
    "        if isinstance(module, nn.Linear):\n",
    "            fan_out, fan_in = module.weight.shape  # TODO: correct base impl\n",
    "            module.weight.data.normal_(mean=0.0, std=init_fn(fan_in, fan_out))\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            embed_dim = module.weight.shape[-1]\n",
    "            module.weight.data.normal_(mean=0.0, std=init_fn(embed_dim, embed_dim))\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "    return inner_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: hide\n",
    "from seaborn import heatmap\n",
    "import pandas as pd\n",
    "\n",
    "np.seterr(divide = 'ignore')\n",
    "hist_bins = np.array([e for e in range(-14, 16+1)])\n",
    "\n",
    "def instrument(module):\n",
    "    stats = {}\n",
    "    instrument_recursive(module, stats)\n",
    "    return stats\n",
    "\n",
    "def instrument_recursive(module, stats, name=''):\n",
    "    children = list(module.named_children())\n",
    "    if children:\n",
    "        for c_name, c in children:\n",
    "            _name = f'{name}.{c_name}' if name and name != 'blocks' else c_name\n",
    "            instrument_recursive(c, stats, _name)\n",
    "    else:\n",
    "        instrument_terminal(module, stats, name)\n",
    "\n",
    "def instrument_terminal(module, stats, name):\n",
    "    module_stats = {}\n",
    "    def require_input_grads(_module, input):\n",
    "        for i in input:\n",
    "            if isinstance(i, Tensor) and i.is_floating_point():\n",
    "                i.requires_grad_()\n",
    "    \n",
    "    module.register_forward_pre_hook(require_input_grads)\n",
    "    \n",
    "    if name.split('.')[-1] == 'softmax':\n",
    "        return\n",
    "    \n",
    "    def record_fwd_scale(_module, input, output):\n",
    "        if isinstance(output, Tensor) and output.is_floating_point():\n",
    "            module_stats['x'] = np.log2(output.std().item())\n",
    "    \n",
    "    module.register_forward_hook(record_fwd_scale)\n",
    "    \n",
    "    def record_bwd_scales(_module, grad_input, grad_output):\n",
    "        grad_input = list(grad_input)\n",
    "        for g in grad_input:\n",
    "            if g is not None and isinstance(g, Tensor) \\\n",
    "                and g.is_floating_point() and len(grad_input) == 1:\n",
    "                module_stats['grad_x'] = np.log2(g.std().item())\n",
    "        \n",
    "        for param_name, param in _module.named_parameters():\n",
    "            if param_name == \"weight\":\n",
    "                module_stats['w'] = np.log2(param.std().item())\n",
    "                if param.grad is not None:\n",
    "                    module_stats['grad_w'] = np.log2(param.grad.std().item())\n",
    "    \n",
    "    module.register_full_backward_hook(record_bwd_scales)\n",
    "    \n",
    "    stats[name] = module_stats\n",
    "\n",
    "def create_histogram(t: Tensor) -> np.ndarray:\n",
    "    return np.histogram(np.log(abs(t.detach())), hist_bins, density=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: hide\n",
    "import altair as alt\n",
    "\n",
    "def visualise(stats):\n",
    "    df = pd.DataFrame(stats)\n",
    "    df = df.stack().to_frame('scale (log₂)').reset_index(names=['type', 'op'])\n",
    "    plot(df)\n",
    "\n",
    "def plot(df):\n",
    "    is_x_or_grad_x = (df['type'] == 'x') | (df['type'] == 'grad_x')\n",
    "    op_order = df[df['type'] == 'x']['op'].tolist()\n",
    "    colors = ['#6C8EBF', '#FF8000', '#5D8944', '#ED3434']\n",
    "    \n",
    "    a = alt.Chart(df[is_x_or_grad_x]).mark_line().encode(\n",
    "        x=alt.X(\n",
    "            'scale (log₂):Q',\n",
    "            axis=alt.Axis(orient='top', values=np.arange(-14, 16+1, 2)),\n",
    "            scale=alt.Scale(domain=[-14, 16]),\n",
    "        ),\n",
    "        y=alt.Y(\n",
    "            'op:O',\n",
    "            title='',\n",
    "            sort=op_order,\n",
    "        ),\n",
    "        color=alt.Color(\n",
    "            'type',\n",
    "            legend=alt.Legend(title='', labelFontSize=12, symbolSize=100),\n",
    "            scale=alt.Scale(range=colors[:2]),\n",
    "            sort='descending'\n",
    "        ),\n",
    "    )\n",
    "    b = alt.Chart(df[~is_x_or_grad_x]).mark_point(size=100).encode(\n",
    "        x=alt.X(\n",
    "            'scale (log₂):Q',\n",
    "            axis=alt.Axis(orient='top', values=np.arange(-14, 16+1, 2)),\n",
    "            scale=alt.Scale(domain=[-14, 16])\n",
    "        ),\n",
    "        y=alt.Y(\n",
    "            'op:O',\n",
    "            title='',\n",
    "            sort=op_order,\n",
    "        ),\n",
    "        color=alt.Color(\n",
    "            'type',\n",
    "            legend=alt.Legend(title='', labelFontSize=12, symbolSize=100),\n",
    "            scale=alt.Scale(range=colors[2:]),\n",
    "            sort='descending'\n",
    "        ),\n",
    "        shape=alt.Shape(\n",
    "            'type',\n",
    "            scale=alt.Scale(range=['square', 'triangle-down']),\n",
    "            sort='descending'\n",
    "        ),\n",
    "    )\n",
    "    display(alt.layer(a, b).resolve_scale(color='independent', shape='independent').configure_axis(\n",
    "        labelFontSize=12,\n",
    "        titleFontSize=16\n",
    "    ).properties(\n",
    "        width=500\n",
    "    ))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can analyse the scale (i.e. standard deviation) of each operation within the MLP. To do this, we provide an `instrument()` operation, which goes through and tracks the scale coming out of each operation in the forward and backward pass.\n",
    "\n",
    "Our network's weights will also use the standard glorot initialisation.\n",
    "\n",
    "So let's feed in a unit normal tensor in both directions, and examine the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_mlp(mlp, config, batch_size=64, seq_len=16):\n",
    "    stats = instrument(mlp)\n",
    "    x = torch.normal(0.0, 1.0, size=(batch_size, seq_len, config.hidden_size))\n",
    "    y = mlp(x)\n",
    "    y.backward(torch.normal(0.0, 1.0, size=y.shape))\n",
    "    visualise(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-8d6e395874fd4739937b9f244f90304a\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-8d6e395874fd4739937b9f244f90304a\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-8d6e395874fd4739937b9f244f90304a\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.17.0?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"4.17.0\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}, \"axis\": {\"labelFontSize\": 12, \"titleFontSize\": 16}}, \"layer\": [{\"data\": {\"name\": \"data-4f76b1258b65f61118e377cd50b7f971\"}, \"mark\": \"line\", \"encoding\": {\"color\": {\"field\": \"type\", \"legend\": {\"labelFontSize\": 12, \"symbolSize\": 100, \"title\": \"\"}, \"scale\": {\"range\": [\"#6C8EBF\", \"#FF8000\"]}, \"sort\": \"descending\", \"type\": \"nominal\"}, \"x\": {\"axis\": {\"orient\": \"top\", \"values\": [-14.0, -12.0, -10.0, -8.0, -6.0, -4.0, -2.0, 0.0, 2.0, 4.0, 6.0, 8.0, 10.0, 12.0, 14.0, 16.0]}, \"field\": \"scale (log\\u2082)\", \"scale\": {\"domain\": [-14, 16]}, \"type\": \"quantitative\"}, \"y\": {\"field\": \"op\", \"sort\": [\"ln\", \"linear_1\", \"act\", \"linear_2\", \"dropout\"], \"title\": \"\", \"type\": \"ordinal\"}}}, {\"data\": {\"name\": \"data-0a72fe8f7155811574754ec3cdf5a29f\"}, \"mark\": {\"type\": \"point\", \"size\": 100}, \"encoding\": {\"color\": {\"field\": \"type\", \"legend\": {\"labelFontSize\": 12, \"symbolSize\": 100, \"title\": \"\"}, \"scale\": {\"range\": [\"#5D8944\", \"#ED3434\"]}, \"sort\": \"descending\", \"type\": \"nominal\"}, \"shape\": {\"field\": \"type\", \"scale\": {\"range\": [\"square\", \"triangle-down\"]}, \"sort\": \"descending\", \"type\": \"nominal\"}, \"x\": {\"axis\": {\"orient\": \"top\", \"values\": [-14.0, -12.0, -10.0, -8.0, -6.0, -4.0, -2.0, 0.0, 2.0, 4.0, 6.0, 8.0, 10.0, 12.0, 14.0, 16.0]}, \"field\": \"scale (log\\u2082)\", \"scale\": {\"domain\": [-14, 16]}, \"type\": \"quantitative\"}, \"y\": {\"field\": \"op\", \"sort\": [\"ln\", \"linear_1\", \"act\", \"linear_2\", \"dropout\"], \"title\": \"\", \"type\": \"ordinal\"}}}], \"resolve\": {\"scale\": {\"color\": \"independent\", \"shape\": \"independent\"}}, \"width\": 500, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.17.0.json\", \"datasets\": {\"data-4f76b1258b65f61118e377cd50b7f971\": [{\"type\": \"x\", \"op\": \"ln\", \"scale (log\\u2082)\": -5.41746367372377e-06}, {\"type\": \"x\", \"op\": \"linear_1\", \"scale (log\\u2082)\": -0.6629883523901317}, {\"type\": \"x\", \"op\": \"act\", \"scale (log\\u2082)\": -1.487519326194847}, {\"type\": \"x\", \"op\": \"linear_2\", \"scale (log\\u2082)\": -1.058964576311004}, {\"type\": \"x\", \"op\": \"dropout\", \"scale (log\\u2082)\": -0.9822708273609165}, {\"type\": \"grad_x\", \"op\": \"ln\", \"scale (log\\u2082)\": -0.9203459519405033}, {\"type\": \"grad_x\", \"op\": \"linear_1\", \"scale (log\\u2082)\": -0.9220805489583831}, {\"type\": \"grad_x\", \"op\": \"act\", \"scale (log\\u2082)\": -1.2661013968855288}, {\"type\": \"grad_x\", \"op\": \"linear_2\", \"scale (log\\u2082)\": -0.5863197811280207}, {\"type\": \"grad_x\", \"op\": \"dropout\", \"scale (log\\u2082)\": 0.07434761788879041}], \"data-0a72fe8f7155811574754ec3cdf5a29f\": [{\"type\": \"w\", \"op\": \"ln\", \"scale (log\\u2082)\": null}, {\"type\": \"w\", \"op\": \"linear_1\", \"scale (log\\u2082)\": -4.954646432323783}, {\"type\": \"w\", \"op\": \"linear_2\", \"scale (log\\u2082)\": -4.9539206431617275}, {\"type\": \"grad_w\", \"op\": \"ln\", \"scale (log\\u2082)\": 4.050015179983157}, {\"type\": \"grad_w\", \"op\": \"linear_1\", \"scale (log\\u2082)\": 3.7315460917642063}, {\"type\": \"grad_w\", \"op\": \"linear_2\", \"scale (log\\u2082)\": 3.6758629465334316}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.LayerChart(...)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def glorot_init(fan_in, fan_out):\n",
    "    return ((fan_in + fan_out) / 2) ** -0.5\n",
    "\n",
    "config = NanoGPTConfig()\n",
    "mlp = MLP(config).apply(init_weights(glorot_init))\n",
    "analyse_mlp(mlp, config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the end of both passes the scale has dropped by 0.5. This is due to glorot under-scaling slightly and the GeLU dropping the scale further.\n",
    "\n",
    "Even worse, the grad_w scales are around 0.03. This is largely because glorot scaling (along with all other weight init schemes) only accounts for the forward and grad_x scales.\n",
    "\n",
    "We'll now implement the equivalent layer using unit scaling, beginning with a basic linear operation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: hide\n",
    "class ScaledGrad(torch.autograd.Function):\n",
    "  @staticmethod\n",
    "  def forward(ctx, X, alpha, beta):\n",
    "    ctx.save_for_backward(\n",
    "      torch.tensor(beta, dtype=X.dtype))\n",
    "    return alpha * X\n",
    "\n",
    "  @staticmethod\n",
    "  def backward(ctx, grad_Y):\n",
    "    beta, = ctx.saved_tensors\n",
    "    return beta * grad_Y, None, None\n",
    "\n",
    "def scaled(X, alpha=1.0, beta=1.0):\n",
    "  # Forward: Y = X * alpha\n",
    "  # Backward: grad_X = grad_Y * beta\n",
    "  return ScaledGrad.apply(X, alpha, beta)\n",
    "\n",
    "def geometric_mean(xs):\n",
    "    xs = np.array(xs)\n",
    "    return xs.prod() ** (1 / xs.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnitScaledLinear(nn.Linear):\n",
    "    def __init__(self, *args, scale_for=\"fwd, grad_x\", **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.scale_for = scale_for\n",
    "    \n",
    "    def get_scales(self, input):\n",
    "        fwd_scale = self.weight.shape[1] ** -0.5\n",
    "        grad_x_scale = self.weight.shape[0] ** -0.5\n",
    "        grad_w_scale = np.prod(input.shape[:-1]) ** -0.5\n",
    "        if self.scale_for == \"fwd\":\n",
    "            grad_x_scale = fwd_scale\n",
    "        elif self.scale_for == \"grad_x\":\n",
    "            fwd_scale = grad_x_scale\n",
    "        elif self.scale_for == \"fwd, grad_x\":\n",
    "            fwd_scale = grad_x_scale = geometric_mean([fwd_scale, grad_x_scale])\n",
    "        else:\n",
    "            assert False, f\"demo implementation has no {self.scale_for} scaling\"\n",
    "        return fwd_scale, grad_x_scale, grad_w_scale\n",
    "           \n",
    "    def forward(self, input):\n",
    "        fwd_scale, grad_x_scale, grad_w_scale = self.get_scales(input)\n",
    "        input = scaled(input, beta=grad_x_scale)\n",
    "        weight = scaled(self.weight, beta=grad_w_scale)\n",
    "        bias = scaled(self.bias, beta=grad_w_scale) if self.bias is not None else None\n",
    "        output = F.linear(input, weight, bias)\n",
    "        return scaled(output, alpha=fwd_scale)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's break this down a bit. The `forward()` method is still based on the fundamental `F.linear(input, weight, bias)` operation, but has each of its operands and its output scaled.\n",
    "\n",
    "This is done via the `scaled(X, alpha, beta)` method. This is a special operation in that it has different dynamics in the forward and backward pass, where we have\n",
    "\n",
    "**Forward:** Y = X * alpha\n",
    "\n",
    "**Backward:** grad_X = grad_Y * beta\n",
    "\n",
    "So what scaling factors do we choose? This is determined in `get_scales()`. The standard approach is to set the forward and grad_x scales as a compromise between their \"ideal\" scale-preserving values (via a geometric mean). See our paper for more details on how we arrive at these ideal values.\n",
    "\n",
    "We also provide version of the operation which select forward and grad_x scales based on only one of their ideal values.\n",
    "\n",
    "Note that (for the sake of valid gradients) we're obliged to always have `fwd_scale = grad_x_scale`. However, we're allowed to have a separate scale for grad_w (again, see the paper) which gets its own ideal value."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the GeLU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnitScaledGELU(GELUActivation):\n",
    "    def forward(self, input):\n",
    "        fwd_scale = bwd_scale = geometric_mean([0.588, 0.675]) ** -1\n",
    "        input = scaled(input, beta=bwd_scale)\n",
    "        output = self.act(input)\n",
    "        return scaled(output, alpha=fwd_scale)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks pretty similar to the linear op. As activation functions are nonlinear, our usual approach of calculating scaling values analytically (i.e. by working through the maths) isn't always possible.\n",
    "\n",
    "Fortunately, for these elemenwise ops we can calulate them empirically, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GeLU: fwd scale=0.588, bwd scale=0.676\n",
      "Tanh: fwd scale=0.628, bwd scale=0.681\n"
     ]
    }
   ],
   "source": [
    "def analyse_elemenwise_fn(fn, num_samples=2**22):\n",
    "    x = torch.normal(0.0, 1.0, size=(num_samples,)).requires_grad_()\n",
    "    y = fn(x)\n",
    "    y.backward(torch.normal(0.0, 1.0, size=(num_samples,)))\n",
    "    print(f\"fwd scale={y.std():.3f}, bwd scale={x.grad.std():.3f}\")\n",
    "\n",
    "print(\"GeLU:\", end=\" \")\n",
    "analyse_elemenwise_fn(GELUActivation())\n",
    "print(\"Tanh:\", end=\" \")\n",
    "analyse_elemenwise_fn(torch.nn.Tanh())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple! Now we just need to unit scale the layernorm and dropout and we're ready.\n",
    "\n",
    "These take a similar approach, so we won't go into too much detail here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnitScaledLayerNorm(nn.LayerNorm):\n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        scale = (np.prod(self.normalized_shape) / input.nelement()) ** 0.5\n",
    "        weight = scaled(self.weight, beta=scale)\n",
    "        bias = scaled(self.bias, beta=scale)\n",
    "        return F.layer_norm(input, self.normalized_shape, weight, bias, self.eps)\n",
    "\n",
    "class UnitScaledDropout(nn.Dropout):\n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        # Dropout is typically implemented with a (1-p) ** -1 scaling\n",
    "        # However, to preserve variance this ought to be (1-p) ** -0.5\n",
    "        # We correct for this by multiplying by (1-p) ** 0.5\n",
    "        scale = (1-self.p) ** 0.5\n",
    "        input = scaled(input, beta=scale)\n",
    "        output = F.dropout(input, self.p, self.training, self.inplace)\n",
    "        return scaled(output, alpha=scale)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're now ready to unit-scale the full MLP 🥳\n",
    "\n",
    "All this requires is swapping out the old layers for our new ones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnitScaledMLP(MLP):\n",
    "    def __init__(self, config: NanoGPTConfig) -> None:\n",
    "        super().__init__(config)\n",
    "        self.ln = UnitScaledLayerNorm(config.hidden_size)\n",
    "        self.linear_1 = UnitScaledLinear(config.hidden_size, config.hidden_size * 4)\n",
    "        self.act = UnitScaledGELU()\n",
    "        self.linear_2 = UnitScaledLinear(config.hidden_size * 4, config.hidden_size)\n",
    "        self.dropout = UnitScaledDropout(config.dropout)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also change our initialisation to give our weights unit scale. Let's analyse our new unit-scaled MLP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-be2668cc17604ac0a8998ad6c37722ad\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-be2668cc17604ac0a8998ad6c37722ad\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-be2668cc17604ac0a8998ad6c37722ad\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.17.0?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"4.17.0\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}, \"axis\": {\"labelFontSize\": 12, \"titleFontSize\": 16}}, \"layer\": [{\"data\": {\"name\": \"data-bc3067c093e3bf6612ac5afcaab2bb82\"}, \"mark\": \"line\", \"encoding\": {\"color\": {\"field\": \"type\", \"legend\": {\"labelFontSize\": 12, \"symbolSize\": 100, \"title\": \"\"}, \"scale\": {\"range\": [\"#6C8EBF\", \"#FF8000\"]}, \"sort\": \"descending\", \"type\": \"nominal\"}, \"x\": {\"axis\": {\"orient\": \"top\", \"values\": [-14.0, -12.0, -10.0, -8.0, -6.0, -4.0, -2.0, 0.0, 2.0, 4.0, 6.0, 8.0, 10.0, 12.0, 14.0, 16.0]}, \"field\": \"scale (log\\u2082)\", \"scale\": {\"domain\": [-14, 16]}, \"type\": \"quantitative\"}, \"y\": {\"field\": \"op\", \"sort\": [\"ln\", \"linear_1\", \"act\", \"linear_2\", \"dropout\"], \"title\": \"\", \"type\": \"ordinal\"}}}, {\"data\": {\"name\": \"data-994762bd091869d8ac358d2908a5e1ec\"}, \"mark\": {\"type\": \"point\", \"size\": 100}, \"encoding\": {\"color\": {\"field\": \"type\", \"legend\": {\"labelFontSize\": 12, \"symbolSize\": 100, \"title\": \"\"}, \"scale\": {\"range\": [\"#5D8944\", \"#ED3434\"]}, \"sort\": \"descending\", \"type\": \"nominal\"}, \"shape\": {\"field\": \"type\", \"scale\": {\"range\": [\"square\", \"triangle-down\"]}, \"sort\": \"descending\", \"type\": \"nominal\"}, \"x\": {\"axis\": {\"orient\": \"top\", \"values\": [-14.0, -12.0, -10.0, -8.0, -6.0, -4.0, -2.0, 0.0, 2.0, 4.0, 6.0, 8.0, 10.0, 12.0, 14.0, 16.0]}, \"field\": \"scale (log\\u2082)\", \"scale\": {\"domain\": [-14, 16]}, \"type\": \"quantitative\"}, \"y\": {\"field\": \"op\", \"sort\": [\"ln\", \"linear_1\", \"act\", \"linear_2\", \"dropout\"], \"title\": \"\", \"type\": \"ordinal\"}}}], \"resolve\": {\"scale\": {\"color\": \"independent\", \"shape\": \"independent\"}}, \"width\": 500, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.17.0.json\", \"datasets\": {\"data-bc3067c093e3bf6612ac5afcaab2bb82\": [{\"type\": \"x\", \"op\": \"ln\", \"scale (log\\u2082)\": -5.41746367372377e-06}, {\"type\": \"x\", \"op\": \"linear_1\", \"scale (log\\u2082)\": -0.5030938170580167}, {\"type\": \"x\", \"op\": \"act\", \"scale (log\\u2082)\": -0.645663574598364}, {\"type\": \"x\", \"op\": \"linear_2\", \"scale (log\\u2082)\": -0.049427906953112125}, {\"type\": \"x\", \"op\": \"dropout\", \"scale (log\\u2082)\": -0.04976298759588158}, {\"type\": \"grad_x\", \"op\": \"ln\", \"scale (log\\u2082)\": 0.013073013224282004}, {\"type\": \"grad_x\", \"op\": \"linear_1\", \"scale (log\\u2082)\": 0.012872401677213024}, {\"type\": \"grad_x\", \"op\": \"act\", \"scale (log\\u2082)\": -0.48811912920480477}, {\"type\": \"grad_x\", \"op\": \"linear_2\", \"scale (log\\u2082)\": -0.5062541047663405}, {\"type\": \"grad_x\", \"op\": \"dropout\", \"scale (log\\u2082)\": -0.004226898298928224}], \"data-994762bd091869d8ac358d2908a5e1ec\": [{\"type\": \"w\", \"op\": \"ln\", \"scale (log\\u2082)\": null}, {\"type\": \"w\", \"op\": \"linear_1\", \"scale (log\\u2082)\": -0.0011971518470987805}, {\"type\": \"w\", \"op\": \"linear_2\", \"scale (log\\u2082)\": -0.0023576577578858023}, {\"type\": \"grad_w\", \"op\": \"ln\", \"scale (log\\u2082)\": -0.016867127443131475}, {\"type\": \"grad_w\", \"op\": \"linear_1\", \"scale (log\\u2082)\": -0.48548494127841807}, {\"type\": \"grad_w\", \"op\": \"linear_2\", \"scale (log\\u2082)\": -0.5273832752015188}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.LayerChart(...)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def unit_init(*args):\n",
    "    return 1\n",
    "\n",
    "mlp = UnitScaledMLP(config).apply(init_weights(unit_init))\n",
    "analyse_mlp(mlp, config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is much better! The final scales are almost exactly 1 in both directions.\n",
    "\n",
    "The compromise fwd & grad_x scaling factors in our linear layers do give temporary non-unit scaling, but this is minor and the second linear layer cancels out the first."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The self-attention layer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with a basic implementation of a (self-)attention module.\n",
    "\n",
    "(Note that we use [ALiBi](https://arxiv.org/abs/2108.12409) biases over positional embeddings here. This is primarily for simplicity, though these have also been [shown to perform](https://arxiv.org/abs/2210.15424) remarkably well!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: hide\n",
    "class Matmul(nn.Module):\n",
    "    def forward(self, a: Tensor, b: Tensor, scale: float=1.0) -> Tensor:\n",
    "        return (a @ b) * scale\n",
    "\n",
    "def split_heads(tensor: Tensor, num_heads: int, head_size: int) -> Tensor:\n",
    "    batch_size, seq_len, hidden_size = tensor.shape\n",
    "    tensor = tensor.view(batch_size, seq_len, num_heads, head_size)\n",
    "    return tensor.permute(0, 2, 1, 3)\n",
    "\n",
    "\n",
    "def merge_heads(tensor: Tensor, num_heads: int) -> Tensor:\n",
    "    tensor = tensor.permute(0, 2, 1, 3).contiguous()\n",
    "    batch_size, seq_len, num_heads, head_size = tensor.shape\n",
    "    return tensor.view(batch_size, seq_len, num_heads * head_size)\n",
    "\n",
    "\n",
    "def causal_mask(seq_len: int, num_heads: int) -> Tensor:\n",
    "    causal_mask = torch.tril(torch.ones((seq_len, seq_len), dtype=torch.float16))\n",
    "    causal_mask = causal_mask.view(1, 1, seq_len, seq_len)\n",
    "    alibi_mask = gen_alibi_mask(causal_mask, num_heads)\n",
    "    causal_mask = (1.0 - causal_mask) * -10_000\n",
    "    return alibi_mask + causal_mask\n",
    "\n",
    "\n",
    "# Based on https://github.com/labmlai/annotated_deep_learning_paper_implementations/blob/master/labml_nn/transformers/alibi/__init__.py\n",
    "def gen_alibi_mask(causal_mask: Tensor, num_heads: int) -> Tensor:\n",
    "    distances = causal_mask.to(torch.float32).cumsum(dim=-1)\n",
    "    slopes = gen_slopes(num_heads)\n",
    "    return distances.to(torch.float16) * slopes.view(1, num_heads, 1, 1)\n",
    "\n",
    "\n",
    "def gen_slopes(num_heads: int) -> Tensor:\n",
    "    n = 2 ** math.floor(math.log2(num_heads))\n",
    "    m_0 = 2.0 ** (-8.0 / n)\n",
    "    m = torch.pow(m_0, torch.arange(1, 1 + n))\n",
    "    if n < num_heads:\n",
    "        m_hat_0 = 2.0 ** (-4.0 / n)\n",
    "        m_hat = torch.pow(m_hat_0, torch.arange(1, 1 + 2 * (num_heads - n), 2))\n",
    "        m = torch.cat([m, m_hat])\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, config: NanoGPTConfig) -> None:\n",
    "        super().__init__()\n",
    "        self.ln = nn.LayerNorm(config.hidden_size)\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.num_heads = config.num_attention_heads\n",
    "        \n",
    "        self.w_qkv = nn.Linear(config.hidden_size, 3 * config.hidden_size)\n",
    "        self.qk_matmul = Matmul()\n",
    "        self.softmax = nn.Softmax(-1)\n",
    "        self.attn_dropout = nn.Dropout(config.dropout)\n",
    "        self.qkv_matmul = Matmul()\n",
    "        self.w_o = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.residual_dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, hidden_states: Tensor, attention_mask: Tensor) -> Tensor:\n",
    "        seq_len = hidden_states.shape[1]\n",
    "        head_size = self.hidden_size // self.num_heads\n",
    "\n",
    "        hidden_states = self.ln(hidden_states)\n",
    "        q_k_v = self.w_qkv(hidden_states)\n",
    "        q, k, v = q_k_v.split(self.hidden_size, dim=-1)\n",
    "        q, k, v = (split_heads(t, self.num_heads, head_size) for t in (q, k, v))\n",
    "\n",
    "        qk = self.qk_matmul(q, k.transpose(-1, -2), scale=1 / head_size ** 0.5).clone()\n",
    "        qk += causal_mask(seq_len, self.num_heads) + attention_mask\n",
    "        qk = self.softmax(qk)\n",
    "        qk = self.attn_dropout(qk)\n",
    "\n",
    "        qkv = self.qkv_matmul(qk, v)\n",
    "        qkv = merge_heads(qkv, self.num_heads)\n",
    "        qkvo = self.w_o(qkv)\n",
    "        return self.residual_dropout(qkvo).clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_attn(attention, config, batch_size=64, seq_len=16):\n",
    "    stats = instrument(attention)\n",
    "    x = torch.normal(0.0, 1.0, size=(batch_size, seq_len, config.hidden_size))\n",
    "    attention_mask = torch.zeros(batch_size, 1, 1, seq_len)\n",
    "    y = attention(x, attention_mask)\n",
    "    y.backward(torch.normal(0.0, 1.0, size=y.shape))\n",
    "    visualise(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-7ad172c6b7574efd9a1c89d1e8eb7da5\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-7ad172c6b7574efd9a1c89d1e8eb7da5\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-7ad172c6b7574efd9a1c89d1e8eb7da5\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.17.0?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"4.17.0\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}, \"axis\": {\"labelFontSize\": 12, \"titleFontSize\": 16}}, \"layer\": [{\"data\": {\"name\": \"data-e9d797d973b2a2fa9e3a1523971420f6\"}, \"mark\": \"line\", \"encoding\": {\"color\": {\"field\": \"type\", \"legend\": {\"labelFontSize\": 12, \"symbolSize\": 100, \"title\": \"\"}, \"scale\": {\"range\": [\"#6C8EBF\", \"#FF8000\"]}, \"sort\": \"descending\", \"type\": \"nominal\"}, \"x\": {\"axis\": {\"orient\": \"top\", \"values\": [-14.0, -12.0, -10.0, -8.0, -6.0, -4.0, -2.0, 0.0, 2.0, 4.0, 6.0, 8.0, 10.0, 12.0, 14.0, 16.0]}, \"field\": \"scale (log\\u2082)\", \"scale\": {\"domain\": [-14, 16]}, \"type\": \"quantitative\"}, \"y\": {\"field\": \"op\", \"sort\": [\"ln\", \"w_qkv\", \"qk_matmul\", \"attn_dropout\", \"qkv_matmul\", \"w_o\", \"residual_dropout\"], \"title\": \"\", \"type\": \"ordinal\"}}}, {\"data\": {\"name\": \"data-33049b39e8ca3be0b4b9d7379b8b242b\"}, \"mark\": {\"type\": \"point\", \"size\": 100}, \"encoding\": {\"color\": {\"field\": \"type\", \"legend\": {\"labelFontSize\": 12, \"symbolSize\": 100, \"title\": \"\"}, \"scale\": {\"range\": [\"#5D8944\", \"#ED3434\"]}, \"sort\": \"descending\", \"type\": \"nominal\"}, \"shape\": {\"field\": \"type\", \"scale\": {\"range\": [\"square\", \"triangle-down\"]}, \"sort\": \"descending\", \"type\": \"nominal\"}, \"x\": {\"axis\": {\"orient\": \"top\", \"values\": [-14.0, -12.0, -10.0, -8.0, -6.0, -4.0, -2.0, 0.0, 2.0, 4.0, 6.0, 8.0, 10.0, 12.0, 14.0, 16.0]}, \"field\": \"scale (log\\u2082)\", \"scale\": {\"domain\": [-14, 16]}, \"type\": \"quantitative\"}, \"y\": {\"field\": \"op\", \"sort\": [\"ln\", \"w_qkv\", \"qk_matmul\", \"attn_dropout\", \"qkv_matmul\", \"w_o\", \"residual_dropout\"], \"title\": \"\", \"type\": \"ordinal\"}}}], \"resolve\": {\"scale\": {\"color\": \"independent\", \"shape\": \"independent\"}}, \"width\": 500, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.17.0.json\", \"datasets\": {\"data-e9d797d973b2a2fa9e3a1523971420f6\": [{\"type\": \"x\", \"op\": \"ln\", \"scale (log\\u2082)\": -5.41746367372377e-06}, {\"type\": \"x\", \"op\": \"w_qkv\", \"scale (log\\u2082)\": -0.49756999197274726}, {\"type\": \"x\", \"op\": \"qk_matmul\", \"scale (log\\u2082)\": -0.9899051066676171}, {\"type\": \"x\", \"op\": \"attn_dropout\", \"scale (log\\u2082)\": -3.033462952451145}, {\"type\": \"x\", \"op\": \"qkv_matmul\", \"scale (log\\u2082)\": -1.3588413675382969}, {\"type\": \"x\", \"op\": \"w_o\", \"scale (log\\u2082)\": -1.3627665338005899}, {\"type\": \"x\", \"op\": \"residual_dropout\", \"scale (log\\u2082)\": -1.2847193675156194}, {\"type\": \"grad_x\", \"op\": \"ln\", \"scale (log\\u2082)\": -1.136208078675839}, {\"type\": \"grad_x\", \"op\": \"w_qkv\", \"scale (log\\u2082)\": -1.1414684807804958}, {\"type\": \"grad_x\", \"op\": \"attn_dropout\", \"scale (log\\u2082)\": 2.647705991608464}, {\"type\": \"grad_x\", \"op\": \"w_o\", \"scale (log\\u2082)\": 0.07274840865645774}, {\"type\": \"grad_x\", \"op\": \"residual_dropout\", \"scale (log\\u2082)\": 0.07569816894624364}], \"data-33049b39e8ca3be0b4b9d7379b8b242b\": [{\"type\": \"w\", \"op\": \"ln\", \"scale (log\\u2082)\": null}, {\"type\": \"w\", \"op\": \"w_qkv\", \"scale (log\\u2082)\": -4.790641534730089}, {\"type\": \"w\", \"op\": \"w_o\", \"scale (log\\u2082)\": -4.293663132819827}, {\"type\": \"grad_w\", \"op\": \"ln\", \"scale (log\\u2082)\": 3.8162681688934184}, {\"type\": \"grad_w\", \"op\": \"w_qkv\", \"scale (log\\u2082)\": 3.568308020923088}, {\"type\": \"grad_w\", \"op\": \"w_o\", \"scale (log\\u2082)\": 3.7157623080556705}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.LayerChart(...)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "attention = Attention(config).apply(init_weights(glorot_init))\n",
    "analyse_attn(attention, config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a lot going on here. The key things to note here are again that the output scale falls by just over half in both directions, and the grad_ws are again significantly under-scaled.\n",
    "\n",
    "Let's fix this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnitScaledAttention(Attention):\n",
    "    def __init__(self, config: NanoGPTConfig) -> None:\n",
    "        super().__init__(config)\n",
    "        self.ln = UnitScaledLayerNorm(config.hidden_size)\n",
    "        self.w_qkv = UnitScaledLinear(\n",
    "            config.hidden_size, 3 * config.hidden_size, scale_for=\"fwd\"\n",
    "        )\n",
    "        self.qk_matmul = UnitScaledMatmul(scale_for=\"fwd\")\n",
    "        self.softmax = UnitScaledSoftmax(-1)\n",
    "        self.attn_dropout = UnitScaledDropout(config.dropout)\n",
    "        self.qkv_matmul = UnitScaledMatmul(scale_for=\"fwd\")\n",
    "        self.w_o = UnitScaledLinear(config.hidden_size, config.hidden_size)\n",
    "        self.residual_dropout = UnitScaledDropout(config.dropout)\n",
    "\n",
    "class UnitScaledMatmul(Matmul):\n",
    "    def __init__(self, scale_for=\"fwd, grad_a, grad_b\") -> None:\n",
    "        super().__init__()\n",
    "        self.scale_for = scale_for\n",
    "    \n",
    "    def get_scales(self, a: Tensor, b: Tensor):\n",
    "        fwd_scale = a.shape[-1] ** -0.5\n",
    "        grad_a_scale = b.shape[-1] ** -0.5\n",
    "        grad_b_scale = a.shape[-2] ** -0.5\n",
    "        if self.scale_for == \"fwd\":\n",
    "            grad_a_scale = grad_b_scale = fwd_scale\n",
    "        elif self.scale_for == \"fwd, grad_a, grad_b\":\n",
    "            fwd_scale = grad_a_scale = grad_b_scale = geometric_mean([\n",
    "                fwd_scale, grad_a_scale, grad_b_scale\n",
    "            ])\n",
    "        else:\n",
    "            assert False, f\"demo implementation has no {self.scale_for} scaling\"\n",
    "        return fwd_scale, grad_a_scale, grad_b_scale\n",
    "    \n",
    "    def forward(self, a: Tensor, b: Tensor, scale: float=1.0) -> Tensor:\n",
    "        # ignores provided scale\n",
    "        fwd_scale, grad_a_scale, grad_b_scale = self.get_scales(a, b)\n",
    "        a = scaled(a, beta=grad_a_scale)\n",
    "        b = scaled(b, beta=grad_b_scale)\n",
    "        output = a @ b\n",
    "        return scaled(output, alpha=fwd_scale)\n",
    "\n",
    "class UnitScaledSoftmax(nn.Softmax):\n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        fwd_scale = bwd_scale = input.shape[-1] ** 0.5\n",
    "        input = scaled(input, fwd_scale)\n",
    "        output = F.softmax(input, self.dim, _stacklevel=5)\n",
    "        return scaled(output, bwd_scale)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To unit scale the attention layer we again swap out regular layers for unit-scaled ones. This requires altering two more operations: matmul and softmax.\n",
    "\n",
    "Like the linear layer, we provide scaling the matmul based on varying criteria. We find that scaling each matmul and linear layer here for the forward pass ensures good scaling in both directions. We see this empirically in our analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-cf746bdd83cf4cedadaf3283e1e77061\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-cf746bdd83cf4cedadaf3283e1e77061\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-cf746bdd83cf4cedadaf3283e1e77061\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.17.0?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"4.17.0\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}, \"axis\": {\"labelFontSize\": 12, \"titleFontSize\": 16}}, \"layer\": [{\"data\": {\"name\": \"data-9dedcff5b4b1a3937e3755e51c4f4691\"}, \"mark\": \"line\", \"encoding\": {\"color\": {\"field\": \"type\", \"legend\": {\"labelFontSize\": 12, \"symbolSize\": 100, \"title\": \"\"}, \"scale\": {\"range\": [\"#6C8EBF\", \"#FF8000\"]}, \"sort\": \"descending\", \"type\": \"nominal\"}, \"x\": {\"axis\": {\"orient\": \"top\", \"values\": [-14.0, -12.0, -10.0, -8.0, -6.0, -4.0, -2.0, 0.0, 2.0, 4.0, 6.0, 8.0, 10.0, 12.0, 14.0, 16.0]}, \"field\": \"scale (log\\u2082)\", \"scale\": {\"domain\": [-14, 16]}, \"type\": \"quantitative\"}, \"y\": {\"field\": \"op\", \"sort\": [\"ln\", \"w_qkv\", \"qk_matmul\", \"attn_dropout\", \"qkv_matmul\", \"w_o\", \"residual_dropout\"], \"title\": \"\", \"type\": \"ordinal\"}}}, {\"data\": {\"name\": \"data-1f178be2ce2048aaa99325c3c50b3ef5\"}, \"mark\": {\"type\": \"point\", \"size\": 100}, \"encoding\": {\"color\": {\"field\": \"type\", \"legend\": {\"labelFontSize\": 12, \"symbolSize\": 100, \"title\": \"\"}, \"scale\": {\"range\": [\"#5D8944\", \"#ED3434\"]}, \"sort\": \"descending\", \"type\": \"nominal\"}, \"shape\": {\"field\": \"type\", \"scale\": {\"range\": [\"square\", \"triangle-down\"]}, \"sort\": \"descending\", \"type\": \"nominal\"}, \"x\": {\"axis\": {\"orient\": \"top\", \"values\": [-14.0, -12.0, -10.0, -8.0, -6.0, -4.0, -2.0, 0.0, 2.0, 4.0, 6.0, 8.0, 10.0, 12.0, 14.0, 16.0]}, \"field\": \"scale (log\\u2082)\", \"scale\": {\"domain\": [-14, 16]}, \"type\": \"quantitative\"}, \"y\": {\"field\": \"op\", \"sort\": [\"ln\", \"w_qkv\", \"qk_matmul\", \"attn_dropout\", \"qkv_matmul\", \"w_o\", \"residual_dropout\"], \"title\": \"\", \"type\": \"ordinal\"}}}], \"resolve\": {\"scale\": {\"color\": \"independent\", \"shape\": \"independent\"}}, \"width\": 500, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.17.0.json\", \"datasets\": {\"data-9dedcff5b4b1a3937e3755e51c4f4691\": [{\"type\": \"x\", \"op\": \"ln\", \"scale (log\\u2082)\": -6.793330703198998e-06}, {\"type\": \"x\", \"op\": \"w_qkv\", \"scale (log\\u2082)\": 0.00010817303189346413}, {\"type\": \"x\", \"op\": \"qk_matmul\", \"scale (log\\u2082)\": -0.004163855577248353}, {\"type\": \"x\", \"op\": \"attn_dropout\", \"scale (log\\u2082)\": -0.16143302168649476}, {\"type\": \"x\", \"op\": \"qkv_matmul\", \"scale (log\\u2082)\": -0.14429010853938504}, {\"type\": \"x\", \"op\": \"w_o\", \"scale (log\\u2082)\": -0.14230397861612656}, {\"type\": \"x\", \"op\": \"residual_dropout\", \"scale (log\\u2082)\": -0.14203447153303927}, {\"type\": \"grad_x\", \"op\": \"ln\", \"scale (log\\u2082)\": -0.14019898756448962}, {\"type\": \"grad_x\", \"op\": \"w_qkv\", \"scale (log\\u2082)\": -0.14092850151416025}, {\"type\": \"grad_x\", \"op\": \"attn_dropout\", \"scale (log\\u2082)\": 0.0035774435168308316}, {\"type\": \"grad_x\", \"op\": \"w_o\", \"scale (log\\u2082)\": -0.0006943750461821142}, {\"type\": \"grad_x\", \"op\": \"residual_dropout\", \"scale (log\\u2082)\": -0.0012966437713774629}], \"data-1f178be2ce2048aaa99325c3c50b3ef5\": [{\"type\": \"w\", \"op\": \"ln\", \"scale (log\\u2082)\": null}, {\"type\": \"w\", \"op\": \"w_qkv\", \"scale (log\\u2082)\": 0.00023542503916269508}, {\"type\": \"w\", \"op\": \"w_o\", \"scale (log\\u2082)\": 0.0012473667171435268}, {\"type\": \"grad_w\", \"op\": \"ln\", \"scale (log\\u2082)\": -0.16173070622848482}, {\"type\": \"grad_w\", \"op\": \"w_qkv\", \"scale (log\\u2082)\": -0.9399478057729982}, {\"type\": \"grad_w\", \"op\": \"w_o\", \"scale (log\\u2082)\": -0.14139649083587802}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.LayerChart(...)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "attention = UnitScaledAttention(config).apply(init_weights(unit_init))\n",
    "analyse_attn(attention, config, batch_size=64, seq_len=64)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, a great improvement for forward, grad_x and grad_w tensors, with all having approximate unit scale. Note that these scaling rules are robust to changes in hyperparameters too. You can experiment with the initial config to verify this.\n",
    "\n",
    "We now have our key building-blocks 🧱 Let's put it all together!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The transformer block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, config: NanoGPTConfig) -> None:\n",
    "        super().__init__()\n",
    "        self.attention_layer = Residual(Attention(config))\n",
    "        self.mlp_layer = Residual(MLP(config))\n",
    "        \n",
    "    def forward(self, hidden_states: Tensor, attention_mask: Tensor) -> Tensor:\n",
    "        hidden_states = self.attention_layer(hidden_states, attention_mask)\n",
    "        return self.mlp_layer(hidden_states)\n",
    "\n",
    "\n",
    "class Residual(nn.Module):\n",
    "    def __init__(self, f: nn.Module):\n",
    "        super().__init__()\n",
    "        self.f = f\n",
    "    \n",
    "    def forward(self, x: Tensor, *args):\n",
    "        return x + self.f(x, *args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnitScaledTransformerBlock(TransformerBlock):\n",
    "    def __init__(self, config: NanoGPTConfig) -> None:\n",
    "        super().__init__(config)\n",
    "        self.attention_layer = UnitScaledResidual(UnitScaledAttention(config))\n",
    "        self.mlp_layer = UnitScaledResidual(UnitScaledMLP(config))\n",
    "\n",
    "class UnitScaledResidual(Residual):\n",
    "    def __init__(self, f: nn.Module, tau: float=0.2):\n",
    "        super().__init__(f)\n",
    "        self.tau = tau\n",
    "    \n",
    "    def forward(self, x: Tensor, *args):\n",
    "        _x = scaled(x, beta=self.tau ** 0.5)\n",
    "        y = self.f(_x, *args)\n",
    "        return x * (1 - self.tau) ** 0.5 + scaled(y, alpha=self.tau ** 0.5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the scaling in our residual layer includes an important trick.\n",
    "\n",
    "It's necessary for unit-scaled models to use a weighted sum when doing their residual-add, to down-weight the residual/trunk branch. If this is not included, training can fail as too much signal comes from each residual (regular models avoid this as their residual implicitly reduces scale).\n",
    "\n",
    "However, the naïve implementation of this breaks unit scale. We get around this by delaying the weighting until the end of the residual branch in the backward pass (see paper for more details)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The full transformer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that our implementation contains a little extra boilerplate to make it 🤗-compliant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NanoGPTModel(PreTrainedModel):\n",
    "    config_class = NanoGPTConfig\n",
    "\n",
    "    def __init__(self, config: NanoGPTConfig) -> None:\n",
    "        super().__init__(config)\n",
    "        self.input_embeddings = nn.Embedding(config.vocab_size, config.hidden_size)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        self.ln = nn.LayerNorm(config.hidden_size)\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [TransformerBlock(config) for _ in range(config.num_hidden_layers)]\n",
    "        )\n",
    "        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        self.apply(init_weights(glorot_init))\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Tensor,\n",
    "        attention_mask: Tensor,\n",
    "        position_ids: Optional[Tensor] = None,\n",
    "        labels: Optional[Tensor] = None,\n",
    "        **kwargs,\n",
    "    ) -> CausalLMOutputWithCrossAttentions:\n",
    "        if position_ids is None:  # True, except at test time\n",
    "            position_ids = torch.arange(0, input_ids.shape[-1], device=input_ids.device)\n",
    "            position_ids = position_ids.unsqueeze(0)\n",
    "\n",
    "        attention_mask = attention_mask[:, None, None, :].to(dtype=self.dtype)\n",
    "        attention_mask = (1.0 - attention_mask) * -10_000\n",
    "\n",
    "        hidden_states = self.input_embeddings(input_ids)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        for block in self.blocks:\n",
    "            hidden_states = block(hidden_states, attention_mask=attention_mask)\n",
    "        hidden_states = self.ln(hidden_states)\n",
    "        logits = self.lm_head(hidden_states)\n",
    "\n",
    "        if labels is None:\n",
    "            return CausalLMOutputWithCrossAttentions(logits=logits)\n",
    "        else:\n",
    "            labels = torch.roll(labels, -1, 1)\n",
    "            labels[:, -1] = -100  # By default, ignore_index of CrossEntropyLoss is -100\n",
    "            loss = self.loss_fn(logits.view(-1, logits.shape[-1]), labels.view(-1))\n",
    "            return CausalLMOutputWithCrossAttentions(loss=loss)\n",
    "\n",
    "    def get_input_embeddings(self) -> torch.nn.Embedding:\n",
    "        return self.input_embeddings\n",
    "\n",
    "    def prepare_inputs_for_generation(\n",
    "        self, input_ids: Tensor, **kwargs\n",
    "    ) -> Dict[str, Tensor]:\n",
    "        attention_mask = kwargs[\"attention_mask\"]\n",
    "        position_ids = attention_mask.long().cumsum(-1) - 1\n",
    "        position_ids.masked_fill_(attention_mask == 0, 1)\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"position_ids\": position_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_full_model(model, config, batch_size=64, seq_len=16):\n",
    "    stats = instrument(model)\n",
    "    input_ids = labels = torch.randint(0, config.vocab_size, size=(batch_size, seq_len))\n",
    "    attention_mask = torch.zeros(batch_size, seq_len)\n",
    "    y = model(input_ids, attention_mask, labels=labels).loss\n",
    "    y.backward()\n",
    "    visualise(stats)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the scaling for the entire (non-unit-scaled) transformer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-6c95a6b05c8145e18fa61d46864df574\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-6c95a6b05c8145e18fa61d46864df574\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-6c95a6b05c8145e18fa61d46864df574\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.17.0?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"4.17.0\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}, \"axis\": {\"labelFontSize\": 12, \"titleFontSize\": 16}}, \"layer\": [{\"data\": {\"name\": \"data-3d1f15da82ac1d053c79315d098ebff2\"}, \"mark\": \"line\", \"encoding\": {\"color\": {\"field\": \"type\", \"legend\": {\"labelFontSize\": 12, \"symbolSize\": 100, \"title\": \"\"}, \"scale\": {\"range\": [\"#6C8EBF\", \"#FF8000\"]}, \"sort\": \"descending\", \"type\": \"nominal\"}, \"x\": {\"axis\": {\"orient\": \"top\", \"values\": [-14.0, -12.0, -10.0, -8.0, -6.0, -4.0, -2.0, 0.0, 2.0, 4.0, 6.0, 8.0, 10.0, 12.0, 14.0, 16.0]}, \"field\": \"scale (log\\u2082)\", \"scale\": {\"domain\": [-14, 16]}, \"type\": \"quantitative\"}, \"y\": {\"field\": \"op\", \"sort\": [\"input_embeddings\", \"dropout\", \"ln\", \"0.attention_layer.f.ln\", \"0.attention_layer.f.w_qkv\", \"0.attention_layer.f.qk_matmul\", \"0.attention_layer.f.attn_dropout\", \"0.attention_layer.f.qkv_matmul\", \"0.attention_layer.f.w_o\", \"0.attention_layer.f.residual_dropout\", \"0.mlp_layer.f.ln\", \"0.mlp_layer.f.linear_1\", \"0.mlp_layer.f.act\", \"0.mlp_layer.f.linear_2\", \"0.mlp_layer.f.dropout\", \"1.attention_layer.f.ln\", \"1.attention_layer.f.w_qkv\", \"1.attention_layer.f.qk_matmul\", \"1.attention_layer.f.attn_dropout\", \"1.attention_layer.f.qkv_matmul\", \"1.attention_layer.f.w_o\", \"1.attention_layer.f.residual_dropout\", \"1.mlp_layer.f.ln\", \"1.mlp_layer.f.linear_1\", \"1.mlp_layer.f.act\", \"1.mlp_layer.f.linear_2\", \"1.mlp_layer.f.dropout\", \"2.attention_layer.f.ln\", \"2.attention_layer.f.w_qkv\", \"2.attention_layer.f.qk_matmul\", \"2.attention_layer.f.attn_dropout\", \"2.attention_layer.f.qkv_matmul\", \"2.attention_layer.f.w_o\", \"2.attention_layer.f.residual_dropout\", \"2.mlp_layer.f.ln\", \"2.mlp_layer.f.linear_1\", \"2.mlp_layer.f.act\", \"2.mlp_layer.f.linear_2\", \"2.mlp_layer.f.dropout\", \"3.attention_layer.f.ln\", \"3.attention_layer.f.w_qkv\", \"3.attention_layer.f.qk_matmul\", \"3.attention_layer.f.attn_dropout\", \"3.attention_layer.f.qkv_matmul\", \"3.attention_layer.f.w_o\", \"3.attention_layer.f.residual_dropout\", \"3.mlp_layer.f.ln\", \"3.mlp_layer.f.linear_1\", \"3.mlp_layer.f.act\", \"3.mlp_layer.f.linear_2\", \"3.mlp_layer.f.dropout\", \"4.attention_layer.f.ln\", \"4.attention_layer.f.w_qkv\", \"4.attention_layer.f.qk_matmul\", \"4.attention_layer.f.attn_dropout\", \"4.attention_layer.f.qkv_matmul\", \"4.attention_layer.f.w_o\", \"4.attention_layer.f.residual_dropout\", \"4.mlp_layer.f.ln\", \"4.mlp_layer.f.linear_1\", \"4.mlp_layer.f.act\", \"4.mlp_layer.f.linear_2\", \"4.mlp_layer.f.dropout\", \"5.attention_layer.f.ln\", \"5.attention_layer.f.w_qkv\", \"5.attention_layer.f.qk_matmul\", \"5.attention_layer.f.attn_dropout\", \"5.attention_layer.f.qkv_matmul\", \"5.attention_layer.f.w_o\", \"5.attention_layer.f.residual_dropout\", \"5.mlp_layer.f.ln\", \"5.mlp_layer.f.linear_1\", \"5.mlp_layer.f.act\", \"5.mlp_layer.f.linear_2\", \"5.mlp_layer.f.dropout\", \"lm_head\"], \"title\": \"\", \"type\": \"ordinal\"}}}, {\"data\": {\"name\": \"data-42a504cc8b52eeb51286ea512abaaf6d\"}, \"mark\": {\"type\": \"point\", \"size\": 100}, \"encoding\": {\"color\": {\"field\": \"type\", \"legend\": {\"labelFontSize\": 12, \"symbolSize\": 100, \"title\": \"\"}, \"scale\": {\"range\": [\"#5D8944\", \"#ED3434\"]}, \"sort\": \"descending\", \"type\": \"nominal\"}, \"shape\": {\"field\": \"type\", \"scale\": {\"range\": [\"square\", \"triangle-down\"]}, \"sort\": \"descending\", \"type\": \"nominal\"}, \"x\": {\"axis\": {\"orient\": \"top\", \"values\": [-14.0, -12.0, -10.0, -8.0, -6.0, -4.0, -2.0, 0.0, 2.0, 4.0, 6.0, 8.0, 10.0, 12.0, 14.0, 16.0]}, \"field\": \"scale (log\\u2082)\", \"scale\": {\"domain\": [-14, 16]}, \"type\": \"quantitative\"}, \"y\": {\"field\": \"op\", \"sort\": [\"input_embeddings\", \"dropout\", \"ln\", \"0.attention_layer.f.ln\", \"0.attention_layer.f.w_qkv\", \"0.attention_layer.f.qk_matmul\", \"0.attention_layer.f.attn_dropout\", \"0.attention_layer.f.qkv_matmul\", \"0.attention_layer.f.w_o\", \"0.attention_layer.f.residual_dropout\", \"0.mlp_layer.f.ln\", \"0.mlp_layer.f.linear_1\", \"0.mlp_layer.f.act\", \"0.mlp_layer.f.linear_2\", \"0.mlp_layer.f.dropout\", \"1.attention_layer.f.ln\", \"1.attention_layer.f.w_qkv\", \"1.attention_layer.f.qk_matmul\", \"1.attention_layer.f.attn_dropout\", \"1.attention_layer.f.qkv_matmul\", \"1.attention_layer.f.w_o\", \"1.attention_layer.f.residual_dropout\", \"1.mlp_layer.f.ln\", \"1.mlp_layer.f.linear_1\", \"1.mlp_layer.f.act\", \"1.mlp_layer.f.linear_2\", \"1.mlp_layer.f.dropout\", \"2.attention_layer.f.ln\", \"2.attention_layer.f.w_qkv\", \"2.attention_layer.f.qk_matmul\", \"2.attention_layer.f.attn_dropout\", \"2.attention_layer.f.qkv_matmul\", \"2.attention_layer.f.w_o\", \"2.attention_layer.f.residual_dropout\", \"2.mlp_layer.f.ln\", \"2.mlp_layer.f.linear_1\", \"2.mlp_layer.f.act\", \"2.mlp_layer.f.linear_2\", \"2.mlp_layer.f.dropout\", \"3.attention_layer.f.ln\", \"3.attention_layer.f.w_qkv\", \"3.attention_layer.f.qk_matmul\", \"3.attention_layer.f.attn_dropout\", \"3.attention_layer.f.qkv_matmul\", \"3.attention_layer.f.w_o\", \"3.attention_layer.f.residual_dropout\", \"3.mlp_layer.f.ln\", \"3.mlp_layer.f.linear_1\", \"3.mlp_layer.f.act\", \"3.mlp_layer.f.linear_2\", \"3.mlp_layer.f.dropout\", \"4.attention_layer.f.ln\", \"4.attention_layer.f.w_qkv\", \"4.attention_layer.f.qk_matmul\", \"4.attention_layer.f.attn_dropout\", \"4.attention_layer.f.qkv_matmul\", \"4.attention_layer.f.w_o\", \"4.attention_layer.f.residual_dropout\", \"4.mlp_layer.f.ln\", \"4.mlp_layer.f.linear_1\", \"4.mlp_layer.f.act\", \"4.mlp_layer.f.linear_2\", \"4.mlp_layer.f.dropout\", \"5.attention_layer.f.ln\", \"5.attention_layer.f.w_qkv\", \"5.attention_layer.f.qk_matmul\", \"5.attention_layer.f.attn_dropout\", \"5.attention_layer.f.qkv_matmul\", \"5.attention_layer.f.w_o\", \"5.attention_layer.f.residual_dropout\", \"5.mlp_layer.f.ln\", \"5.mlp_layer.f.linear_1\", \"5.mlp_layer.f.act\", \"5.mlp_layer.f.linear_2\", \"5.mlp_layer.f.dropout\", \"lm_head\"], \"title\": \"\", \"type\": \"ordinal\"}}}], \"resolve\": {\"scale\": {\"color\": \"independent\", \"shape\": \"independent\"}}, \"width\": 500, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.17.0.json\", \"datasets\": {\"data-3d1f15da82ac1d053c79315d098ebff2\": [{\"type\": \"x\", \"op\": \"input_embeddings\", \"scale (log\\u2082)\": -4.290411195056242}, {\"type\": \"x\", \"op\": \"dropout\", \"scale (log\\u2082)\": -4.214252614761554}, {\"type\": \"x\", \"op\": \"ln\", \"scale (log\\u2082)\": -1.7198266111377426e-07}, {\"type\": \"x\", \"op\": \"0.attention_layer.f.ln\", \"scale (log\\u2082)\": -0.002498748819840394}, {\"type\": \"x\", \"op\": \"0.attention_layer.f.w_qkv\", \"scale (log\\u2082)\": -0.49815322015821384}, {\"type\": \"x\", \"op\": \"0.attention_layer.f.qk_matmul\", \"scale (log\\u2082)\": -0.9885208879706147}, {\"type\": \"x\", \"op\": \"0.attention_layer.f.attn_dropout\", \"scale (log\\u2082)\": -3.0500501597864496}, {\"type\": \"x\", \"op\": \"0.attention_layer.f.qkv_matmul\", \"scale (log\\u2082)\": -1.369447349095369}, {\"type\": \"x\", \"op\": \"0.attention_layer.f.w_o\", \"scale (log\\u2082)\": -1.3664716910361232}, {\"type\": \"x\", \"op\": \"0.attention_layer.f.residual_dropout\", \"scale (log\\u2082)\": -1.2901304280153452}, {\"type\": \"x\", \"op\": \"0.mlp_layer.f.ln\", \"scale (log\\u2082)\": -5.813130713431246e-05}, {\"type\": \"x\", \"op\": \"0.mlp_layer.f.linear_1\", \"scale (log\\u2082)\": -0.6643978378639446}, {\"type\": \"x\", \"op\": \"0.mlp_layer.f.act\", \"scale (log\\u2082)\": -1.493346778135104}, {\"type\": \"x\", \"op\": \"0.mlp_layer.f.linear_2\", \"scale (log\\u2082)\": -1.0563202194406325}, {\"type\": \"x\", \"op\": \"0.mlp_layer.f.dropout\", \"scale (log\\u2082)\": -0.979916823895215}, {\"type\": \"x\", \"op\": \"1.attention_layer.f.ln\", \"scale (log\\u2082)\": -1.6510428956442116e-05}, {\"type\": \"x\", \"op\": \"1.attention_layer.f.w_qkv\", \"scale (log\\u2082)\": -0.49863085841386934}, {\"type\": \"x\", \"op\": \"1.attention_layer.f.qk_matmul\", \"scale (log\\u2082)\": -0.988707728600542}, {\"type\": \"x\", \"op\": \"1.attention_layer.f.attn_dropout\", \"scale (log\\u2082)\": -3.0883474364335686}, {\"type\": \"x\", \"op\": \"1.attention_layer.f.qkv_matmul\", \"scale (log\\u2082)\": -0.9092387035160961}, {\"type\": \"x\", \"op\": \"1.attention_layer.f.w_o\", \"scale (log\\u2082)\": -0.90770223802248}, {\"type\": \"x\", \"op\": \"1.attention_layer.f.residual_dropout\", \"scale (log\\u2082)\": -0.8324219855612635}, {\"type\": \"x\", \"op\": \"1.mlp_layer.f.ln\", \"scale (log\\u2082)\": -8.685150011199769e-06}, {\"type\": \"x\", \"op\": \"1.mlp_layer.f.linear_1\", \"scale (log\\u2082)\": -0.6622230912212898}, {\"type\": \"x\", \"op\": \"1.mlp_layer.f.act\", \"scale (log\\u2082)\": -1.4810577979979473}, {\"type\": \"x\", \"op\": \"1.mlp_layer.f.linear_2\", \"scale (log\\u2082)\": -1.0477196274672347}, {\"type\": \"x\", \"op\": \"1.mlp_layer.f.dropout\", \"scale (log\\u2082)\": -0.9705616126445142}, {\"type\": \"x\", \"op\": \"2.attention_layer.f.ln\", \"scale (log\\u2082)\": -5.7614303080799775e-06}, {\"type\": \"x\", \"op\": \"2.attention_layer.f.w_qkv\", \"scale (log\\u2082)\": -0.5018219210189244}, {\"type\": \"x\", \"op\": \"2.attention_layer.f.qk_matmul\", \"scale (log\\u2082)\": -0.9763498617080075}, {\"type\": \"x\", \"op\": \"2.attention_layer.f.attn_dropout\", \"scale (log\\u2082)\": -3.091694245786372}, {\"type\": \"x\", \"op\": \"2.attention_layer.f.qkv_matmul\", \"scale (log\\u2082)\": -0.8141299167641863}, {\"type\": \"x\", \"op\": \"2.attention_layer.f.w_o\", \"scale (log\\u2082)\": -0.8044852592439202}, {\"type\": \"x\", \"op\": \"2.attention_layer.f.residual_dropout\", \"scale (log\\u2082)\": -0.7284704335980982}, {\"type\": \"x\", \"op\": \"2.mlp_layer.f.ln\", \"scale (log\\u2082)\": -3.6976317320669667e-06}, {\"type\": \"x\", \"op\": \"2.mlp_layer.f.linear_1\", \"scale (log\\u2082)\": -0.6603876839780273}, {\"type\": \"x\", \"op\": \"2.mlp_layer.f.act\", \"scale (log\\u2082)\": -1.4753346663755869}, {\"type\": \"x\", \"op\": \"2.mlp_layer.f.linear_2\", \"scale (log\\u2082)\": -1.0439895815202094}, {\"type\": \"x\", \"op\": \"2.mlp_layer.f.dropout\", \"scale (log\\u2082)\": -0.9667554723535985}, {\"type\": \"x\", \"op\": \"3.attention_layer.f.ln\", \"scale (log\\u2082)\": -2.751725038055267e-06}, {\"type\": \"x\", \"op\": \"3.attention_layer.f.w_qkv\", \"scale (log\\u2082)\": -0.5059945839575458}, {\"type\": \"x\", \"op\": \"3.attention_layer.f.qk_matmul\", \"scale (log\\u2082)\": -1.0172322364839503}, {\"type\": \"x\", \"op\": \"3.attention_layer.f.attn_dropout\", \"scale (log\\u2082)\": -3.1071842892518347}, {\"type\": \"x\", \"op\": \"3.attention_layer.f.qkv_matmul\", \"scale (log\\u2082)\": -0.770863612272176}, {\"type\": \"x\", \"op\": \"3.attention_layer.f.w_o\", \"scale (log\\u2082)\": -0.795373934282398}, {\"type\": \"x\", \"op\": \"3.attention_layer.f.residual_dropout\", \"scale (log\\u2082)\": -0.7197963199988098}, {\"type\": \"x\", \"op\": \"3.mlp_layer.f.ln\", \"scale (log\\u2082)\": -1.9778018406136345e-06}, {\"type\": \"x\", \"op\": \"3.mlp_layer.f.linear_1\", \"scale (log\\u2082)\": -0.6590944014211186}, {\"type\": \"x\", \"op\": \"3.mlp_layer.f.act\", \"scale (log\\u2082)\": -1.4905958652996916}, {\"type\": \"x\", \"op\": \"3.mlp_layer.f.linear_2\", \"scale (log\\u2082)\": -1.0386214050161153}, {\"type\": \"x\", \"op\": \"3.mlp_layer.f.dropout\", \"scale (log\\u2082)\": -0.9618179490271215}, {\"type\": \"x\", \"op\": \"4.attention_layer.f.ln\", \"scale (log\\u2082)\": -1.4618532729665813e-06}, {\"type\": \"x\", \"op\": \"4.attention_layer.f.w_qkv\", \"scale (log\\u2082)\": -0.5136919326666775}, {\"type\": \"x\", \"op\": \"4.attention_layer.f.qk_matmul\", \"scale (log\\u2082)\": -1.0327892234654965}, {\"type\": \"x\", \"op\": \"4.attention_layer.f.attn_dropout\", \"scale (log\\u2082)\": -3.106599213788651}, {\"type\": \"x\", \"op\": \"4.attention_layer.f.qkv_matmul\", \"scale (log\\u2082)\": -0.7737073260354465}, {\"type\": \"x\", \"op\": \"4.attention_layer.f.w_o\", \"scale (log\\u2082)\": -0.7788781561661436}, {\"type\": \"x\", \"op\": \"4.attention_layer.f.residual_dropout\", \"scale (log\\u2082)\": -0.7022639515849477}, {\"type\": \"x\", \"op\": \"4.mlp_layer.f.ln\", \"scale (log\\u2082)\": -9.459048898372686e-07}, {\"type\": \"x\", \"op\": \"4.mlp_layer.f.linear_1\", \"scale (log\\u2082)\": -0.6579440557066134}, {\"type\": \"x\", \"op\": \"4.mlp_layer.f.act\", \"scale (log\\u2082)\": -1.4810018679787724}, {\"type\": \"x\", \"op\": \"4.mlp_layer.f.linear_2\", \"scale (log\\u2082)\": -1.0386823501602194}, {\"type\": \"x\", \"op\": \"4.mlp_layer.f.dropout\", \"scale (log\\u2082)\": -0.9623766386730537}, {\"type\": \"x\", \"op\": \"5.attention_layer.f.ln\", \"scale (log\\u2082)\": -6.879307674667236e-07}, {\"type\": \"x\", \"op\": \"5.attention_layer.f.w_qkv\", \"scale (log\\u2082)\": -0.5049441543602524}, {\"type\": \"x\", \"op\": \"5.attention_layer.f.qk_matmul\", \"scale (log\\u2082)\": -1.018902421842267}, {\"type\": \"x\", \"op\": \"5.attention_layer.f.attn_dropout\", \"scale (log\\u2082)\": -3.108487624745146}, {\"type\": \"x\", \"op\": \"5.attention_layer.f.qkv_matmul\", \"scale (log\\u2082)\": -0.7367267873574371}, {\"type\": \"x\", \"op\": \"5.attention_layer.f.w_o\", \"scale (log\\u2082)\": -0.7405871603926601}, {\"type\": \"x\", \"op\": \"5.attention_layer.f.residual_dropout\", \"scale (log\\u2082)\": -0.6640927208051159}, {\"type\": \"x\", \"op\": \"5.mlp_layer.f.ln\", \"scale (log\\u2082)\": -4.299566912255644e-07}, {\"type\": \"x\", \"op\": \"5.mlp_layer.f.linear_1\", \"scale (log\\u2082)\": -0.6646696221744116}, {\"type\": \"x\", \"op\": \"5.mlp_layer.f.act\", \"scale (log\\u2082)\": -1.4942916450394093}, {\"type\": \"x\", \"op\": \"5.mlp_layer.f.linear_2\", \"scale (log\\u2082)\": -1.0793760075573877}, {\"type\": \"x\", \"op\": \"5.mlp_layer.f.dropout\", \"scale (log\\u2082)\": -1.0037994519159912}, {\"type\": \"x\", \"op\": \"lm_head\", \"scale (log\\u2082)\": 0.022185425805386686}, {\"type\": \"grad_x\", \"op\": \"dropout\", \"scale (log\\u2082)\": -9.83736680887471}, {\"type\": \"grad_x\", \"op\": \"ln\", \"scale (log\\u2082)\": -15.160799384197885}, {\"type\": \"grad_x\", \"op\": \"0.attention_layer.f.ln\", \"scale (log\\u2082)\": -9.920671640489765}, {\"type\": \"grad_x\", \"op\": \"0.attention_layer.f.w_qkv\", \"scale (log\\u2082)\": -14.133340607357269}, {\"type\": \"grad_x\", \"op\": \"0.attention_layer.f.attn_dropout\", \"scale (log\\u2082)\": -10.582191127126155}, {\"type\": \"grad_x\", \"op\": \"0.attention_layer.f.w_o\", \"scale (log\\u2082)\": -13.152539435034242}, {\"type\": \"grad_x\", \"op\": \"0.attention_layer.f.residual_dropout\", \"scale (log\\u2082)\": -13.145628177095864}, {\"type\": \"grad_x\", \"op\": \"0.mlp_layer.f.ln\", \"scale (log\\u2082)\": -13.521635602038781}, {\"type\": \"grad_x\", \"op\": \"0.mlp_layer.f.linear_1\", \"scale (log\\u2082)\": -14.940303196683804}, {\"type\": \"grad_x\", \"op\": \"0.mlp_layer.f.act\", \"scale (log\\u2082)\": -15.276095375432288}, {\"type\": \"grad_x\", \"op\": \"0.mlp_layer.f.linear_2\", \"scale (log\\u2082)\": -14.592221243466977}, {\"type\": \"grad_x\", \"op\": \"0.mlp_layer.f.dropout\", \"scale (log\\u2082)\": -13.932812510124451}, {\"type\": \"grad_x\", \"op\": \"1.attention_layer.f.ln\", \"scale (log\\u2082)\": -14.818177284524175}, {\"type\": \"grad_x\", \"op\": \"1.attention_layer.f.w_qkv\", \"scale (log\\u2082)\": -15.255657528039785}, {\"type\": \"grad_x\", \"op\": \"1.attention_layer.f.attn_dropout\", \"scale (log\\u2082)\": -11.64000787291364}, {\"type\": \"grad_x\", \"op\": \"1.attention_layer.f.w_o\", \"scale (log\\u2082)\": -14.207624428464168}, {\"type\": \"grad_x\", \"op\": \"1.attention_layer.f.residual_dropout\", \"scale (log\\u2082)\": -14.215649714345629}, {\"type\": \"grad_x\", \"op\": \"1.mlp_layer.f.ln\", \"scale (log\\u2082)\": -15.208572374074606}, {\"type\": \"grad_x\", \"op\": \"1.mlp_layer.f.linear_1\", \"scale (log\\u2082)\": -15.45062400326793}, {\"type\": \"grad_x\", \"op\": \"1.mlp_layer.f.act\", \"scale (log\\u2082)\": -15.784700302228542}, {\"type\": \"grad_x\", \"op\": \"1.mlp_layer.f.linear_2\", \"scale (log\\u2082)\": -15.113483904479555}, {\"type\": \"grad_x\", \"op\": \"1.mlp_layer.f.dropout\", \"scale (log\\u2082)\": -14.454746204915795}, {\"type\": \"grad_x\", \"op\": \"2.attention_layer.f.ln\", \"scale (log\\u2082)\": -15.882946536606354}, {\"type\": \"grad_x\", \"op\": \"2.attention_layer.f.w_qkv\", \"scale (log\\u2082)\": -15.748564398765884}, {\"type\": \"grad_x\", \"op\": \"2.attention_layer.f.attn_dropout\", \"scale (log\\u2082)\": -12.012609593163056}, {\"type\": \"grad_x\", \"op\": \"2.attention_layer.f.w_o\", \"scale (log\\u2082)\": -14.573937607077259}, {\"type\": \"grad_x\", \"op\": \"2.attention_layer.f.residual_dropout\", \"scale (log\\u2082)\": -14.574485828818048}, {\"type\": \"grad_x\", \"op\": \"2.mlp_layer.f.ln\", \"scale (log\\u2082)\": -15.897997888721001}, {\"type\": \"grad_x\", \"op\": \"2.mlp_layer.f.linear_1\", \"scale (log\\u2082)\": -15.70167573858357}, {\"type\": \"grad_x\", \"op\": \"2.mlp_layer.f.act\", \"scale (log\\u2082)\": -16.04386529439781}, {\"type\": \"grad_x\", \"op\": \"2.mlp_layer.f.linear_2\", \"scale (log\\u2082)\": -15.37250363127985}, {\"type\": \"grad_x\", \"op\": \"2.mlp_layer.f.dropout\", \"scale (log\\u2082)\": -14.71126372674607}, {\"type\": \"grad_x\", \"op\": \"3.attention_layer.f.ln\", \"scale (log\\u2082)\": -16.513488640307028}, {\"type\": \"grad_x\", \"op\": \"3.attention_layer.f.w_qkv\", \"scale (log\\u2082)\": -16.048745348608517}, {\"type\": \"grad_x\", \"op\": \"3.attention_layer.f.attn_dropout\", \"scale (log\\u2082)\": -12.224535497255777}, {\"type\": \"grad_x\", \"op\": \"3.attention_layer.f.w_o\", \"scale (log\\u2082)\": -14.783229723365386}, {\"type\": \"grad_x\", \"op\": \"3.attention_layer.f.residual_dropout\", \"scale (log\\u2082)\": -14.780692755074979}, {\"type\": \"grad_x\", \"op\": \"3.mlp_layer.f.ln\", \"scale (log\\u2082)\": -16.35435695808089}, {\"type\": \"grad_x\", \"op\": \"3.mlp_layer.f.linear_1\", \"scale (log\\u2082)\": -15.885870856594558}, {\"type\": \"grad_x\", \"op\": \"3.mlp_layer.f.act\", \"scale (log\\u2082)\": -16.225212940398826}, {\"type\": \"grad_x\", \"op\": \"3.mlp_layer.f.linear_2\", \"scale (log\\u2082)\": -15.540100164225798}, {\"type\": \"grad_x\", \"op\": \"3.mlp_layer.f.dropout\", \"scale (log\\u2082)\": -14.874298033986818}, {\"type\": \"grad_x\", \"op\": \"4.attention_layer.f.ln\", \"scale (log\\u2082)\": -16.94876483367134}, {\"type\": \"grad_x\", \"op\": \"4.attention_layer.f.w_qkv\", \"scale (log\\u2082)\": -16.26995107549534}, {\"type\": \"grad_x\", \"op\": \"4.attention_layer.f.attn_dropout\", \"scale (log\\u2082)\": -12.375632224142825}, {\"type\": \"grad_x\", \"op\": \"4.attention_layer.f.w_o\", \"scale (log\\u2082)\": -14.925581655796929}, {\"type\": \"grad_x\", \"op\": \"4.attention_layer.f.residual_dropout\", \"scale (log\\u2082)\": -14.921286596378467}, {\"type\": \"grad_x\", \"op\": \"4.mlp_layer.f.ln\", \"scale (log\\u2082)\": -16.675774121505807}, {\"type\": \"grad_x\", \"op\": \"4.mlp_layer.f.linear_1\", \"scale (log\\u2082)\": -16.000938900767448}, {\"type\": \"grad_x\", \"op\": \"4.mlp_layer.f.act\", \"scale (log\\u2082)\": -16.33633614665704}, {\"type\": \"grad_x\", \"op\": \"4.mlp_layer.f.linear_2\", \"scale (log\\u2082)\": -15.659179814687239}, {\"type\": \"grad_x\", \"op\": \"4.mlp_layer.f.dropout\", \"scale (log\\u2082)\": -14.995949309135543}, {\"type\": \"grad_x\", \"op\": \"5.attention_layer.f.ln\", \"scale (log\\u2082)\": -17.291311490198524}, {\"type\": \"grad_x\", \"op\": \"5.attention_layer.f.w_qkv\", \"scale (log\\u2082)\": -16.43988422923289}, {\"type\": \"grad_x\", \"op\": \"5.attention_layer.f.attn_dropout\", \"scale (log\\u2082)\": -12.473083290768862}, {\"type\": \"grad_x\", \"op\": \"5.attention_layer.f.w_o\", \"scale (log\\u2082)\": -15.026001412429544}, {\"type\": \"grad_x\", \"op\": \"5.attention_layer.f.residual_dropout\", \"scale (log\\u2082)\": -15.028814148525592}, {\"type\": \"grad_x\", \"op\": \"5.mlp_layer.f.ln\", \"scale (log\\u2082)\": -16.940113280992634}, {\"type\": \"grad_x\", \"op\": \"5.mlp_layer.f.linear_1\", \"scale (log\\u2082)\": -16.096106297203857}, {\"type\": \"grad_x\", \"op\": \"5.mlp_layer.f.act\", \"scale (log\\u2082)\": -16.433900308644816}, {\"type\": \"grad_x\", \"op\": \"5.mlp_layer.f.linear_2\", \"scale (log\\u2082)\": -15.748108649743802}, {\"type\": \"grad_x\", \"op\": \"5.mlp_layer.f.dropout\", \"scale (log\\u2082)\": -15.085015852426574}, {\"type\": \"grad_x\", \"op\": \"lm_head\", \"scale (log\\u2082)\": -14.24235678444702}], \"data-42a504cc8b52eeb51286ea512abaaf6d\": [{\"type\": \"w\", \"op\": \"input_embeddings\", \"scale (log\\u2082)\": -4.289723360279215}, {\"type\": \"w\", \"op\": \"ln\", \"scale (log\\u2082)\": null}, {\"type\": \"w\", \"op\": \"0.attention_layer.f.ln\", \"scale (log\\u2082)\": null}, {\"type\": \"w\", \"op\": \"0.attention_layer.f.w_qkv\", \"scale (log\\u2082)\": -4.791188449031124}, {\"type\": \"w\", \"op\": \"0.attention_layer.f.w_o\", \"scale (log\\u2082)\": -4.296846885092625}, {\"type\": \"w\", \"op\": \"0.mlp_layer.f.ln\", \"scale (log\\u2082)\": null}, {\"type\": \"w\", \"op\": \"0.mlp_layer.f.linear_1\", \"scale (log\\u2082)\": -4.954933449707419}, {\"type\": \"w\", \"op\": \"0.mlp_layer.f.linear_2\", \"scale (log\\u2082)\": -4.949907076761701}, {\"type\": \"w\", \"op\": \"1.attention_layer.f.ln\", \"scale (log\\u2082)\": null}, {\"type\": \"w\", \"op\": \"1.attention_layer.f.w_qkv\", \"scale (log\\u2082)\": -4.789800143936264}, {\"type\": \"w\", \"op\": \"1.attention_layer.f.w_o\", \"scale (log\\u2082)\": -4.289484240514725}, {\"type\": \"w\", \"op\": \"1.mlp_layer.f.ln\", \"scale (log\\u2082)\": null}, {\"type\": \"w\", \"op\": \"1.mlp_layer.f.linear_1\", \"scale (log\\u2082)\": -4.955093817615777}, {\"type\": \"w\", \"op\": \"1.mlp_layer.f.linear_2\", \"scale (log\\u2082)\": -4.952578994878367}, {\"type\": \"w\", \"op\": \"2.attention_layer.f.ln\", \"scale (log\\u2082)\": null}, {\"type\": \"w\", \"op\": \"2.attention_layer.f.w_qkv\", \"scale (log\\u2082)\": -4.790309708215587}, {\"type\": \"w\", \"op\": \"2.attention_layer.f.w_o\", \"scale (log\\u2082)\": -4.291891961861038}, {\"type\": \"w\", \"op\": \"2.mlp_layer.f.ln\", \"scale (log\\u2082)\": null}, {\"type\": \"w\", \"op\": \"2.mlp_layer.f.linear_1\", \"scale (log\\u2082)\": -4.952814334106534}, {\"type\": \"w\", \"op\": \"2.mlp_layer.f.linear_2\", \"scale (log\\u2082)\": -4.952570340985094}, {\"type\": \"w\", \"op\": \"3.attention_layer.f.ln\", \"scale (log\\u2082)\": null}, {\"type\": \"w\", \"op\": \"3.attention_layer.f.w_qkv\", \"scale (log\\u2082)\": -4.796126600075669}, {\"type\": \"w\", \"op\": \"3.attention_layer.f.w_o\", \"scale (log\\u2082)\": -4.29705923030699}, {\"type\": \"w\", \"op\": \"3.mlp_layer.f.ln\", \"scale (log\\u2082)\": null}, {\"type\": \"w\", \"op\": \"3.mlp_layer.f.linear_1\", \"scale (log\\u2082)\": -4.951917288251376}, {\"type\": \"w\", \"op\": \"3.mlp_layer.f.linear_2\", \"scale (log\\u2082)\": -4.956714667046581}, {\"type\": \"w\", \"op\": \"4.attention_layer.f.ln\", \"scale (log\\u2082)\": null}, {\"type\": \"w\", \"op\": \"4.attention_layer.f.w_qkv\", \"scale (log\\u2082)\": -4.793879321144615}, {\"type\": \"w\", \"op\": \"4.attention_layer.f.w_o\", \"scale (log\\u2082)\": -4.29576071589017}, {\"type\": \"w\", \"op\": \"4.mlp_layer.f.ln\", \"scale (log\\u2082)\": null}, {\"type\": \"w\", \"op\": \"4.mlp_layer.f.linear_1\", \"scale (log\\u2082)\": -4.954202851116893}, {\"type\": \"w\", \"op\": \"4.mlp_layer.f.linear_2\", \"scale (log\\u2082)\": -4.955950303980775}, {\"type\": \"w\", \"op\": \"5.attention_layer.f.ln\", \"scale (log\\u2082)\": null}, {\"type\": \"w\", \"op\": \"5.attention_layer.f.w_qkv\", \"scale (log\\u2082)\": -4.7916194611962535}, {\"type\": \"w\", \"op\": \"5.attention_layer.f.w_o\", \"scale (log\\u2082)\": -4.291775848842703}, {\"type\": \"w\", \"op\": \"5.mlp_layer.f.ln\", \"scale (log\\u2082)\": null}, {\"type\": \"w\", \"op\": \"5.mlp_layer.f.linear_1\", \"scale (log\\u2082)\": -4.955925449374588}, {\"type\": \"w\", \"op\": \"5.mlp_layer.f.linear_2\", \"scale (log\\u2082)\": -4.955589203632399}, {\"type\": \"w\", \"op\": \"lm_head\", \"scale (log\\u2082)\": -4.292567350089183}, {\"type\": \"grad_w\", \"op\": \"ln\", \"scale (log\\u2082)\": -8.907116103985375}, {\"type\": \"grad_w\", \"op\": \"0.attention_layer.f.ln\", \"scale (log\\u2082)\": -9.156590314741743}, {\"type\": \"grad_w\", \"op\": \"0.attention_layer.f.w_qkv\", \"scale (log\\u2082)\": -9.421238283022682}, {\"type\": \"grad_w\", \"op\": \"0.attention_layer.f.w_o\", \"scale (log\\u2082)\": -9.216294125462976}, {\"type\": \"grad_w\", \"op\": \"0.mlp_layer.f.ln\", \"scale (log\\u2082)\": -9.627206660770133}, {\"type\": \"grad_w\", \"op\": \"0.mlp_layer.f.linear_1\", \"scale (log\\u2082)\": -9.998834256750099}, {\"type\": \"grad_w\", \"op\": \"0.mlp_layer.f.linear_2\", \"scale (log\\u2082)\": -9.929473649391317}, {\"type\": \"grad_w\", \"op\": \"1.attention_layer.f.ln\", \"scale (log\\u2082)\": -9.679835047504858}, {\"type\": \"grad_w\", \"op\": \"1.attention_layer.f.w_qkv\", \"scale (log\\u2082)\": -9.9888679701136}, {\"type\": \"grad_w\", \"op\": \"1.attention_layer.f.w_o\", \"scale (log\\u2082)\": -9.742258972906855}, {\"type\": \"grad_w\", \"op\": \"1.mlp_layer.f.ln\", \"scale (log\\u2082)\": -10.190051651626828}, {\"type\": \"grad_w\", \"op\": \"1.mlp_layer.f.linear_1\", \"scale (log\\u2082)\": -10.541616793217626}, {\"type\": \"grad_w\", \"op\": \"1.mlp_layer.f.linear_2\", \"scale (log\\u2082)\": -10.493394472846159}, {\"type\": \"grad_w\", \"op\": \"2.attention_layer.f.ln\", \"scale (log\\u2082)\": -10.065975440511208}, {\"type\": \"grad_w\", \"op\": \"2.attention_layer.f.w_qkv\", \"scale (log\\u2082)\": -10.383479648221819}, {\"type\": \"grad_w\", \"op\": \"2.attention_layer.f.w_o\", \"scale (log\\u2082)\": -10.100836165825461}, {\"type\": \"grad_w\", \"op\": \"2.mlp_layer.f.ln\", \"scale (log\\u2082)\": -10.502909437763693}, {\"type\": \"grad_w\", \"op\": \"2.mlp_layer.f.linear_1\", \"scale (log\\u2082)\": -10.863532308653278}, {\"type\": \"grad_w\", \"op\": \"2.mlp_layer.f.linear_2\", \"scale (log\\u2082)\": -10.814234697434857}, {\"type\": \"grad_w\", \"op\": \"3.attention_layer.f.ln\", \"scale (log\\u2082)\": -10.411782332450985}, {\"type\": \"grad_w\", \"op\": \"3.attention_layer.f.w_qkv\", \"scale (log\\u2082)\": -10.603396394628792}, {\"type\": \"grad_w\", \"op\": \"3.attention_layer.f.w_o\", \"scale (log\\u2082)\": -10.335587655798424}, {\"type\": \"grad_w\", \"op\": \"3.mlp_layer.f.ln\", \"scale (log\\u2082)\": -10.77246425580238}, {\"type\": \"grad_w\", \"op\": \"3.mlp_layer.f.linear_1\", \"scale (log\\u2082)\": -11.078615115261343}, {\"type\": \"grad_w\", \"op\": \"3.mlp_layer.f.linear_2\", \"scale (log\\u2082)\": -11.05401837748798}, {\"type\": \"grad_w\", \"op\": \"4.attention_layer.f.ln\", \"scale (log\\u2082)\": -10.562582283828066}, {\"type\": \"grad_w\", \"op\": \"4.attention_layer.f.w_qkv\", \"scale (log\\u2082)\": -10.823534936235832}, {\"type\": \"grad_w\", \"op\": \"4.attention_layer.f.w_o\", \"scale (log\\u2082)\": -10.55781782508717}, {\"type\": \"grad_w\", \"op\": \"4.mlp_layer.f.ln\", \"scale (log\\u2082)\": -10.975662959624026}, {\"type\": \"grad_w\", \"op\": \"4.mlp_layer.f.linear_1\", \"scale (log\\u2082)\": -11.236403646437859}, {\"type\": \"grad_w\", \"op\": \"4.mlp_layer.f.linear_2\", \"scale (log\\u2082)\": -11.236778918896517}, {\"type\": \"grad_w\", \"op\": \"5.attention_layer.f.ln\", \"scale (log\\u2082)\": -10.652161584799734}, {\"type\": \"grad_w\", \"op\": \"5.attention_layer.f.w_qkv\", \"scale (log\\u2082)\": -10.982454991313466}, {\"type\": \"grad_w\", \"op\": \"5.attention_layer.f.w_o\", \"scale (log\\u2082)\": -10.703920209167109}, {\"type\": \"grad_w\", \"op\": \"5.mlp_layer.f.ln\", \"scale (log\\u2082)\": -11.047182161999919}, {\"type\": \"grad_w\", \"op\": \"5.mlp_layer.f.linear_1\", \"scale (log\\u2082)\": -11.388271946609228}, {\"type\": \"grad_w\", \"op\": \"5.mlp_layer.f.linear_2\", \"scale (log\\u2082)\": -11.389195920380931}, {\"type\": \"grad_w\", \"op\": \"lm_head\", \"scale (log\\u2082)\": -9.142638844872483}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.LayerChart(...)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = NanoGPTModel(config)\n",
    "analyse_full_model(model, config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results clearly demonstrates the inadiquacy of the standard approach to model design when it comes to scale. Grad_x and grad_w values are very far from having unit scale, creating the potential for numerics issues.\n",
    "\n",
    "A key issue introduced by this layer is the cross-entropy loss definition. This typically uses a `1 / batch_size` term to average over labels, which has the effect of dramatically under-scaling gradients in the backward pass.\n",
    "\n",
    "We can also see here that a single loss scaling factor for all gradients isn't ideal—what we really need is per-op scaling! Again, we'll fix this for the unit-scaled implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnitScaledNanoGPTModel(NanoGPTModel):\n",
    "    config_class = NanoGPTConfig\n",
    "\n",
    "    def __init__(self, config: NanoGPTConfig) -> None:\n",
    "        super().__init__(config)\n",
    "        self.input_embeddings = UnitScaledEmbedding(config.vocab_size, config.hidden_size)\n",
    "        self.dropout = UnitScaledDropout(config.dropout)\n",
    "        self.ln = UnitScaledLayerNorm(config.hidden_size)\n",
    "        self.blocks = nn.ModuleList([\n",
    "            UnitScaledTransformerBlock(config)\n",
    "            for _ in range(config.num_hidden_layers)\n",
    "        ])\n",
    "        self.lm_head = UnitScaledLinear(\n",
    "            config.hidden_size, config.vocab_size, bias=False, scale_for=\"grad_x\"\n",
    "        )\n",
    "        self.loss_fn = UnitScaledCrossEntropyLoss()\n",
    "        self.apply(init_weights(unit_init))\n",
    "\n",
    "class UnitScaledEmbedding(nn.Embedding):\n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        batch_size = np.prod(input.shape)\n",
    "        weight = scaled(self.weight, beta=self.num_embeddings / batch_size)\n",
    "        return F.embedding(input, weight, self.padding_idx, self.max_norm,\n",
    "                           self.norm_type, self.scale_grad_by_freq, self.sparse)\n",
    "\n",
    "class UnitScaledCrossEntropyLoss(nn.CrossEntropyLoss):    \n",
    "    def forward(self, input: Tensor, target: Tensor) -> Tensor:\n",
    "        batch_size, seq_len = input.shape\n",
    "        input = scaled(input, beta=seq_len / (seq_len) ** 0.5)\n",
    "        loss = F.cross_entropy(input, target, reduction='sum')\n",
    "        return scaled(loss, alpha=1 / batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-83dbca10f8804078914599b27eb560a8\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-83dbca10f8804078914599b27eb560a8\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-83dbca10f8804078914599b27eb560a8\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.17.0?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"4.17.0\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}, \"axis\": {\"labelFontSize\": 12, \"titleFontSize\": 16}}, \"layer\": [{\"data\": {\"name\": \"data-f1e226dc312c308a266cbf595c083e64\"}, \"mark\": \"line\", \"encoding\": {\"color\": {\"field\": \"type\", \"legend\": {\"labelFontSize\": 12, \"symbolSize\": 100, \"title\": \"\"}, \"scale\": {\"range\": [\"#6C8EBF\", \"#FF8000\"]}, \"sort\": \"descending\", \"type\": \"nominal\"}, \"x\": {\"axis\": {\"orient\": \"top\", \"values\": [-14.0, -12.0, -10.0, -8.0, -6.0, -4.0, -2.0, 0.0, 2.0, 4.0, 6.0, 8.0, 10.0, 12.0, 14.0, 16.0]}, \"field\": \"scale (log\\u2082)\", \"scale\": {\"domain\": [-14, 16]}, \"type\": \"quantitative\"}, \"y\": {\"field\": \"op\", \"sort\": [\"input_embeddings\", \"dropout\", \"ln\", \"0.attention_layer.f.ln\", \"0.attention_layer.f.w_qkv\", \"0.attention_layer.f.qk_matmul\", \"0.attention_layer.f.attn_dropout\", \"0.attention_layer.f.qkv_matmul\", \"0.attention_layer.f.w_o\", \"0.attention_layer.f.residual_dropout\", \"0.mlp_layer.f.ln\", \"0.mlp_layer.f.linear_1\", \"0.mlp_layer.f.act\", \"0.mlp_layer.f.linear_2\", \"0.mlp_layer.f.dropout\", \"1.attention_layer.f.ln\", \"1.attention_layer.f.w_qkv\", \"1.attention_layer.f.qk_matmul\", \"1.attention_layer.f.attn_dropout\", \"1.attention_layer.f.qkv_matmul\", \"1.attention_layer.f.w_o\", \"1.attention_layer.f.residual_dropout\", \"1.mlp_layer.f.ln\", \"1.mlp_layer.f.linear_1\", \"1.mlp_layer.f.act\", \"1.mlp_layer.f.linear_2\", \"1.mlp_layer.f.dropout\", \"2.attention_layer.f.ln\", \"2.attention_layer.f.w_qkv\", \"2.attention_layer.f.qk_matmul\", \"2.attention_layer.f.attn_dropout\", \"2.attention_layer.f.qkv_matmul\", \"2.attention_layer.f.w_o\", \"2.attention_layer.f.residual_dropout\", \"2.mlp_layer.f.ln\", \"2.mlp_layer.f.linear_1\", \"2.mlp_layer.f.act\", \"2.mlp_layer.f.linear_2\", \"2.mlp_layer.f.dropout\", \"3.attention_layer.f.ln\", \"3.attention_layer.f.w_qkv\", \"3.attention_layer.f.qk_matmul\", \"3.attention_layer.f.attn_dropout\", \"3.attention_layer.f.qkv_matmul\", \"3.attention_layer.f.w_o\", \"3.attention_layer.f.residual_dropout\", \"3.mlp_layer.f.ln\", \"3.mlp_layer.f.linear_1\", \"3.mlp_layer.f.act\", \"3.mlp_layer.f.linear_2\", \"3.mlp_layer.f.dropout\", \"4.attention_layer.f.ln\", \"4.attention_layer.f.w_qkv\", \"4.attention_layer.f.qk_matmul\", \"4.attention_layer.f.attn_dropout\", \"4.attention_layer.f.qkv_matmul\", \"4.attention_layer.f.w_o\", \"4.attention_layer.f.residual_dropout\", \"4.mlp_layer.f.ln\", \"4.mlp_layer.f.linear_1\", \"4.mlp_layer.f.act\", \"4.mlp_layer.f.linear_2\", \"4.mlp_layer.f.dropout\", \"5.attention_layer.f.ln\", \"5.attention_layer.f.w_qkv\", \"5.attention_layer.f.qk_matmul\", \"5.attention_layer.f.attn_dropout\", \"5.attention_layer.f.qkv_matmul\", \"5.attention_layer.f.w_o\", \"5.attention_layer.f.residual_dropout\", \"5.mlp_layer.f.ln\", \"5.mlp_layer.f.linear_1\", \"5.mlp_layer.f.act\", \"5.mlp_layer.f.linear_2\", \"5.mlp_layer.f.dropout\", \"lm_head\"], \"title\": \"\", \"type\": \"ordinal\"}}}, {\"data\": {\"name\": \"data-df7d9d111df04cc58f15cfa962701cdd\"}, \"mark\": {\"type\": \"point\", \"size\": 100}, \"encoding\": {\"color\": {\"field\": \"type\", \"legend\": {\"labelFontSize\": 12, \"symbolSize\": 100, \"title\": \"\"}, \"scale\": {\"range\": [\"#5D8944\", \"#ED3434\"]}, \"sort\": \"descending\", \"type\": \"nominal\"}, \"shape\": {\"field\": \"type\", \"scale\": {\"range\": [\"square\", \"triangle-down\"]}, \"sort\": \"descending\", \"type\": \"nominal\"}, \"x\": {\"axis\": {\"orient\": \"top\", \"values\": [-14.0, -12.0, -10.0, -8.0, -6.0, -4.0, -2.0, 0.0, 2.0, 4.0, 6.0, 8.0, 10.0, 12.0, 14.0, 16.0]}, \"field\": \"scale (log\\u2082)\", \"scale\": {\"domain\": [-14, 16]}, \"type\": \"quantitative\"}, \"y\": {\"field\": \"op\", \"sort\": [\"input_embeddings\", \"dropout\", \"ln\", \"0.attention_layer.f.ln\", \"0.attention_layer.f.w_qkv\", \"0.attention_layer.f.qk_matmul\", \"0.attention_layer.f.attn_dropout\", \"0.attention_layer.f.qkv_matmul\", \"0.attention_layer.f.w_o\", \"0.attention_layer.f.residual_dropout\", \"0.mlp_layer.f.ln\", \"0.mlp_layer.f.linear_1\", \"0.mlp_layer.f.act\", \"0.mlp_layer.f.linear_2\", \"0.mlp_layer.f.dropout\", \"1.attention_layer.f.ln\", \"1.attention_layer.f.w_qkv\", \"1.attention_layer.f.qk_matmul\", \"1.attention_layer.f.attn_dropout\", \"1.attention_layer.f.qkv_matmul\", \"1.attention_layer.f.w_o\", \"1.attention_layer.f.residual_dropout\", \"1.mlp_layer.f.ln\", \"1.mlp_layer.f.linear_1\", \"1.mlp_layer.f.act\", \"1.mlp_layer.f.linear_2\", \"1.mlp_layer.f.dropout\", \"2.attention_layer.f.ln\", \"2.attention_layer.f.w_qkv\", \"2.attention_layer.f.qk_matmul\", \"2.attention_layer.f.attn_dropout\", \"2.attention_layer.f.qkv_matmul\", \"2.attention_layer.f.w_o\", \"2.attention_layer.f.residual_dropout\", \"2.mlp_layer.f.ln\", \"2.mlp_layer.f.linear_1\", \"2.mlp_layer.f.act\", \"2.mlp_layer.f.linear_2\", \"2.mlp_layer.f.dropout\", \"3.attention_layer.f.ln\", \"3.attention_layer.f.w_qkv\", \"3.attention_layer.f.qk_matmul\", \"3.attention_layer.f.attn_dropout\", \"3.attention_layer.f.qkv_matmul\", \"3.attention_layer.f.w_o\", \"3.attention_layer.f.residual_dropout\", \"3.mlp_layer.f.ln\", \"3.mlp_layer.f.linear_1\", \"3.mlp_layer.f.act\", \"3.mlp_layer.f.linear_2\", \"3.mlp_layer.f.dropout\", \"4.attention_layer.f.ln\", \"4.attention_layer.f.w_qkv\", \"4.attention_layer.f.qk_matmul\", \"4.attention_layer.f.attn_dropout\", \"4.attention_layer.f.qkv_matmul\", \"4.attention_layer.f.w_o\", \"4.attention_layer.f.residual_dropout\", \"4.mlp_layer.f.ln\", \"4.mlp_layer.f.linear_1\", \"4.mlp_layer.f.act\", \"4.mlp_layer.f.linear_2\", \"4.mlp_layer.f.dropout\", \"5.attention_layer.f.ln\", \"5.attention_layer.f.w_qkv\", \"5.attention_layer.f.qk_matmul\", \"5.attention_layer.f.attn_dropout\", \"5.attention_layer.f.qkv_matmul\", \"5.attention_layer.f.w_o\", \"5.attention_layer.f.residual_dropout\", \"5.mlp_layer.f.ln\", \"5.mlp_layer.f.linear_1\", \"5.mlp_layer.f.act\", \"5.mlp_layer.f.linear_2\", \"5.mlp_layer.f.dropout\", \"lm_head\"], \"title\": \"\", \"type\": \"ordinal\"}}}], \"resolve\": {\"scale\": {\"color\": \"independent\", \"shape\": \"independent\"}}, \"width\": 500, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.17.0.json\", \"datasets\": {\"data-f1e226dc312c308a266cbf595c083e64\": [{\"type\": \"x\", \"op\": \"input_embeddings\", \"scale (log\\u2082)\": -0.0031460667810760475}, {\"type\": \"x\", \"op\": \"dropout\", \"scale (log\\u2082)\": -0.003245348462992528}, {\"type\": \"x\", \"op\": \"ln\", \"scale (log\\u2082)\": -6.53535553522462e-06}, {\"type\": \"x\", \"op\": \"0.attention_layer.f.ln\", \"scale (log\\u2082)\": -5.5034553246245395e-06}, {\"type\": \"x\", \"op\": \"0.attention_layer.f.w_qkv\", \"scale (log\\u2082)\": 0.002964684377851132}, {\"type\": \"x\", \"op\": \"0.attention_layer.f.qk_matmul\", \"scale (log\\u2082)\": 0.009733641118371059}, {\"type\": \"x\", \"op\": \"0.attention_layer.f.attn_dropout\", \"scale (log\\u2082)\": -0.30288766749702084}, {\"type\": \"x\", \"op\": \"0.attention_layer.f.qkv_matmul\", \"scale (log\\u2082)\": -0.241256377822622}, {\"type\": \"x\", \"op\": \"0.attention_layer.f.w_o\", \"scale (log\\u2082)\": -0.23959840563659854}, {\"type\": \"x\", \"op\": \"0.attention_layer.f.residual_dropout\", \"scale (log\\u2082)\": -0.23964348426621998}, {\"type\": \"x\", \"op\": \"0.mlp_layer.f.ln\", \"scale (log\\u2082)\": -5.933413656011219e-06}, {\"type\": \"x\", \"op\": \"0.mlp_layer.f.linear_1\", \"scale (log\\u2082)\": -0.5011650332741654}, {\"type\": \"x\", \"op\": \"0.mlp_layer.f.act\", \"scale (log\\u2082)\": -0.6448810920103492}, {\"type\": \"x\", \"op\": \"0.mlp_layer.f.linear_2\", \"scale (log\\u2082)\": -0.0335139106687628}, {\"type\": \"x\", \"op\": \"0.mlp_layer.f.dropout\", \"scale (log\\u2082)\": -0.032759138690992166}, {\"type\": \"x\", \"op\": \"1.attention_layer.f.ln\", \"scale (log\\u2082)\": -5.847421979482836e-06}, {\"type\": \"x\", \"op\": \"1.attention_layer.f.w_qkv\", \"scale (log\\u2082)\": -0.004452529297916869}, {\"type\": \"x\", \"op\": \"1.attention_layer.f.qk_matmul\", \"scale (log\\u2082)\": -0.017024436692102626}, {\"type\": \"x\", \"op\": \"1.attention_layer.f.attn_dropout\", \"scale (log\\u2082)\": -0.29833406409892327}, {\"type\": \"x\", \"op\": \"1.attention_layer.f.qkv_matmul\", \"scale (log\\u2082)\": -0.22599338412737913}, {\"type\": \"x\", \"op\": \"1.attention_layer.f.w_o\", \"scale (log\\u2082)\": -0.21966399013792987}, {\"type\": \"x\", \"op\": \"1.attention_layer.f.residual_dropout\", \"scale (log\\u2082)\": -0.2201577335636503}, {\"type\": \"x\", \"op\": \"1.mlp_layer.f.ln\", \"scale (log\\u2082)\": -6.277380413380001e-06}, {\"type\": \"x\", \"op\": \"1.mlp_layer.f.linear_1\", \"scale (log\\u2082)\": -0.5004319268016376}, {\"type\": \"x\", \"op\": \"1.mlp_layer.f.act\", \"scale (log\\u2082)\": -0.638868489001674}, {\"type\": \"x\", \"op\": \"1.mlp_layer.f.linear_2\", \"scale (log\\u2082)\": -0.028552836294944775}, {\"type\": \"x\", \"op\": \"1.mlp_layer.f.dropout\", \"scale (log\\u2082)\": -0.029877955797723163}, {\"type\": \"x\", \"op\": \"2.attention_layer.f.ln\", \"scale (log\\u2082)\": -6.191388716349517e-06}, {\"type\": \"x\", \"op\": \"2.attention_layer.f.w_qkv\", \"scale (log\\u2082)\": -0.0010964619891973893}, {\"type\": \"x\", \"op\": \"2.attention_layer.f.qk_matmul\", \"scale (log\\u2082)\": -0.016610492645830394}, {\"type\": \"x\", \"op\": \"2.attention_layer.f.attn_dropout\", \"scale (log\\u2082)\": -0.3123085488394366}, {\"type\": \"x\", \"op\": \"2.attention_layer.f.qkv_matmul\", \"scale (log\\u2082)\": -0.21619243080640785}, {\"type\": \"x\", \"op\": \"2.attention_layer.f.w_o\", \"scale (log\\u2082)\": -0.2073785788904516}, {\"type\": \"x\", \"op\": \"2.attention_layer.f.residual_dropout\", \"scale (log\\u2082)\": -0.20838199989967707}, {\"type\": \"x\", \"op\": \"2.mlp_layer.f.ln\", \"scale (log\\u2082)\": -6.449363822817553e-06}, {\"type\": \"x\", \"op\": \"2.mlp_layer.f.linear_1\", \"scale (log\\u2082)\": -0.4991745283221958}, {\"type\": \"x\", \"op\": \"2.mlp_layer.f.act\", \"scale (log\\u2082)\": -0.6407745767464567}, {\"type\": \"x\", \"op\": \"2.mlp_layer.f.linear_2\", \"scale (log\\u2082)\": -0.026442845622424823}, {\"type\": \"x\", \"op\": \"2.mlp_layer.f.dropout\", \"scale (log\\u2082)\": -0.026456420892365325}, {\"type\": \"x\", \"op\": \"3.attention_layer.f.ln\", \"scale (log\\u2082)\": -6.363372115536013e-06}, {\"type\": \"x\", \"op\": \"3.attention_layer.f.w_qkv\", \"scale (log\\u2082)\": -0.000683190835495688}, {\"type\": \"x\", \"op\": \"3.attention_layer.f.qk_matmul\", \"scale (log\\u2082)\": 0.004071613767411747}, {\"type\": \"x\", \"op\": \"3.attention_layer.f.attn_dropout\", \"scale (log\\u2082)\": -0.3128651666103831}, {\"type\": \"x\", \"op\": \"3.attention_layer.f.qkv_matmul\", \"scale (log\\u2082)\": -0.21604369779865776}, {\"type\": \"x\", \"op\": \"3.attention_layer.f.w_o\", \"scale (log\\u2082)\": -0.22906193746515444}, {\"type\": \"x\", \"op\": \"3.attention_layer.f.residual_dropout\", \"scale (log\\u2082)\": -0.22924608903340332}, {\"type\": \"x\", \"op\": \"3.mlp_layer.f.ln\", \"scale (log\\u2082)\": -6.7073389754153415e-06}, {\"type\": \"x\", \"op\": \"3.mlp_layer.f.linear_1\", \"scale (log\\u2082)\": -0.49946540414726165}, {\"type\": \"x\", \"op\": \"3.mlp_layer.f.act\", \"scale (log\\u2082)\": -0.6488510117155556}, {\"type\": \"x\", \"op\": \"3.mlp_layer.f.linear_2\", \"scale (log\\u2082)\": -0.03628808330548603}, {\"type\": \"x\", \"op\": \"3.mlp_layer.f.dropout\", \"scale (log\\u2082)\": -0.036432620344315214}, {\"type\": \"x\", \"op\": \"4.attention_layer.f.ln\", \"scale (log\\u2082)\": -6.449363822817553e-06}, {\"type\": \"x\", \"op\": \"4.attention_layer.f.w_qkv\", \"scale (log\\u2082)\": 0.0030048451442851527}, {\"type\": \"x\", \"op\": \"4.attention_layer.f.qk_matmul\", \"scale (log\\u2082)\": -0.0007552875002121055}, {\"type\": \"x\", \"op\": \"4.attention_layer.f.attn_dropout\", \"scale (log\\u2082)\": -0.31687376814124285}, {\"type\": \"x\", \"op\": \"4.attention_layer.f.qkv_matmul\", \"scale (log\\u2082)\": -0.20543430890491188}, {\"type\": \"x\", \"op\": \"4.attention_layer.f.w_o\", \"scale (log\\u2082)\": -0.20463160850283085}, {\"type\": \"x\", \"op\": \"4.attention_layer.f.residual_dropout\", \"scale (log\\u2082)\": -0.2054660375324159}, {\"type\": \"x\", \"op\": \"4.mlp_layer.f.ln\", \"scale (log\\u2082)\": -6.7073389754153415e-06}, {\"type\": \"x\", \"op\": \"4.mlp_layer.f.linear_1\", \"scale (log\\u2082)\": -0.5009174997406671}, {\"type\": \"x\", \"op\": \"4.mlp_layer.f.act\", \"scale (log\\u2082)\": -0.6347631396910394}, {\"type\": \"x\", \"op\": \"4.mlp_layer.f.linear_2\", \"scale (log\\u2082)\": -0.03412061894553794}, {\"type\": \"x\", \"op\": \"4.mlp_layer.f.dropout\", \"scale (log\\u2082)\": -0.0342176525517929}, {\"type\": \"x\", \"op\": \"5.attention_layer.f.ln\", \"scale (log\\u2082)\": -6.53535553522462e-06}, {\"type\": \"x\", \"op\": \"5.attention_layer.f.w_qkv\", \"scale (log\\u2082)\": 0.005548312690963752}, {\"type\": \"x\", \"op\": \"5.attention_layer.f.qk_matmul\", \"scale (log\\u2082)\": 0.0013540716957978491}, {\"type\": \"x\", \"op\": \"5.attention_layer.f.attn_dropout\", \"scale (log\\u2082)\": -0.32465517688295076}, {\"type\": \"x\", \"op\": \"5.attention_layer.f.qkv_matmul\", \"scale (log\\u2082)\": -0.199436071443369}, {\"type\": \"x\", \"op\": \"5.attention_layer.f.w_o\", \"scale (log\\u2082)\": -0.20227614434229216}, {\"type\": \"x\", \"op\": \"5.attention_layer.f.residual_dropout\", \"scale (log\\u2082)\": -0.20256436804676808}, {\"type\": \"x\", \"op\": \"5.mlp_layer.f.ln\", \"scale (log\\u2082)\": -6.7073389754153415e-06}, {\"type\": \"x\", \"op\": \"5.mlp_layer.f.linear_1\", \"scale (log\\u2082)\": -0.4943547550825509}, {\"type\": \"x\", \"op\": \"5.mlp_layer.f.act\", \"scale (log\\u2082)\": -0.6292113732629798}, {\"type\": \"x\", \"op\": \"5.mlp_layer.f.linear_2\", \"scale (log\\u2082)\": -0.012497087715666688}, {\"type\": \"x\", \"op\": \"5.mlp_layer.f.dropout\", \"scale (log\\u2082)\": -0.012265165081055797}, {\"type\": \"x\", \"op\": \"lm_head\", \"scale (log\\u2082)\": -0.00046726653308905546}, {\"type\": \"grad_x\", \"op\": \"dropout\", \"scale (log\\u2082)\": 0.03458405515095833}, {\"type\": \"grad_x\", \"op\": \"ln\", \"scale (log\\u2082)\": 0.059110398433944694}, {\"type\": \"grad_x\", \"op\": \"0.attention_layer.f.ln\", \"scale (log\\u2082)\": -0.10959840435773023}, {\"type\": \"grad_x\", \"op\": \"0.attention_layer.f.w_qkv\", \"scale (log\\u2082)\": -0.11375296841398398}, {\"type\": \"grad_x\", \"op\": \"0.attention_layer.f.attn_dropout\", \"scale (log\\u2082)\": 1.0650206336906656}, {\"type\": \"grad_x\", \"op\": \"0.attention_layer.f.w_o\", \"scale (log\\u2082)\": 0.06685207830590686}, {\"type\": \"grad_x\", \"op\": \"0.attention_layer.f.residual_dropout\", \"scale (log\\u2082)\": 0.06582171467385976}, {\"type\": \"grad_x\", \"op\": \"0.mlp_layer.f.ln\", \"scale (log\\u2082)\": 0.11889292667618476}, {\"type\": \"grad_x\", \"op\": \"0.mlp_layer.f.linear_1\", \"scale (log\\u2082)\": 0.07450000999863637}, {\"type\": \"grad_x\", \"op\": \"0.mlp_layer.f.act\", \"scale (log\\u2082)\": -0.4265193410464641}, {\"type\": \"grad_x\", \"op\": \"0.mlp_layer.f.linear_2\", \"scale (log\\u2082)\": -0.4458870991584995}, {\"type\": \"grad_x\", \"op\": \"0.mlp_layer.f.dropout\", \"scale (log\\u2082)\": 0.05108942666312315}, {\"type\": \"grad_x\", \"op\": \"1.attention_layer.f.ln\", \"scale (log\\u2082)\": -0.07180451457577004}, {\"type\": \"grad_x\", \"op\": \"1.attention_layer.f.w_qkv\", \"scale (log\\u2082)\": -0.10483403552591192}, {\"type\": \"grad_x\", \"op\": \"1.attention_layer.f.attn_dropout\", \"scale (log\\u2082)\": 1.0821857575960612}, {\"type\": \"grad_x\", \"op\": \"1.attention_layer.f.w_o\", \"scale (log\\u2082)\": 0.07571905729186883}, {\"type\": \"grad_x\", \"op\": \"1.attention_layer.f.residual_dropout\", \"scale (log\\u2082)\": 0.07669769621529404}, {\"type\": \"grad_x\", \"op\": \"1.mlp_layer.f.ln\", \"scale (log\\u2082)\": 0.15359254113185458}, {\"type\": \"grad_x\", \"op\": \"1.mlp_layer.f.linear_1\", \"scale (log\\u2082)\": 0.08328648591086087}, {\"type\": \"grad_x\", \"op\": \"1.mlp_layer.f.act\", \"scale (log\\u2082)\": -0.4203865473761586}, {\"type\": \"grad_x\", \"op\": \"1.mlp_layer.f.linear_2\", \"scale (log\\u2082)\": -0.44161420556247943}, {\"type\": \"grad_x\", \"op\": \"1.mlp_layer.f.dropout\", \"scale (log\\u2082)\": 0.0606199120648053}, {\"type\": \"grad_x\", \"op\": \"2.attention_layer.f.ln\", \"scale (log\\u2082)\": -0.059727451014266984}, {\"type\": \"grad_x\", \"op\": \"2.attention_layer.f.w_qkv\", \"scale (log\\u2082)\": -0.11228182410335154}, {\"type\": \"grad_x\", \"op\": \"2.attention_layer.f.attn_dropout\", \"scale (log\\u2082)\": 1.0867575804932081}, {\"type\": \"grad_x\", \"op\": \"2.attention_layer.f.w_o\", \"scale (log\\u2082)\": 0.08142606093724045}, {\"type\": \"grad_x\", \"op\": \"2.attention_layer.f.residual_dropout\", \"scale (log\\u2082)\": 0.08593563659362982}, {\"type\": \"grad_x\", \"op\": \"2.mlp_layer.f.ln\", \"scale (log\\u2082)\": 0.17130295610232943}, {\"type\": \"grad_x\", \"op\": \"2.mlp_layer.f.linear_1\", \"scale (log\\u2082)\": 0.07949861866799476}, {\"type\": \"grad_x\", \"op\": \"2.mlp_layer.f.act\", \"scale (log\\u2082)\": -0.4201478891362145}, {\"type\": \"grad_x\", \"op\": \"2.mlp_layer.f.linear_2\", \"scale (log\\u2082)\": -0.43966168422113755}, {\"type\": \"grad_x\", \"op\": \"2.mlp_layer.f.dropout\", \"scale (log\\u2082)\": 0.06152067670992693}, {\"type\": \"grad_x\", \"op\": \"3.attention_layer.f.ln\", \"scale (log\\u2082)\": -0.06588245793906054}, {\"type\": \"grad_x\", \"op\": \"3.attention_layer.f.w_qkv\", \"scale (log\\u2082)\": -0.1342682392046327}, {\"type\": \"grad_x\", \"op\": \"3.attention_layer.f.attn_dropout\", \"scale (log\\u2082)\": 1.0803107354745942}, {\"type\": \"grad_x\", \"op\": \"3.attention_layer.f.w_o\", \"scale (log\\u2082)\": 0.09288792748248337}, {\"type\": \"grad_x\", \"op\": \"3.attention_layer.f.residual_dropout\", \"scale (log\\u2082)\": 0.08944821810974211}, {\"type\": \"grad_x\", \"op\": \"3.mlp_layer.f.ln\", \"scale (log\\u2082)\": 0.1861341935246384}, {\"type\": \"grad_x\", \"op\": \"3.mlp_layer.f.linear_1\", \"scale (log\\u2082)\": 0.07388789401009604}, {\"type\": \"grad_x\", \"op\": \"3.mlp_layer.f.act\", \"scale (log\\u2082)\": -0.4259197665091063}, {\"type\": \"grad_x\", \"op\": \"3.mlp_layer.f.linear_2\", \"scale (log\\u2082)\": -0.437405747638449}, {\"type\": \"grad_x\", \"op\": \"3.mlp_layer.f.dropout\", \"scale (log\\u2082)\": 0.0620616234765197}, {\"type\": \"grad_x\", \"op\": \"4.attention_layer.f.ln\", \"scale (log\\u2082)\": -0.07096468157452208}, {\"type\": \"grad_x\", \"op\": \"4.attention_layer.f.w_qkv\", \"scale (log\\u2082)\": -0.14942464642475342}, {\"type\": \"grad_x\", \"op\": \"4.attention_layer.f.attn_dropout\", \"scale (log\\u2082)\": 1.0910127244222898}, {\"type\": \"grad_x\", \"op\": \"4.attention_layer.f.w_o\", \"scale (log\\u2082)\": 0.0945010615089536}, {\"type\": \"grad_x\", \"op\": \"4.attention_layer.f.residual_dropout\", \"scale (log\\u2082)\": 0.09051951615869595}, {\"type\": \"grad_x\", \"op\": \"4.mlp_layer.f.ln\", \"scale (log\\u2082)\": 0.19962382385379687}, {\"type\": \"grad_x\", \"op\": \"4.mlp_layer.f.linear_1\", \"scale (log\\u2082)\": 0.08205302233893179}, {\"type\": \"grad_x\", \"op\": \"4.mlp_layer.f.act\", \"scale (log\\u2082)\": -0.4181243335420257}, {\"type\": \"grad_x\", \"op\": \"4.mlp_layer.f.linear_2\", \"scale (log\\u2082)\": -0.443472779512258}, {\"type\": \"grad_x\", \"op\": \"4.mlp_layer.f.dropout\", \"scale (log\\u2082)\": 0.0593903444857623}, {\"type\": \"grad_x\", \"op\": \"5.attention_layer.f.ln\", \"scale (log\\u2082)\": -0.08065626921153508}, {\"type\": \"grad_x\", \"op\": \"5.attention_layer.f.w_qkv\", \"scale (log\\u2082)\": -0.16724555074008776}, {\"type\": \"grad_x\", \"op\": \"5.attention_layer.f.attn_dropout\", \"scale (log\\u2082)\": 1.091904560669095}, {\"type\": \"grad_x\", \"op\": \"5.attention_layer.f.w_o\", \"scale (log\\u2082)\": 0.08816387451742203}, {\"type\": \"grad_x\", \"op\": \"5.attention_layer.f.residual_dropout\", \"scale (log\\u2082)\": 0.09115206477367033}, {\"type\": \"grad_x\", \"op\": \"5.mlp_layer.f.ln\", \"scale (log\\u2082)\": 0.20727548202099957}, {\"type\": \"grad_x\", \"op\": \"5.mlp_layer.f.linear_1\", \"scale (log\\u2082)\": 0.08544328277746358}, {\"type\": \"grad_x\", \"op\": \"5.mlp_layer.f.act\", \"scale (log\\u2082)\": -0.41912109651708085}, {\"type\": \"grad_x\", \"op\": \"5.mlp_layer.f.linear_2\", \"scale (log\\u2082)\": -0.4421782792864119}, {\"type\": \"grad_x\", \"op\": \"5.mlp_layer.f.dropout\", \"scale (log\\u2082)\": 0.05930864414657724}, {\"type\": \"grad_x\", \"op\": \"lm_head\", \"scale (log\\u2082)\": -0.039067600705500465}], \"data-df7d9d111df04cc58f15cfa962701cdd\": [{\"type\": \"w\", \"op\": \"input_embeddings\", \"scale (log\\u2082)\": -0.003961464801583951}, {\"type\": \"w\", \"op\": \"ln\", \"scale (log\\u2082)\": null}, {\"type\": \"w\", \"op\": \"0.attention_layer.f.ln\", \"scale (log\\u2082)\": null}, {\"type\": \"w\", \"op\": \"0.attention_layer.f.w_qkv\", \"scale (log\\u2082)\": 0.0005486922851118535}, {\"type\": \"w\", \"op\": \"0.attention_layer.f.w_o\", \"scale (log\\u2082)\": 0.0003439243043825912}, {\"type\": \"w\", \"op\": \"0.mlp_layer.f.ln\", \"scale (log\\u2082)\": null}, {\"type\": \"w\", \"op\": \"0.mlp_layer.f.linear_1\", \"scale (log\\u2082)\": 0.0011085181497108828}, {\"type\": \"w\", \"op\": \"0.mlp_layer.f.linear_2\", \"scale (log\\u2082)\": 0.0023130312811672554}, {\"type\": \"w\", \"op\": \"1.attention_layer.f.ln\", \"scale (log\\u2082)\": null}, {\"type\": \"w\", \"op\": \"1.attention_layer.f.w_qkv\", \"scale (log\\u2082)\": -0.0037751386197997856}, {\"type\": \"w\", \"op\": \"1.attention_layer.f.w_o\", \"scale (log\\u2082)\": 3.439648916876765e-06}, {\"type\": \"w\", \"op\": \"1.mlp_layer.f.ln\", \"scale (log\\u2082)\": null}, {\"type\": \"w\", \"op\": \"1.mlp_layer.f.linear_1\", \"scale (log\\u2082)\": -0.00029928085261473974}, {\"type\": \"w\", \"op\": \"1.mlp_layer.f.linear_2\", \"scale (log\\u2082)\": -0.001696660221312708}, {\"type\": \"w\", \"op\": \"2.attention_layer.f.ln\", \"scale (log\\u2082)\": null}, {\"type\": \"w\", \"op\": \"2.attention_layer.f.w_qkv\", \"scale (log\\u2082)\": 0.0016933788764870047}, {\"type\": \"w\", \"op\": \"2.attention_layer.f.w_o\", \"scale (log\\u2082)\": 0.0006033609361057678}, {\"type\": \"w\", \"op\": \"2.mlp_layer.f.ln\", \"scale (log\\u2082)\": null}, {\"type\": \"w\", \"op\": \"2.mlp_layer.f.linear_1\", \"scale (log\\u2082)\": -0.0002473322517595917}, {\"type\": \"w\", \"op\": \"2.mlp_layer.f.linear_2\", \"scale (log\\u2082)\": -0.0008139654881827067}, {\"type\": \"w\", \"op\": \"3.attention_layer.f.ln\", \"scale (log\\u2082)\": null}, {\"type\": \"w\", \"op\": \"3.attention_layer.f.w_qkv\", \"scale (log\\u2082)\": 0.001529662387231861}, {\"type\": \"w\", \"op\": \"3.attention_layer.f.w_o\", \"scale (log\\u2082)\": 0.0003989445853795659}, {\"type\": \"w\", \"op\": \"3.mlp_layer.f.ln\", \"scale (log\\u2082)\": null}, {\"type\": \"w\", \"op\": \"3.mlp_layer.f.linear_1\", \"scale (log\\u2082)\": 0.0013473706491973752}, {\"type\": \"w\", \"op\": \"3.mlp_layer.f.linear_2\", \"scale (log\\u2082)\": 0.0008467660092181688}, {\"type\": \"w\", \"op\": \"4.attention_layer.f.ln\", \"scale (log\\u2082)\": null}, {\"type\": \"w\", \"op\": \"4.attention_layer.f.w_qkv\", \"scale (log\\u2082)\": -0.0011777878671093308}, {\"type\": \"w\", \"op\": \"4.attention_layer.f.w_o\", \"scale (log\\u2082)\": 0.001642015472231903}, {\"type\": \"w\", \"op\": \"4.mlp_layer.f.ln\", \"scale (log\\u2082)\": null}, {\"type\": \"w\", \"op\": \"4.mlp_layer.f.linear_1\", \"scale (log\\u2082)\": -0.00046614828416686636}, {\"type\": \"w\", \"op\": \"4.mlp_layer.f.linear_2\", \"scale (log\\u2082)\": -0.00114749451754341}, {\"type\": \"w\", \"op\": \"5.attention_layer.f.ln\", \"scale (log\\u2082)\": null}, {\"type\": \"w\", \"op\": \"5.attention_layer.f.w_qkv\", \"scale (log\\u2082)\": 0.0019951663560706678}, {\"type\": \"w\", \"op\": \"5.attention_layer.f.w_o\", \"scale (log\\u2082)\": -0.002185317984564731}, {\"type\": \"w\", \"op\": \"5.mlp_layer.f.ln\", \"scale (log\\u2082)\": null}, {\"type\": \"w\", \"op\": \"5.mlp_layer.f.linear_1\", \"scale (log\\u2082)\": -0.00012073687290400767}, {\"type\": \"w\", \"op\": \"5.mlp_layer.f.linear_2\", \"scale (log\\u2082)\": 0.0005284059065029234}, {\"type\": \"w\", \"op\": \"lm_head\", \"scale (log\\u2082)\": 0.0015262263751894879}, {\"type\": \"grad_w\", \"op\": \"ln\", \"scale (log\\u2082)\": 0.17820193656238892}, {\"type\": \"grad_w\", \"op\": \"0.attention_layer.f.ln\", \"scale (log\\u2082)\": -0.07624590482304071}, {\"type\": \"grad_w\", \"op\": \"0.attention_layer.f.w_qkv\", \"scale (log\\u2082)\": -0.896421799029056}, {\"type\": \"grad_w\", \"op\": \"0.attention_layer.f.w_o\", \"scale (log\\u2082)\": -0.11667580364626208}, {\"type\": \"grad_w\", \"op\": \"0.mlp_layer.f.ln\", \"scale (log\\u2082)\": 0.06615293044053676}, {\"type\": \"grad_w\", \"op\": \"0.mlp_layer.f.linear_1\", \"scale (log\\u2082)\": -0.42416776595500316}, {\"type\": \"grad_w\", \"op\": \"0.mlp_layer.f.linear_2\", \"scale (log\\u2082)\": -0.41632851990963043}, {\"type\": \"grad_w\", \"op\": \"1.attention_layer.f.ln\", \"scale (log\\u2082)\": -0.07206293272175092}, {\"type\": \"grad_w\", \"op\": \"1.attention_layer.f.w_qkv\", \"scale (log\\u2082)\": -0.8864239402592843}, {\"type\": \"grad_w\", \"op\": \"1.attention_layer.f.w_o\", \"scale (log\\u2082)\": -0.08834807638646298}, {\"type\": \"grad_w\", \"op\": \"1.mlp_layer.f.ln\", \"scale (log\\u2082)\": 0.042435899415813064}, {\"type\": \"grad_w\", \"op\": \"1.mlp_layer.f.linear_1\", \"scale (log\\u2082)\": -0.4139725172601958}, {\"type\": \"grad_w\", \"op\": \"1.mlp_layer.f.linear_2\", \"scale (log\\u2082)\": -0.38436576105096065}, {\"type\": \"grad_w\", \"op\": \"2.attention_layer.f.ln\", \"scale (log\\u2082)\": -0.11786531597187734}, {\"type\": \"grad_w\", \"op\": \"2.attention_layer.f.w_qkv\", \"scale (log\\u2082)\": -0.86242327603701}, {\"type\": \"grad_w\", \"op\": \"2.attention_layer.f.w_o\", \"scale (log\\u2082)\": -0.06295751344401279}, {\"type\": \"grad_w\", \"op\": \"2.mlp_layer.f.ln\", \"scale (log\\u2082)\": 0.025464197306232665}, {\"type\": \"grad_w\", \"op\": \"2.mlp_layer.f.linear_1\", \"scale (log\\u2082)\": -0.40600190904413014}, {\"type\": \"grad_w\", \"op\": \"2.mlp_layer.f.linear_2\", \"scale (log\\u2082)\": -0.4027430556904992}, {\"type\": \"grad_w\", \"op\": \"3.attention_layer.f.ln\", \"scale (log\\u2082)\": -0.07037993336968218}, {\"type\": \"grad_w\", \"op\": \"3.attention_layer.f.w_qkv\", \"scale (log\\u2082)\": -0.8584149779976744}, {\"type\": \"grad_w\", \"op\": \"3.attention_layer.f.w_o\", \"scale (log\\u2082)\": -0.07349098729134367}, {\"type\": \"grad_w\", \"op\": \"3.mlp_layer.f.ln\", \"scale (log\\u2082)\": 0.0463701698519597}, {\"type\": \"grad_w\", \"op\": \"3.mlp_layer.f.linear_1\", \"scale (log\\u2082)\": -0.4084272066614621}, {\"type\": \"grad_w\", \"op\": \"3.mlp_layer.f.linear_2\", \"scale (log\\u2082)\": -0.4071194022390818}, {\"type\": \"grad_w\", \"op\": \"4.attention_layer.f.ln\", \"scale (log\\u2082)\": -0.07116260140391767}, {\"type\": \"grad_w\", \"op\": \"4.attention_layer.f.w_qkv\", \"scale (log\\u2082)\": -0.8591145426545164}, {\"type\": \"grad_w\", \"op\": \"4.attention_layer.f.w_o\", \"scale (log\\u2082)\": -0.06615791259968519}, {\"type\": \"grad_w\", \"op\": \"4.mlp_layer.f.ln\", \"scale (log\\u2082)\": 0.17907536192671236}, {\"type\": \"grad_w\", \"op\": \"4.mlp_layer.f.linear_1\", \"scale (log\\u2082)\": -0.3888028186383527}, {\"type\": \"grad_w\", \"op\": \"4.mlp_layer.f.linear_2\", \"scale (log\\u2082)\": -0.381100150382989}, {\"type\": \"grad_w\", \"op\": \"5.attention_layer.f.ln\", \"scale (log\\u2082)\": 0.053675693842670076}, {\"type\": \"grad_w\", \"op\": \"5.attention_layer.f.w_qkv\", \"scale (log\\u2082)\": -0.8599900220409731}, {\"type\": \"grad_w\", \"op\": \"5.attention_layer.f.w_o\", \"scale (log\\u2082)\": -0.06329179831896445}, {\"type\": \"grad_w\", \"op\": \"5.mlp_layer.f.ln\", \"scale (log\\u2082)\": 0.1307113184456126}, {\"type\": \"grad_w\", \"op\": \"5.mlp_layer.f.linear_1\", \"scale (log\\u2082)\": -0.388033027889844}, {\"type\": \"grad_w\", \"op\": \"5.mlp_layer.f.linear_2\", \"scale (log\\u2082)\": -0.37740754744772986}, {\"type\": \"grad_w\", \"op\": \"lm_head\", \"scale (log\\u2082)\": 0.008187861001519912}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.LayerChart(...)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = UnitScaledNanoGPTModel(config)\n",
    "analyse_full_model(model, config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Success! 🥂 🎊 🍾 We've managed to keep every tensor in the model at unit scale for the forward and backward pass. All that was required was to re-implement standard layers with the right scaling factors.\n",
    "\n",
    "Of course, this only gives us the right scaling *at initialisation*. Scales inevitably drift throughout training, but we've given ourselves headroom by starting in the ideal place. The results in our paper show that this is sufficient to enable accurate, out-the-box training of many models, up to the size of BERT Large (we haven't tested anything larger—yet!)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "### Regular model\n",
    "\n",
    "To show that this really does work, let's train both our models.\n",
    "\n",
    "As in Karpathy's original NanoGPT, we'll train on the small Shakespeare dataset for a few minutes and see if we can get something that captures the rough style. Starting with the regular (non-unit-scaled) model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'poptorch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# TODO: hide\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39moptimum\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mgraphcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mgeneration_utils\u001b[39;00m \u001b[39mimport\u001b[39;00m IPUGenerationMixin\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39moptimum\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mgraphcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodeling_utils\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m      4\u001b[0m     PipelineMixin,\n\u001b[1;32m      5\u001b[0m     outline_attribute,\n\u001b[1;32m      6\u001b[0m     register,\n\u001b[1;32m      7\u001b[0m     tied_weight_model,\n\u001b[1;32m      8\u001b[0m )\n\u001b[1;32m     10\u001b[0m \u001b[39m@tied_weight_model\u001b[39m(NanoGPTModel)\n\u001b[1;32m     11\u001b[0m \u001b[39m@register\u001b[39m(NanoGPTModel)\n\u001b[1;32m     12\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mPipelinedNanoGPTModel\u001b[39;00m(NanoGPTModel, PipelineMixin, IPUGenerationMixin):\n",
      "File \u001b[0;32m~/Projects/graphcore/unit-scaling-notebook/.venv/lib/python3.10/site-packages/optimum/graphcore/__init__.py:19\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# flake8: noqa\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m# There's no way to ignore \"F401 '...' imported but unused\" warnings in this\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m# module, but to preserve other warnings. So, don't check this module at all.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[39m#  See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[39m#  limitations under the License.\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpoptorch\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mipu_configuration\u001b[39;00m \u001b[39mimport\u001b[39;00m IPUConfig\n\u001b[1;32m     22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbart\u001b[39;00m \u001b[39mimport\u001b[39;00m PipelinedBartForConditionalGeneration, PipelinedBartForSequenceClassification\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'poptorch'"
     ]
    }
   ],
   "source": [
    "# TODO: hide\n",
    "from optimum.graphcore.generation_utils import IPUGenerationMixin\n",
    "from optimum.graphcore.modeling_utils import (\n",
    "    PipelineMixin,\n",
    "    outline_attribute,\n",
    "    register,\n",
    "    tied_weight_model,\n",
    ")\n",
    "\n",
    "@tied_weight_model(NanoGPTModel)\n",
    "@register(NanoGPTModel)\n",
    "class PipelinedNanoGPTModel(NanoGPTModel, PipelineMixin, IPUGenerationMixin):\n",
    "    def parallelize(self):\n",
    "        self._hooks = [outline_attribute(self.ln, \"LayerNorm\")]\n",
    "\n",
    "    def deparallelize(self):\n",
    "        pass\n",
    "\n",
    "@tied_weight_model(UnitScaledNanoGPTModel)\n",
    "@register(UnitScaledNanoGPTModel)\n",
    "class PipelinedUnitScaledNanoGPTModel(\n",
    "    UnitScaledNanoGPTModel, PipelineMixin, IPUGenerationMixin\n",
    "):\n",
    "    def parallelize(self):\n",
    "        self._hooks = [outline_attribute(self.ln, \"UnitScaledLayerNorm\")]\n",
    "\n",
    "    def deparallelize(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import poptorch_experimental_addons as pea\n",
    "\n",
    "def scaled(X, alpha=1.0, beta=1.0):\n",
    "  # Forward: Y = X * alpha\n",
    "  # Backward: grad_X = grad_Y * beta\n",
    "  return pea.autograd_proxy(X * alpha, X * beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "from datasets.load import load_dataset\n",
    "from optimum.graphcore import (\n",
    "    IPUConfig,\n",
    "    IPUTrainer,\n",
    "    IPUTrainingArguments,\n",
    "    pipeline,\n",
    "    pipelines,\n",
    ")\n",
    "from transformers import AutoTokenizer, DataCollatorForLanguageModeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset tiny_shakespeare (/nethome/charlieb/.cache/huggingface/datasets/tiny_shakespeare/default/1.0.0/b5b13969f09fe8707337f6cb296314fbe06960bd9a868dca39e713e163d27b5e)\n",
      "100%|██████████| 3/3 [00:00<00:00, 1062.39it/s]\n",
      "Loading cached processed dataset at /nethome/charlieb/.cache/huggingface/datasets/tiny_shakespeare/default/1.0.0/b5b13969f09fe8707337f6cb296314fbe06960bd9a868dca39e713e163d27b5e/cache-bebf307cc6e838af.arrow\n",
      "Loading cached processed dataset at /nethome/charlieb/.cache/huggingface/datasets/tiny_shakespeare/default/1.0.0/b5b13969f09fe8707337f6cb296314fbe06960bd9a868dca39e713e163d27b5e/cache-c2f9b4618a37a1c8.arrow\n",
      "Loading cached processed dataset at /nethome/charlieb/.cache/huggingface/datasets/tiny_shakespeare/default/1.0.0/b5b13969f09fe8707337f6cb296314fbe06960bd9a868dca39e713e163d27b5e/cache-6df33a0afb6a9efa.arrow\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"tiny_shakespeare\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/byt5-small\")\n",
    "\n",
    "config = NanoGPTConfig(vocab_size=len(tokenizer), eos_token_id=tokenizer.eos_token_id)\n",
    "batch_sz = 16\n",
    "seq_len = 128\n",
    "ipu_config = IPUConfig(\n",
    "    gradient_accumulation_steps=5 * 64 // batch_sz,\n",
    "    layers_per_ipu=[config.num_hidden_layers],\n",
    "    executable_cache_dir=\"./exe_cache\",\n",
    ")\n",
    "\n",
    "def split_and_tokenize(data, seq_len, batch_sz):\n",
    "    tokens = tokenizer(data[\"text\"])\n",
    "    seqs = [\n",
    "        tokens[\"input_ids\"][0][i : i + seq_len]\n",
    "        for i in range(0, len(tokens[\"input_ids\"][0]), seq_len)\n",
    "    ]\n",
    "    seqs = seqs[: int(len(seqs) / batch_sz) * batch_sz]  # make divisible by batch size\n",
    "    return {\"input_ids\": seqs}\n",
    "\n",
    "\n",
    "prep_data = partial(split_and_tokenize, seq_len=seq_len, batch_sz=batch_sz)\n",
    "tokenized_dataset = dataset.map(\n",
    "    prep_data, batched=True, remove_columns=dataset[\"train\"].column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "Compiling Model...\n",
      "Graph compilation: 100%|██████████| 100/100 [01:35<00:00]\n",
      "2023-03-12T23:31:42.132222Z popart:session 1153668.1153668 W: Rng state buffer was not serialized.You did not load poplar Engine.Remember that if you would like to run the model using the model runtime then you have to create your own buffer and callback in your model runtime application for rngStateTensor.\n",
      "Compiled/Loaded model in 101.21531434357166 secs\n",
      "***** Running training *****\n",
      "  Num examples = 7840\n",
      "  Num Epochs = 42\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 320\n",
      "  Gradient Accumulation steps = 20\n",
      "  Total optimization steps = 1000\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcharlieb\u001b[0m (\u001b[33mresearch\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/nethome/charlieb/projects/unit-scaling-notebook/wandb/run-20230312_233146-h9nyfeuf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.sourcevertex.net/research/huggingface/runs/h9nyfeuf' target=\"_blank\">out</a></strong> to <a href='https://wandb.sourcevertex.net/research/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.sourcevertex.net/research/huggingface' target=\"_blank\">https://wandb.sourcevertex.net/research/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.sourcevertex.net/research/huggingface/runs/h9nyfeuf' target=\"_blank\">https://wandb.sourcevertex.net/research/huggingface/runs/h9nyfeuf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 11/1000 [00:01<02:33,  6.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 5.0271, 'learning_rate': 0.0002, 'epoch': 0.42}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 21/1000 [00:03<02:23,  6.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.4369, 'learning_rate': 0.0004, 'epoch': 0.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 31/1000 [00:04<02:10,  7.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.9324, 'learning_rate': 0.0006, 'epoch': 1.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 41/1000 [00:05<02:09,  7.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.6041, 'learning_rate': 0.0008, 'epoch': 1.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 51/1000 [00:07<02:23,  6.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4713, 'learning_rate': 0.001, 'epoch': 2.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 61/1000 [00:08<02:09,  7.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.3746, 'learning_rate': 0.0012, 'epoch': 2.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 71/1000 [00:10<02:16,  6.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.2982, 'learning_rate': 0.0014, 'epoch': 2.92}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 81/1000 [00:11<02:22,  6.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.2604, 'learning_rate': 0.0016, 'epoch': 3.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 91/1000 [00:13<02:10,  6.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1883, 'learning_rate': 0.0018000000000000002, 'epoch': 3.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 101/1000 [00:14<02:09,  6.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1166, 'learning_rate': 0.002, 'epoch': 4.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 111/1000 [00:15<02:03,  7.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0984, 'learning_rate': 0.001977777777777778, 'epoch': 4.58}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 121/1000 [00:17<02:04,  7.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0443, 'learning_rate': 0.0019555555555555554, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 131/1000 [00:18<02:05,  6.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9692, 'learning_rate': 0.0019333333333333333, 'epoch': 5.42}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 141/1000 [00:20<01:56,  7.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9189, 'learning_rate': 0.0019111111111111113, 'epoch': 5.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 151/1000 [00:21<01:58,  7.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8759, 'learning_rate': 0.001888888888888889, 'epoch': 6.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 161/1000 [00:23<02:02,  6.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8571, 'learning_rate': 0.0018666666666666666, 'epoch': 6.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 171/1000 [00:24<02:02,  6.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7812, 'learning_rate': 0.0018444444444444446, 'epoch': 7.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 181/1000 [00:25<01:51,  7.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7725, 'learning_rate': 0.0018222222222222223, 'epoch': 7.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 191/1000 [00:27<01:58,  6.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7665, 'learning_rate': 0.0018000000000000002, 'epoch': 7.92}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 201/1000 [00:28<02:02,  6.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7496, 'learning_rate': 0.0017777777777777776, 'epoch': 8.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 211/1000 [00:30<01:49,  7.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7112, 'learning_rate': 0.0017555555555555556, 'epoch': 8.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 221/1000 [00:31<01:44,  7.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6651, 'learning_rate': 0.0017333333333333335, 'epoch': 9.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 231/1000 [00:33<01:45,  7.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6963, 'learning_rate': 0.0017111111111111112, 'epoch': 9.58}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 241/1000 [00:34<01:49,  6.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6472, 'learning_rate': 0.0016888888888888889, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 250/1000 [00:35<01:43,  7.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6521, 'learning_rate': 0.0016666666666666668, 'epoch': 10.42}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compiling Model...\n",
      "Graph compilation: 100%|██████████| 100/100 [00:24<00:00]\n",
      "Compiled/Loaded model in 28.387929940596223 secs\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 432\n",
      "  Batch size = 16\n",
      "                                                  \n",
      " 25%|██▌       | 250/1000 [01:07<01:43,  7.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.68359375, 'eval_runtime': 0.6334, 'eval_samples_per_second': 682.07, 'eval_steps_per_second': 42.629, 'epoch': 10.42}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Graph compilation: 100%|██████████| 100/100 [00:01<00:00]\n",
      " 26%|██▌       | 261/1000 [01:15<05:40,  2.17it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6067, 'learning_rate': 0.0016444444444444445, 'epoch': 10.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 271/1000 [01:16<01:44,  6.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5835, 'learning_rate': 0.0016222222222222222, 'epoch': 11.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 281/1000 [01:17<01:45,  6.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5747, 'learning_rate': 0.0016, 'epoch': 11.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 291/1000 [01:19<01:46,  6.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5582, 'learning_rate': 0.0015777777777777778, 'epoch': 12.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 301/1000 [01:20<01:42,  6.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5634, 'learning_rate': 0.0015555555555555557, 'epoch': 12.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 311/1000 [01:22<01:29,  7.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5479, 'learning_rate': 0.0015333333333333334, 'epoch': 12.92}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 321/1000 [01:23<01:34,  7.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5112, 'learning_rate': 0.001511111111111111, 'epoch': 13.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 331/1000 [01:24<01:27,  7.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5323, 'learning_rate': 0.001488888888888889, 'epoch': 13.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 341/1000 [01:26<01:25,  7.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5062, 'learning_rate': 0.0014666666666666667, 'epoch': 14.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 351/1000 [01:27<01:22,  7.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4773, 'learning_rate': 0.0014444444444444444, 'epoch': 14.58}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 361/1000 [01:28<01:34,  6.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4915, 'learning_rate': 0.0014222222222222223, 'epoch': 15.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 371/1000 [01:30<01:21,  7.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4746, 'learning_rate': 0.0014, 'epoch': 15.42}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 381/1000 [01:31<01:19,  7.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4629, 'learning_rate': 0.001377777777777778, 'epoch': 15.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 391/1000 [01:32<01:20,  7.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4767, 'learning_rate': 0.0013555555555555556, 'epoch': 16.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 401/1000 [01:34<01:28,  6.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4582, 'learning_rate': 0.0013333333333333333, 'epoch': 16.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 411/1000 [01:35<01:18,  7.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4351, 'learning_rate': 0.0013111111111111112, 'epoch': 17.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 421/1000 [01:37<01:14,  7.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4083, 'learning_rate': 0.001288888888888889, 'epoch': 17.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 431/1000 [01:38<01:19,  7.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4168, 'learning_rate': 0.0012666666666666666, 'epoch': 17.92}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 441/1000 [01:39<01:20,  6.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4179, 'learning_rate': 0.0012444444444444445, 'epoch': 18.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 451/1000 [01:41<01:16,  7.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4348, 'learning_rate': 0.0012222222222222224, 'epoch': 18.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 461/1000 [01:42<01:10,  7.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4024, 'learning_rate': 0.0012, 'epoch': 19.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 471/1000 [01:44<01:15,  7.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3835, 'learning_rate': 0.0011777777777777778, 'epoch': 19.58}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 481/1000 [01:45<01:13,  7.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4061, 'learning_rate': 0.0011555555555555555, 'epoch': 20.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 491/1000 [01:46<01:07,  7.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3834, 'learning_rate': 0.0011333333333333334, 'epoch': 20.42}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 500/1000 [01:48<01:11,  6.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3814, 'learning_rate': 0.0011111111111111111, 'epoch': 20.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compiling Model...\n",
      "Graph compilation: 100%|██████████| 100/100 [00:00<00:00]\n",
      "Compiled/Loaded model in 3.5375875793397427 secs\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 432\n",
      "  Batch size = 16\n",
      "                                                  \n",
      " 50%|█████     | 500/1000 [01:54<01:11,  6.98it/s]Saving model checkpoint to out/checkpoint-500\n",
      "/nethome/charlieb/projects/unit-scaling-notebook/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1428: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Configuration saved in out/checkpoint-500/ipu_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.517578125, 'eval_runtime': 0.4535, 'eval_samples_per_second': 952.581, 'eval_steps_per_second': 59.536, 'epoch': 20.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Graph compilation: 100%|██████████| 100/100 [01:27<00:00]\n",
      "2023-03-12T23:35:14.319987Z popart:session 1153668.1153668 W: Rng state buffer was not serialized.You did not load poplar Engine.Remember that if you would like to run the model using the model runtime then you have to create your own buffer and callback in your model runtime application for rngStateTensor.\n",
      " 51%|█████     | 511/1000 [03:29<08:05,  1.01it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3745, 'learning_rate': 0.0010888888888888888, 'epoch': 21.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 521/1000 [03:30<01:26,  5.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3424, 'learning_rate': 0.0010666666666666667, 'epoch': 21.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 531/1000 [03:32<01:06,  7.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3691, 'learning_rate': 0.0010444444444444446, 'epoch': 22.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 541/1000 [03:33<01:06,  6.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3479, 'learning_rate': 0.0010222222222222221, 'epoch': 22.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 551/1000 [03:34<01:00,  7.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3419, 'learning_rate': 0.001, 'epoch': 22.92}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 561/1000 [03:36<01:04,  6.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3301, 'learning_rate': 0.0009777777777777777, 'epoch': 23.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 571/1000 [03:37<01:01,  6.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3346, 'learning_rate': 0.0009555555555555556, 'epoch': 23.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 581/1000 [03:38<00:56,  7.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3392, 'learning_rate': 0.0009333333333333333, 'epoch': 24.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 591/1000 [03:40<00:56,  7.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3545, 'learning_rate': 0.0009111111111111111, 'epoch': 24.58}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 601/1000 [03:41<00:56,  7.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3281, 'learning_rate': 0.0008888888888888888, 'epoch': 25.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 611/1000 [03:43<00:50,  7.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2939, 'learning_rate': 0.0008666666666666667, 'epoch': 25.42}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 621/1000 [03:44<00:51,  7.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3199, 'learning_rate': 0.0008444444444444444, 'epoch': 25.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 631/1000 [03:45<00:48,  7.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2957, 'learning_rate': 0.0008222222222222222, 'epoch': 26.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 641/1000 [03:47<00:52,  6.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2911, 'learning_rate': 0.0008, 'epoch': 26.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 651/1000 [03:48<00:55,  6.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2979, 'learning_rate': 0.0007777777777777778, 'epoch': 27.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 661/1000 [03:50<00:48,  6.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2705, 'learning_rate': 0.0007555555555555555, 'epoch': 27.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 671/1000 [03:51<00:50,  6.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.299, 'learning_rate': 0.0007333333333333333, 'epoch': 27.92}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 681/1000 [03:53<00:43,  7.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2711, 'learning_rate': 0.0007111111111111111, 'epoch': 28.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 691/1000 [03:54<00:43,  7.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.245, 'learning_rate': 0.000688888888888889, 'epoch': 28.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 701/1000 [03:55<00:43,  6.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2573, 'learning_rate': 0.0006666666666666666, 'epoch': 29.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 711/1000 [03:57<00:40,  7.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2537, 'learning_rate': 0.0006444444444444444, 'epoch': 29.58}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 721/1000 [03:58<00:41,  6.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2746, 'learning_rate': 0.0006222222222222223, 'epoch': 30.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 731/1000 [04:00<00:42,  6.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2401, 'learning_rate': 0.0006, 'epoch': 30.42}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 741/1000 [04:01<00:40,  6.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2556, 'learning_rate': 0.0005777777777777778, 'epoch': 30.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 750/1000 [04:02<00:37,  6.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2387, 'learning_rate': 0.0005555555555555556, 'epoch': 31.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compiling Model...\n",
      "Graph compilation: 100%|██████████| 100/100 [00:24<00:00]\n",
      "Compiled/Loaded model in 27.859491711482406 secs\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 432\n",
      "  Batch size = 16\n",
      "                                                  \n",
      " 75%|███████▌  | 750/1000 [04:34<00:37,  6.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.4990234375, 'eval_runtime': 0.6159, 'eval_samples_per_second': 701.402, 'eval_steps_per_second': 43.838, 'epoch': 31.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Graph compilation: 100%|██████████| 100/100 [00:01<00:00]\n",
      " 76%|███████▌  | 761/1000 [04:41<01:48,  2.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2353, 'learning_rate': 0.0005333333333333334, 'epoch': 31.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 771/1000 [04:42<00:35,  6.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2286, 'learning_rate': 0.0005111111111111111, 'epoch': 32.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 781/1000 [04:43<00:30,  7.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2085, 'learning_rate': 0.0004888888888888889, 'epoch': 32.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 791/1000 [04:45<00:30,  6.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2312, 'learning_rate': 0.00046666666666666666, 'epoch': 32.92}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 801/1000 [04:46<00:29,  6.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2042, 'learning_rate': 0.0004444444444444444, 'epoch': 33.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 811/1000 [04:47<00:25,  7.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2057, 'learning_rate': 0.0004222222222222222, 'epoch': 33.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 821/1000 [04:49<00:24,  7.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.208, 'learning_rate': 0.0004, 'epoch': 34.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 831/1000 [04:50<00:25,  6.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1915, 'learning_rate': 0.00037777777777777777, 'epoch': 34.58}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 841/1000 [04:52<00:23,  6.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1996, 'learning_rate': 0.00035555555555555557, 'epoch': 35.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 851/1000 [04:53<00:20,  7.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1902, 'learning_rate': 0.0003333333333333333, 'epoch': 35.42}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 861/1000 [04:54<00:20,  6.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1891, 'learning_rate': 0.0003111111111111111, 'epoch': 35.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 871/1000 [04:56<00:16,  7.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1698, 'learning_rate': 0.0002888888888888889, 'epoch': 36.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 881/1000 [04:57<00:17,  6.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1554, 'learning_rate': 0.0002666666666666667, 'epoch': 36.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 891/1000 [04:59<00:16,  6.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.185, 'learning_rate': 0.00024444444444444443, 'epoch': 37.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 901/1000 [05:00<00:15,  6.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1438, 'learning_rate': 0.0002222222222222222, 'epoch': 37.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 911/1000 [05:02<00:13,  6.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1703, 'learning_rate': 0.0002, 'epoch': 37.92}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 921/1000 [05:03<00:11,  6.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1346, 'learning_rate': 0.00017777777777777779, 'epoch': 38.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 931/1000 [05:04<00:09,  6.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1595, 'learning_rate': 0.00015555555555555556, 'epoch': 38.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 941/1000 [05:06<00:08,  7.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1626, 'learning_rate': 0.00013333333333333334, 'epoch': 39.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 951/1000 [05:07<00:06,  7.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1399, 'learning_rate': 0.0001111111111111111, 'epoch': 39.58}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 961/1000 [05:09<00:05,  6.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1473, 'learning_rate': 8.888888888888889e-05, 'epoch': 40.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 971/1000 [05:10<00:04,  7.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1273, 'learning_rate': 6.666666666666667e-05, 'epoch': 40.42}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 981/1000 [05:12<00:02,  6.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.154, 'learning_rate': 4.4444444444444447e-05, 'epoch': 40.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 991/1000 [05:13<00:01,  6.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1563, 'learning_rate': 2.2222222222222223e-05, 'epoch': 41.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [05:14<00:00,  7.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1297, 'learning_rate': 0.0, 'epoch': 41.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compiling Model...\n",
      "Graph compilation: 100%|██████████| 100/100 [00:00<00:00]\n",
      "Compiled/Loaded model in 3.576560833491385 secs\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 432\n",
      "  Batch size = 16\n",
      "                                                   \n",
      "100%|██████████| 1000/1000 [05:21<00:00,  7.23it/s]Saving model checkpoint to out/checkpoint-1000\n",
      "/nethome/charlieb/projects/unit-scaling-notebook/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1428: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Configuration saved in out/checkpoint-1000/ipu_config.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "100%|██████████| 1000/1000 [05:21<00:00,  3.11it/s]\n",
      "Saving model checkpoint to trained_model/\n",
      "Configuration saved in trained_model/ipu_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5205078125, 'eval_runtime': 0.4475, 'eval_samples_per_second': 965.284, 'eval_steps_per_second': 60.33, 'epoch': 41.67}\n",
      "{'train_runtime': 326.0602, 'train_samples_per_second': 981.414, 'train_steps_per_second': 3.067, 'train_loss': 1.5380458984375, 'epoch': 41.67}\n"
     ]
    }
   ],
   "source": [
    "train_args = IPUTrainingArguments(\n",
    "    output_dir=\"out\",\n",
    "    per_device_train_batch_size=batch_sz,\n",
    "    per_device_eval_batch_size=batch_sz,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=250,\n",
    "    logging_steps=10,\n",
    "    max_steps=1000,\n",
    "    weight_decay=0.1,\n",
    "    warmup_steps=100,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    learning_rate=2e-3,\n",
    "    report_to=\"wandb\",\n",
    "    loss_scaling=2**6,\n",
    ")\n",
    "\n",
    "model = NanoGPTModel(config)\n",
    "\n",
    "trainer = IPUTrainer(\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    "    model=model,\n",
    "    args=train_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    ipu_config=ipu_config,\n",
    ")\n",
    "trainer.train()\n",
    "trainer.save_model(\"trained_model/\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above should give a Weights and Biases link, which you can click to see the training and evaluation loss curves.\n",
    "\n",
    "Note that we use loss scaling here, though this model is sufficiently small that it can get away without loss scaling. This certainly doesn't hold for larger models though.\n",
    "\n",
    "To really get a sense of how the model's doing, we ought to get it to write some Shakespeare. Here's two attempts at *The Tempest*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IPUConfig {\n",
      "  \"auto_loss_scaling\": false,\n",
      "  \"device_iterations\": 1,\n",
      "  \"embedding_serialization_factor\": 1,\n",
      "  \"enable_half_partials\": true,\n",
      "  \"executable_cache_dir\": \"./exe_cache\",\n",
      "  \"execute_encoder_on_cpu_for_generation\": false,\n",
      "  \"gradient_accumulation_steps\": 20,\n",
      "  \"inference_device_iterations\": 1,\n",
      "  \"inference_replication_factor\": 1,\n",
      "  \"ipus_per_replica\": 1,\n",
      "  \"layers_per_ipu\": [\n",
      "    6\n",
      "  ],\n",
      "  \"matmul_proportion\": 0.6,\n",
      "  \"optimizer_state_offchip\": true,\n",
      "  \"optimum_version\": \"1.6.1\",\n",
      "  \"output_mode\": \"final\",\n",
      "  \"recompute_checkpoint_every_layer\": false,\n",
      "  \"replicated_tensor_sharding\": false,\n",
      "  \"replication_factor\": 1,\n",
      "  \"seed\": null,\n",
      "  \"transformers_version\": \"4.25.1\"\n",
      "}\n",
      "\n",
      "Graph compilation: 100%|██████████| 100/100 [00:25<00:00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Completion [1] =====\n",
      "\n",
      "PROSPERO:\n",
      "Our revels now are ended. These our actors,\n",
      "As I foretold you, were all spirits of the dead;\n",
      "And then the times of your honour to the sea\n",
      "That bear the princes of the base that lives\n",
      "That I may be a little palase you to him\n",
      "That you have present made the common made them\n",
      "And friends that you find the common people.\n",
      "\n",
      "PRINCE:\n",
      "Then, I shall be so, I do another them and\n",
      "The sea of his eyes of his charged that makes me.\n",
      "\n",
      "LADY ANNE:\n",
      "Why, being it so?\n",
      "\n",
      "GLOUCESTER:\n",
      "I cannot be guarded thee to speak.\n",
      "\n",
      "BUCK\n",
      "\n",
      "===== Completion [2] =====\n",
      "\n",
      "PROSPERO:\n",
      "Our revels now are ended. These our actors,\n",
      "As I foretold you, were all spirits of the love\n",
      "At the sun of his son worthy for your love:\n",
      "I have been yourself and a good lords,\n",
      "To say the perfect of the court, and the charity\n",
      "Shall be so and break in the house of York.\n",
      "\n",
      "QUEEN ELIZABETH:\n",
      "The king is not a subject of send the world.\n",
      "\n",
      "KING RICHARD III:\n",
      "I cannot tell you for the court-house.\n",
      "\n",
      "QUEEN ELIZABETH:\n",
      "A goodly friends, shall be a prince:\n",
      "If we be be seen the advantage of the commons' field.\n",
      "\n",
      "KIN\n"
     ]
    }
   ],
   "source": [
    "pipelines.check_model_type = lambda self, supported_models: ...\n",
    "\n",
    "final_model = NanoGPTModel.from_pretrained(\"trained_model/\")\n",
    "\n",
    "TEST_INPUT = \"\"\"PROSPERO:\n",
    "Our revels now are ended. These our actors,\n",
    "As I foretold you, were all spirits\"\"\"\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    ipu_config=ipu_config.to_dict(),  # TODO: feature request -> no to_dict()\n",
    "    model=final_model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=512,\n",
    "    do_sample=True,\n",
    ")\n",
    "\n",
    "outputs = pipe(TEST_INPUT, num_return_sequences=2, temperature=0.4)\n",
    "for i, output in enumerate(outputs):\n",
    "    print(f\"\\n===== Completion [{i+1}] =====\\n\")\n",
    "    print(output[\"generated_text\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's not going to win any literary awards, but pretty good for a few minutes of training. Now let's see if our unit-scaled model can do the same..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unit-scaled model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All we need to do is swap in our new model, remove the loss scaling (of course!), and increase the learning rate (as our weights are now larger due to unit-initialisation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: hide?\n",
    "from types import MethodType\n",
    "\n",
    "class _IPUConfig(IPUConfig):\n",
    "    def to_options(self, *args, **kwargs):\n",
    "        options = super().to_options(*args, **kwargs)\n",
    "        options._popart.setPatterns(dict(AutogradProxyOpPattern=True))\n",
    "        return options\n",
    "\n",
    "    def for_pod_type(self, *args, **kwargs):\n",
    "        config = super().for_pod_type(*args, **kwargs)\n",
    "        config.__class__ = _IPUConfig\n",
    "        return config\n",
    "\n",
    "ipu_config.__class__ = _IPUConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UnitScaledNanoGPTModel(config)\n",
    "\n",
    "train_args.loss_scaling = 1.0\n",
    "train_args.learning_rate = 2e-2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🤞 The moment of truth ... let's train our unit-scaled model 🤞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "Compiling Model...\n",
      "Graph compilation: 100%|██████████| 100/100 [02:02<00:00]\n",
      "2023-03-12T23:39:55.384706Z popart:session 1153668.1153668 W: Rng state buffer was not serialized.You did not load poplar Engine.Remember that if you would like to run the model using the model runtime then you have to create your own buffer and callback in your model runtime application for rngStateTensor.\n",
      "Compiled/Loaded model in 135.06661581527442 secs\n",
      "***** Running training *****\n",
      "  Num examples = 7840\n",
      "  Num Epochs = 42\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 320\n",
      "  Gradient Accumulation steps = 20\n",
      "  Total optimization steps = 1000\n",
      "  1%|          | 11/1000 [00:01<02:16,  7.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 5.8398, 'learning_rate': 0.002, 'epoch': 0.42}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 21/1000 [00:02<02:18,  7.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.726, 'learning_rate': 0.004, 'epoch': 0.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 31/1000 [00:04<02:28,  6.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.9838, 'learning_rate': 0.006, 'epoch': 1.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 41/1000 [00:05<02:28,  6.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.6725, 'learning_rate': 0.008, 'epoch': 1.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 51/1000 [00:07<02:19,  6.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4875, 'learning_rate': 0.01, 'epoch': 2.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 61/1000 [00:08<02:23,  6.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4027, 'learning_rate': 0.012, 'epoch': 2.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 71/1000 [00:10<02:16,  6.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.3176, 'learning_rate': 0.013999999999999999, 'epoch': 2.92}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 81/1000 [00:11<02:21,  6.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.2305, 'learning_rate': 0.016, 'epoch': 3.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 91/1000 [00:13<02:25,  6.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1574, 'learning_rate': 0.018000000000000002, 'epoch': 3.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 101/1000 [00:14<02:07,  7.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1291, 'learning_rate': 0.02, 'epoch': 4.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 111/1000 [00:16<02:17,  6.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0818, 'learning_rate': 0.01977777777777778, 'epoch': 4.58}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 121/1000 [00:17<02:20,  6.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0438, 'learning_rate': 0.019555555555555555, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 131/1000 [00:19<02:10,  6.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9888, 'learning_rate': 0.019333333333333334, 'epoch': 5.42}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 141/1000 [00:20<02:06,  6.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9536, 'learning_rate': 0.019111111111111113, 'epoch': 5.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 151/1000 [00:21<02:01,  6.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9127, 'learning_rate': 0.01888888888888889, 'epoch': 6.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 161/1000 [00:23<01:54,  7.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.906, 'learning_rate': 0.018666666666666668, 'epoch': 6.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 171/1000 [00:24<01:58,  6.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8675, 'learning_rate': 0.018444444444444447, 'epoch': 7.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 181/1000 [00:26<02:10,  6.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8376, 'learning_rate': 0.018222222222222223, 'epoch': 7.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 191/1000 [00:27<02:03,  6.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8215, 'learning_rate': 0.018000000000000002, 'epoch': 7.92}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 201/1000 [00:29<02:00,  6.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7811, 'learning_rate': 0.017777777777777778, 'epoch': 8.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 211/1000 [00:30<02:01,  6.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7716, 'learning_rate': 0.017555555555555557, 'epoch': 8.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 221/1000 [00:32<01:55,  6.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7611, 'learning_rate': 0.017333333333333336, 'epoch': 9.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 231/1000 [00:33<01:52,  6.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.757, 'learning_rate': 0.01711111111111111, 'epoch': 9.58}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 241/1000 [00:35<01:50,  6.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.728, 'learning_rate': 0.01688888888888889, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 250/1000 [00:36<01:41,  7.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6922, 'learning_rate': 0.016666666666666666, 'epoch': 10.42}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compiling Model...\n",
      "2023-03-12T23:41:00.767441Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:41:00.767887Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:41:00.768183Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:41:00.768481Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:41:00.768779Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:41:00.769070Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:41:00.769356Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:41:00.769650Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:41:00.769945Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:41:00.770229Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:41:00.770520Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:41:00.770815Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:41:00.771107Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:41:00.771386Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:41:00.771698Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:41:00.771984Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:41:00.772265Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:41:00.772546Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:41:00.772837Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:41:00.773118Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:41:00.773362Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:41:00.773647Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:41:00.773944Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:41:00.774235Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:41:00.774563Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:41:00.774864Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:41:00.775150Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:41:00.775431Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:41:00.775731Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:41:00.776011Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:41:00.776288Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:41:00.776562Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:41:00.776844Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:41:00.777124Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:41:00.777404Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:41:00.777692Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:41:00.777982Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:41:00.778268Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:41:00.781606Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:41:00.782003Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:41:00.782354Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:41:00.782715Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:41:00.783074Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:41:00.783422Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:41:00.783828Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:41:00.784213Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:41:00.784591Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:41:00.785025Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:41:00.785422Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:41:00.785815Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:41:00.794430Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:41:00.834179Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:41:00.850499Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:41:00.938594Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:41:00.949837Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:41:00.959170Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:41:00.968027Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:41:00.977036Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:41:00.998164Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:41:02.155325Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:41:02.422883Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:41:02.439883Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:41:02.689726Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:41:02.698452Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:41:02.706550Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:41:02.714568Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:41:02.722826Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:41:02.731377Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:41:02.742127Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:41:02.753037Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:41:02.761748Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:41:02.960689Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:41:02.977736Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:41:03.248829Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:41:03.267356Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:41:03.285336Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:41:03.303285Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:41:03.321051Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:41:03.338903Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:41:03.356702Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:41:03.374611Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:41:03.392299Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:41:03.410026Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:41:03.427570Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:41:03.446567Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:41:03.464699Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:41:03.482435Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:41:03.500000Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:41:03.517489Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:41:03.535158Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:41:03.555065Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:41:03.572488Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:41:03.590323Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:41:03.608794Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:41:03.626800Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:41:03.645106Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:41:03.662724Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:41:03.685917Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:41:03.720280Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:41:04.682416Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:41:04.701259Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:41:04.719379Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:41:04.737531Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:41:04.859782Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:41:04.873812Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:41:05.098804Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:41:05.114814Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:41:05.131725Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:41:05.147533Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:41:05.164459Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:41:05.179774Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:41:05.194810Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:41:05.209740Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:41:05.224751Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:41:05.238575Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:41:05.261984Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:41:05.320667Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:41:05.838566Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:41:05.856466Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:41:05.872385Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:41:05.887858Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:41:05.902903Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:41:05.918114Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:41:05.931743Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:41:05.954964Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:41:06.014864Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:41:06.488801Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:41:06.506646Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:41:06.522176Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:41:06.537300Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:41:06.552687Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:41:06.567995Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:41:06.581883Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:41:06.605625Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:41:06.667201Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:41:07.195149Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:41:07.212872Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:41:07.228185Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:41:07.243141Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:41:07.258311Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:41:07.273275Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:41:07.287062Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:41:07.310318Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:41:07.370217Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:41:07.859587Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:41:07.877304Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:41:07.893231Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:41:07.908486Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:41:07.923905Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:41:07.939196Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:41:08.803357Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:41:09.007295Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:41:09.022727Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:41:10.740406Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:41:10.757928Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:41:10.781925Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:41:10.841011Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:41:11.319513Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "Graph compilation: 100%|██████████| 100/100 [00:50<00:00]\n",
      "Compiled/Loaded model in 61.01420885324478 secs\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 432\n",
      "  Batch size = 16\n",
      "\n",
      " 25%|██▌       | 250/1000 [01:43<01:41,  7.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7333984375, 'eval_runtime': 1.1379, 'eval_samples_per_second': 379.656, 'eval_steps_per_second': 23.729, 'epoch': 10.42}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Graph compilation: 100%|██████████| 100/100 [00:02<00:00]\n",
      " 26%|██▌       | 261/1000 [01:58<10:04,  1.22it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7271, 'learning_rate': 0.016444444444444446, 'epoch': 10.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 271/1000 [01:59<02:07,  5.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6881, 'learning_rate': 0.01622222222222222, 'epoch': 11.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 281/1000 [02:01<01:40,  7.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.679, 'learning_rate': 0.016, 'epoch': 11.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 291/1000 [02:02<01:45,  6.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6436, 'learning_rate': 0.015777777777777776, 'epoch': 12.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 301/1000 [02:03<01:46,  6.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6207, 'learning_rate': 0.015555555555555557, 'epoch': 12.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 311/1000 [02:05<01:39,  6.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6296, 'learning_rate': 0.015333333333333334, 'epoch': 12.92}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 321/1000 [02:06<01:35,  7.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6132, 'learning_rate': 0.015111111111111112, 'epoch': 13.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 331/1000 [02:08<01:38,  6.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6141, 'learning_rate': 0.014888888888888889, 'epoch': 13.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 341/1000 [02:09<01:32,  7.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6121, 'learning_rate': 0.014666666666666666, 'epoch': 14.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 351/1000 [02:11<01:33,  6.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.572, 'learning_rate': 0.014444444444444444, 'epoch': 14.58}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 361/1000 [02:12<01:39,  6.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5707, 'learning_rate': 0.014222222222222223, 'epoch': 15.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 371/1000 [02:14<01:31,  6.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5278, 'learning_rate': 0.013999999999999999, 'epoch': 15.42}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 381/1000 [02:15<01:32,  6.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5702, 'learning_rate': 0.013777777777777778, 'epoch': 15.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 391/1000 [02:17<01:31,  6.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.551, 'learning_rate': 0.013555555555555557, 'epoch': 16.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 401/1000 [02:18<01:23,  7.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5581, 'learning_rate': 0.013333333333333332, 'epoch': 16.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 411/1000 [02:19<01:27,  6.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5416, 'learning_rate': 0.013111111111111112, 'epoch': 17.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 421/1000 [02:21<01:24,  6.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5026, 'learning_rate': 0.01288888888888889, 'epoch': 17.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 431/1000 [02:22<01:20,  7.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5347, 'learning_rate': 0.012666666666666666, 'epoch': 17.92}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 441/1000 [02:24<01:21,  6.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5135, 'learning_rate': 0.012444444444444445, 'epoch': 18.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 451/1000 [02:25<01:15,  7.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4982, 'learning_rate': 0.012222222222222223, 'epoch': 18.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 461/1000 [02:27<01:22,  6.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5139, 'learning_rate': 0.012, 'epoch': 19.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 471/1000 [02:28<01:19,  6.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4805, 'learning_rate': 0.011777777777777778, 'epoch': 19.58}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 481/1000 [02:30<01:17,  6.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5014, 'learning_rate': 0.011555555555555555, 'epoch': 20.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 491/1000 [02:31<01:14,  6.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4911, 'learning_rate': 0.011333333333333332, 'epoch': 20.42}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 500/1000 [02:32<01:07,  7.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4955, 'learning_rate': 0.011111111111111112, 'epoch': 20.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compiling Model...\n",
      "Graph compilation: 100%|██████████| 100/100 [00:01<00:00]\n",
      "Compiled/Loaded model in 11.303244029171765 secs\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 432\n",
      "  Batch size = 16\n",
      "\n",
      " 50%|█████     | 500/1000 [02:48<01:07,  7.37it/s]Saving model checkpoint to out/checkpoint-500\n",
      "/nethome/charlieb/projects/unit-scaling-notebook/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1428: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Configuration saved in out/checkpoint-500/ipu_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.548828125, 'eval_runtime': 0.7062, 'eval_samples_per_second': 611.732, 'eval_steps_per_second': 38.233, 'epoch': 20.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Graph compilation: 100%|██████████| 100/100 [00:02<00:00]\n",
      " 51%|█████     | 511/1000 [03:02<03:06,  2.63it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4493, 'learning_rate': 0.010888888888888889, 'epoch': 21.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 521/1000 [03:04<01:18,  6.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4591, 'learning_rate': 0.010666666666666666, 'epoch': 21.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 531/1000 [03:05<01:09,  6.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4536, 'learning_rate': 0.010444444444444445, 'epoch': 22.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 541/1000 [03:06<01:02,  7.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4563, 'learning_rate': 0.010222222222222221, 'epoch': 22.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 551/1000 [03:08<01:06,  6.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4385, 'learning_rate': 0.01, 'epoch': 22.92}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 561/1000 [03:09<01:04,  6.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4509, 'learning_rate': 0.009777777777777778, 'epoch': 23.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 571/1000 [03:11<01:00,  7.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4318, 'learning_rate': 0.009555555555555557, 'epoch': 23.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 581/1000 [03:12<00:57,  7.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4503, 'learning_rate': 0.009333333333333334, 'epoch': 24.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 591/1000 [03:14<01:00,  6.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.396, 'learning_rate': 0.009111111111111111, 'epoch': 24.58}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 601/1000 [03:15<00:57,  6.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4438, 'learning_rate': 0.008888888888888889, 'epoch': 25.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 611/1000 [03:17<01:01,  6.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4076, 'learning_rate': 0.008666666666666668, 'epoch': 25.42}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 621/1000 [03:18<00:50,  7.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4074, 'learning_rate': 0.008444444444444445, 'epoch': 25.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 631/1000 [03:19<00:50,  7.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3782, 'learning_rate': 0.008222222222222223, 'epoch': 26.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 641/1000 [03:21<00:47,  7.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3779, 'learning_rate': 0.008, 'epoch': 26.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 651/1000 [03:22<00:50,  6.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3929, 'learning_rate': 0.007777777777777778, 'epoch': 27.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 661/1000 [03:23<00:46,  7.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.377, 'learning_rate': 0.007555555555555556, 'epoch': 27.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 671/1000 [03:25<00:48,  6.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3975, 'learning_rate': 0.007333333333333333, 'epoch': 27.92}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 681/1000 [03:26<00:43,  7.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3966, 'learning_rate': 0.0071111111111111115, 'epoch': 28.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 691/1000 [03:28<00:43,  7.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3677, 'learning_rate': 0.006888888888888889, 'epoch': 28.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 701/1000 [03:29<00:42,  7.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3507, 'learning_rate': 0.006666666666666666, 'epoch': 29.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 711/1000 [03:30<00:42,  6.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3469, 'learning_rate': 0.006444444444444445, 'epoch': 29.58}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 721/1000 [03:32<00:37,  7.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3599, 'learning_rate': 0.006222222222222223, 'epoch': 30.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 731/1000 [03:33<00:37,  7.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3308, 'learning_rate': 0.006, 'epoch': 30.42}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 741/1000 [03:35<00:40,  6.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3429, 'learning_rate': 0.0057777777777777775, 'epoch': 30.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 750/1000 [03:36<00:37,  6.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3476, 'learning_rate': 0.005555555555555556, 'epoch': 31.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compiling Model...\n",
      "Graph compilation: 100%|██████████| 100/100 [00:01<00:00]\n",
      "Compiled/Loaded model in 10.792989015579224 secs\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 432\n",
      "  Batch size = 16\n",
      "\n",
      " 75%|███████▌  | 750/1000 [03:51<00:37,  6.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.4873046875, 'eval_runtime': 0.7039, 'eval_samples_per_second': 613.734, 'eval_steps_per_second': 38.358, 'epoch': 31.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Graph compilation: 100%|██████████| 100/100 [00:02<00:00]\n",
      " 76%|███████▌  | 761/1000 [04:06<01:31,  2.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3406, 'learning_rate': 0.005333333333333333, 'epoch': 31.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 771/1000 [04:07<00:36,  6.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3567, 'learning_rate': 0.0051111111111111105, 'epoch': 32.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 781/1000 [04:08<00:32,  6.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.315, 'learning_rate': 0.004888888888888889, 'epoch': 32.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 791/1000 [04:10<00:30,  6.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3331, 'learning_rate': 0.004666666666666667, 'epoch': 32.92}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 801/1000 [04:11<00:26,  7.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3228, 'learning_rate': 0.0044444444444444444, 'epoch': 33.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 811/1000 [04:13<00:34,  5.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3225, 'learning_rate': 0.004222222222222223, 'epoch': 33.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 821/1000 [04:14<00:25,  6.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3359, 'learning_rate': 0.004, 'epoch': 34.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 831/1000 [04:16<00:23,  7.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.291, 'learning_rate': 0.003777777777777778, 'epoch': 34.58}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 841/1000 [04:17<00:22,  7.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3017, 'learning_rate': 0.0035555555555555557, 'epoch': 35.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 851/1000 [04:19<00:19,  7.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.294, 'learning_rate': 0.003333333333333333, 'epoch': 35.42}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 861/1000 [04:20<00:21,  6.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.304, 'learning_rate': 0.0031111111111111114, 'epoch': 35.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 871/1000 [04:21<00:17,  7.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3004, 'learning_rate': 0.0028888888888888888, 'epoch': 36.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 881/1000 [04:23<00:17,  6.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2693, 'learning_rate': 0.0026666666666666666, 'epoch': 36.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 891/1000 [04:24<00:14,  7.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3039, 'learning_rate': 0.0024444444444444444, 'epoch': 37.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 901/1000 [04:26<00:13,  7.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2831, 'learning_rate': 0.0022222222222222222, 'epoch': 37.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 911/1000 [04:27<00:11,  7.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2877, 'learning_rate': 0.002, 'epoch': 37.92}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 921/1000 [04:28<00:11,  7.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2669, 'learning_rate': 0.0017777777777777779, 'epoch': 38.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 931/1000 [04:30<00:10,  6.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2662, 'learning_rate': 0.0015555555555555557, 'epoch': 38.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 941/1000 [04:31<00:08,  6.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.281, 'learning_rate': 0.0013333333333333333, 'epoch': 39.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 951/1000 [04:33<00:07,  6.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2652, 'learning_rate': 0.0011111111111111111, 'epoch': 39.58}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 961/1000 [04:34<00:06,  6.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2788, 'learning_rate': 0.0008888888888888889, 'epoch': 40.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 971/1000 [04:36<00:04,  6.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2463, 'learning_rate': 0.0006666666666666666, 'epoch': 40.42}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 981/1000 [04:37<00:02,  7.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.256, 'learning_rate': 0.00044444444444444447, 'epoch': 40.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 991/1000 [04:39<00:01,  6.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2739, 'learning_rate': 0.00022222222222222223, 'epoch': 41.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [04:40<00:00,  7.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2407, 'learning_rate': 0.0, 'epoch': 41.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compiling Model...\n",
      "Graph compilation: 100%|██████████| 100/100 [00:01<00:00]\n",
      "Compiled/Loaded model in 11.236214805394411 secs\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 432\n",
      "  Batch size = 16\n",
      "\n",
      "100%|██████████| 1000/1000 [04:54<00:00,  7.35it/s]Saving model checkpoint to out/checkpoint-1000\n",
      "/nethome/charlieb/projects/unit-scaling-notebook/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1428: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Configuration saved in out/checkpoint-1000/ipu_config.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "100%|██████████| 1000/1000 [04:55<00:00,  3.39it/s]\n",
      "Saving model checkpoint to trained_model/\n",
      "Configuration saved in trained_model/ipu_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.46875, 'eval_runtime': 0.6953, 'eval_samples_per_second': 621.352, 'eval_steps_per_second': 38.834, 'epoch': 41.67}\n",
      "{'train_runtime': 295.0799, 'train_samples_per_second': 1084.452, 'train_steps_per_second': 3.389, 'train_loss': 1.634490234375, 'epoch': 41.67}\n"
     ]
    }
   ],
   "source": [
    "unit_scale_trainer = IPUTrainer(\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    "    model=model,\n",
    "    args=train_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    ipu_config=ipu_config,\n",
    ")\n",
    "\n",
    "unit_scale_trainer.train()\n",
    "unit_scale_trainer.save_model(\"trained_model/\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get a similar evaluation loss as the regular model—a success!\n",
    "\n",
    "Let's celebrate with some unit-scaled Shakespeare..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IPUConfig {\n",
      "  \"auto_loss_scaling\": false,\n",
      "  \"device_iterations\": 1,\n",
      "  \"embedding_serialization_factor\": 1,\n",
      "  \"enable_half_partials\": true,\n",
      "  \"executable_cache_dir\": \"./exe_cache\",\n",
      "  \"execute_encoder_on_cpu_for_generation\": false,\n",
      "  \"gradient_accumulation_steps\": 20,\n",
      "  \"inference_device_iterations\": 1,\n",
      "  \"inference_replication_factor\": 1,\n",
      "  \"ipus_per_replica\": 1,\n",
      "  \"layers_per_ipu\": [\n",
      "    6\n",
      "  ],\n",
      "  \"matmul_proportion\": 0.6,\n",
      "  \"optimizer_state_offchip\": true,\n",
      "  \"optimum_version\": \"1.6.1\",\n",
      "  \"output_mode\": \"final\",\n",
      "  \"recompute_checkpoint_every_layer\": false,\n",
      "  \"replicated_tensor_sharding\": false,\n",
      "  \"replication_factor\": 1,\n",
      "  \"seed\": null,\n",
      "  \"transformers_version\": \"4.25.1\"\n",
      "}\n",
      "\n",
      "Graph compilation:  18%|█▊        | 18/100 [00:14<00:31]2023-03-12T23:45:16.933967Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:45:16.934544Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:45:16.934938Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:45:16.935292Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:45:16.935667Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:45:16.936085Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:45:16.936431Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:45:16.936720Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:45:16.937001Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:45:16.937280Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:45:16.937561Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:45:16.937848Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:45:16.938127Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:45:16.938402Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:45:16.938708Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:45:16.939005Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:45:16.939282Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:45:16.939559Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:45:16.939901Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:45:16.940181Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:45:16.940460Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:45:16.940754Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:45:16.941034Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:45:16.941310Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:45:16.941590Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:45:16.941922Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:45:16.942204Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:45:16.942482Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:45:16.942775Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:45:16.943056Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:45:16.943334Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:45:16.943631Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:45:16.943942Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:45:16.944223Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:45:16.944501Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:45:16.944796Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:45:16.945075Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:45:16.945357Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:45:16.947093Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:45:16.947473Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:45:16.947855Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:45:16.948208Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:45:16.948557Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:45:16.948917Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:45:16.949301Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:45:16.949696Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:45:16.950081Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:45:16.950463Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:45:16.950857Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:45:16.951240Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:45:16.960056Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:45:16.968644Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:45:17.012068Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:45:17.028941Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:45:17.103750Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:45:17.112942Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "Graph compilation:  24%|██▍       | 24/100 [00:14<00:19]2023-03-12T23:45:17.187953Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:45:17.197316Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:45:17.206358Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:45:17.215194Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:45:17.224092Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:45:17.241227Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:45:18.314021Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:45:18.545775Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:45:18.563074Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:45:18.791636Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:45:18.800834Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:45:18.807917Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:45:18.814983Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:45:18.824160Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:45:18.831192Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:45:18.838610Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:45:18.845716Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:45:18.852196Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:45:18.858654Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "Graph compilation:  28%|██▊       | 28/100 [00:16<00:21]2023-03-12T23:45:18.866663Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:45:18.873460Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:45:18.880061Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:45:18.896917Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:45:18.914240Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:45:18.930497Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:45:19.189302Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:45:19.207581Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:45:19.225475Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:45:19.243513Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:45:19.262508Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:45:19.282665Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:45:19.300951Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:45:19.319215Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:45:19.337452Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:45:19.355313Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:45:19.372803Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:45:19.390593Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:45:19.408477Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:45:19.426561Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:45:19.444473Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:45:19.462119Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "Graph compilation:  31%|███       | 31/100 [00:17<00:19]2023-03-12T23:45:19.691525Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:45:19.759663Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:45:21.166278Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:45:21.196539Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:45:21.264261Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:45:22.707795Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:45:22.727157Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:45:22.745978Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:45:22.764632Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:45:22.950187Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:45:22.970081Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:45:23.181351Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "Graph compilation:  33%|███▎      | 33/100 [00:20<00:37]2023-03-12T23:45:23.200027Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:45:23.216741Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:45:23.233152Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:45:23.252853Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:45:23.271957Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:45:23.289790Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:45:23.306394Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:45:23.323123Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:45:23.340244Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:45:23.455358Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:45:23.471302Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:45:23.486526Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:45:23.501896Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:45:23.517578Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:45:23.535143Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "Graph compilation:  35%|███▌      | 35/100 [00:21<00:31]2023-03-12T23:45:23.553208Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:45:23.580487Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:45:23.649283Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:45:25.035309Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:45:25.053675Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:45:25.071561Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:45:25.088975Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:45:25.117693Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:45:25.193107Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:45:25.669780Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "Graph compilation:  37%|███▋      | 37/100 [00:23<00:38]2023-03-12T23:45:25.697523Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:45:25.713611Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:45:25.729135Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:45:25.744703Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "Graph compilation:  38%|███▊      | 38/100 [00:23<00:33]2023-03-12T23:45:26.389633Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "2023-03-12T23:45:26.596272Z popart:popart 1153668.1153668 W: `autograd_proxy(fwd, proxy)` has not pruned the forward pass of `proxy`, leading to inefficient execution - please use the setting: `PopTorchOptions._popart.setPatterns(dict(AutogradProxyOpPattern=True))`\n",
      "Graph compilation: 100%|██████████| 100/100 [00:52<00:00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Completion [1] =====\n",
      "\n",
      "PROSPERO:\n",
      "Our revels now are ended. These our actors,\n",
      "As I foretold you, were all spirits of the people,\n",
      "Which then I speak with me to the streets,\n",
      "And then I am so fair soul of a man\n",
      "That I was slew them such sounded to the body.\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "The king is the duke's death.\n",
      "\n",
      "CAMILLO:\n",
      "My lord,\n",
      "Stand so I should be so.\n",
      "\n",
      "AUTOLYCUS:\n",
      "I have pass'd him to our son a prince:\n",
      "The gentleman should be have the duke of his honour,\n",
      "And presently and he private still stands\n",
      "Which he hath pride him of your traitors to \n",
      "\n",
      "===== Completion [2] =====\n",
      "\n",
      "PROSPERO:\n",
      "Our revels now are ended. These our actors,\n",
      "As I foretold you, were all spirits for this:\n",
      "The manners of your honour of my country's head,\n",
      "That I do not so long have the world is off,\n",
      "And sent it of the house of York and more\n",
      "Than the earth of his death.\n",
      "\n",
      "KING RICHARD II:\n",
      "Why, he was a traitor of all the common soul than to me?\n",
      "\n",
      "KING RICHARD III:\n",
      "Ay, thou wilt do thy soul thy death.\n",
      "\n",
      "Second Keeper:\n",
      "The county of this most thing are speed,\n",
      "To hear me speak to the country:\n",
      "The bloody state and the s\n"
     ]
    }
   ],
   "source": [
    "pipelines.check_model_type = lambda self, supported_models: ...\n",
    "\n",
    "final_model = UnitScaledNanoGPTModel.from_pretrained(\"trained_model/\")\n",
    "\n",
    "\n",
    "TEST_INPUT = \"\"\"PROSPERO:\n",
    "Our revels now are ended. These our actors,\n",
    "As I foretold you, were all spirits\"\"\"\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    ipu_config=ipu_config.to_dict(),  # TODO: feature request -> no to_dict()\n",
    "    model=final_model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=512,\n",
    "    do_sample=True,\n",
    ")\n",
    "\n",
    "outputs = pipe(TEST_INPUT, num_return_sequences=2, temperature=0.4)\n",
    "for i, output in enumerate(outputs):\n",
    "    print(f\"\\n===== Completion [{i+1}] =====\\n\")\n",
    "    print(output[\"generated_text\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some neat verse, from a neatly-scaled model. From which we can only conclude:\n",
    "\n",
    "*Enter: unit scaling*\n",
    "\n",
    "*Exeunt: loss scaling, automatic loss scaling*\n",
    "\n",
    "FIN\n",
    "\n",
    "---\n",
    "\n",
    "We hope that practitioners will consider using unit scaling for future projects, particularly those having difficulties with loss scaling or automatic mixed precision. With FP8 on the horizon, these issues are likely to become more prevalent. We hope unit scaling can help.\n",
    "\n",
    "If you're interested in using unit scaling yourself, or have questions, please do reach out 🙏☎️ We're keen to hear from anyone that has a problem unit scaling might help solve.\n",
    "\n",
    "The definitions provided here are the closest we have to an \"official\" PyTorch implementation, but if there's demand for a library tell us and we'll make one!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
