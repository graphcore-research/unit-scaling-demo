{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) 2023 Graphcore Ltd. All rights reserved."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unit Scaling: A How-To Guide\n",
    "\n",
    "In our paper [Unit Scaling: Out-of-the-Box Low-Precision Training](TODO), we describe a scheme for designing neural networks that have approximate unit variance after every operation in the forward and backward pass.\n",
    "\n",
    "This can be seen as an alternative to (static) loss scaling, or its automatic variant, as used in Automatic Mixed Precision. Whereas both of those schemes rely on a single, global scaling factor for all the gradients, unit scaling is more fine-grained.\n",
    "\n",
    "A unit-scaled model adds scaling factors (constant scalar multiplications) to each operation in the computational graph to achieve this unit variance property. The result is a model which naturally produces tensors in the middle of the dynamic range provided by floating-point formats. There's no extra loss-scale hyperparameterâ€”it works out-of-the-box!\n",
    "\n",
    "## Implementing unit scaling\n",
    "\n",
    "Here we demonstrate how to go about unit scaling in practice. This involves re-implementing common neural network layers to add variance-preserving scaling factors.\n",
    "\n",
    "As explained in the paper, we can sometimes justify using different scaling factors in the forward and backward pass. We introduce a special `scaled()` op which allows us to do just that.\n",
    "\n",
    "Below we implement two models. The first is a simple implementation of a transformer-decoder. The design and hyperparameters are inspired by Andrej Karpathy's [popular NanoGPT implementation](https://github.com/karpathy/nanoGPT) (though some differ). It's also ðŸ¤—-compatible!\n",
    "\n",
    "The second model is the same, but unit-scaled. Let's get stuck in...\n",
    "\n",
    "## Building a unit-scaled NanoGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/huggingface/optimum-graphcore.git\n",
    "!pip install git+https://github.com/graphcore-research/poptorch-experimental-addons\n",
    "!pip install altair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import Dict, Optional\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, Tensor\n",
    "from transformers.activations import GELUActivation\n",
    "from transformers.configuration_utils import PretrainedConfig\n",
    "from transformers.modeling_outputs import CausalLMOutputWithCrossAttentions\n",
    "from transformers.modeling_utils import PreTrainedModel"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The MLP layer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regular scaling\n",
    "\n",
    "We'll start by setting up a basic config for a reasonably small transformer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NanoGPTConfig(PretrainedConfig):\n",
    "    model_type = \"nano-gpt\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_size: int = 384,\n",
    "        num_hidden_layers: int = 6,\n",
    "        num_attention_heads: int = 6,\n",
    "        dropout: float = 0.1,\n",
    "        vocab_size: int = 384,\n",
    "        eos_token_id: int = 1,\n",
    "        **kwargs,\n",
    "    ) -> None:\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "        self.dropout = dropout\n",
    "        self.vocab_size = vocab_size\n",
    "        self.eos_token_id = eos_token_id\n",
    "        super().__init__(**kwargs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Along with a standard (pre-norm) MLP module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, config: NanoGPTConfig) -> None:\n",
    "        super().__init__()\n",
    "        self.ln = nn.LayerNorm(config.hidden_size)\n",
    "        self.linear_1 = nn.Linear(config.hidden_size, config.hidden_size * 4)\n",
    "        self.act = GELUActivation()\n",
    "        self.linear_2 = nn.Linear(config.hidden_size * 4, config.hidden_size)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, hidden_states: Tensor) -> Tensor:\n",
    "        hidden_states = self.ln(hidden_states)\n",
    "        hidden_states = self.linear_1(hidden_states)\n",
    "        hidden_states = self.act(hidden_states)\n",
    "        hidden_states = self.linear_2(hidden_states)\n",
    "        return self.dropout(hidden_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: hide\n",
    "def init_weights(init_fn) -> None:\n",
    "    def inner_fn(module: nn.Module) -> None:\n",
    "        if isinstance(module, nn.Linear):\n",
    "            fan_out, fan_in = module.weight.shape  # TODO: correct base impl\n",
    "            module.weight.data.normal_(mean=0.0, std=init_fn(fan_in, fan_out))\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            module.weight.data.normal_(mean=0.0, std=1)\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "    return inner_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: hide\n",
    "import pandas as pd\n",
    "\n",
    "np.seterr(divide = 'ignore')\n",
    "\n",
    "def instrument(module):\n",
    "    stats = {}\n",
    "    instrument_recursive(module, stats)\n",
    "    return stats\n",
    "\n",
    "def instrument_recursive(module, stats, name=''):\n",
    "    children = list(module.named_children())\n",
    "    if children:\n",
    "        for c_name, c in children:\n",
    "            _name = f'{name}.{c_name}' if name and name != 'blocks' else c_name\n",
    "            instrument_recursive(c, stats, _name)\n",
    "    else:\n",
    "        instrument_terminal(module, stats, name)\n",
    "\n",
    "def instrument_terminal(module, stats, name):\n",
    "    module_stats = {}\n",
    "    def require_input_grads(_module, input):\n",
    "        for i in input:\n",
    "            if isinstance(i, Tensor) and i.is_floating_point():\n",
    "                i.requires_grad_()\n",
    "    \n",
    "    module.register_forward_pre_hook(require_input_grads)\n",
    "    \n",
    "    if name.split('.')[-1] == 'softmax':\n",
    "        return\n",
    "    \n",
    "    def record_fwd_scale(_module, input, output):\n",
    "        if isinstance(output, Tensor) and output.is_floating_point():\n",
    "            module_stats['x'] = np.log2(output.std().item())\n",
    "    \n",
    "    module.register_forward_hook(record_fwd_scale)\n",
    "    \n",
    "    def record_bwd_scales(_module, grad_input, grad_output):\n",
    "        grad_input = list(grad_input)\n",
    "        for g in grad_input:\n",
    "            if g is not None and isinstance(g, Tensor) \\\n",
    "                and g.is_floating_point() and len(grad_input) == 1:\n",
    "                module_stats['grad_x'] = np.log2(g.std().item())\n",
    "        \n",
    "        for param_name, param in _module.named_parameters():\n",
    "            if param_name == \"weight\":\n",
    "                module_stats['w'] = np.log2(param.std().item())\n",
    "                if param.grad is not None:\n",
    "                    module_stats['grad_w'] = np.log2(param.grad.std().item())\n",
    "    \n",
    "    module.register_full_backward_hook(record_bwd_scales)\n",
    "    \n",
    "    stats[name] = module_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: hide\n",
    "import altair as alt\n",
    "\n",
    "def visualise(stats, subnormal=False):\n",
    "    df = pd.DataFrame(stats)\n",
    "    df = df.stack().to_frame('scale (logâ‚‚)').reset_index(names=['type', 'op'])\n",
    "    plot(df, subnormal)\n",
    "\n",
    "def plot(df, subnormal=False):\n",
    "    is_x_or_grad_x = (df['type'] == 'x') | (df['type'] == 'grad_x')\n",
    "    op_order = df[df['type'] == 'x']['op'].tolist()\n",
    "    colors = ['#6C8EBF', '#FF8000', '#5D8944', '#ED3434']\n",
    "    x_range = np.arange(-18 if subnormal else -14, 18+1 if subnormal else 16+1, 2)\n",
    "    \n",
    "    fp16_min = alt.Chart().mark_rule(strokeDash=(4, 4)).encode(x=alt.datum(-14))\n",
    "    fp16_min_text = alt.Chart().mark_text(dy=-740).encode(text=alt.Text(value='Min FP16 (normal)'), x=alt.datum(-10))\n",
    "    fp16_max = alt.Chart().mark_rule(strokeDash=(4, 4)).encode(x=alt.datum(16))\n",
    "    fp16_max_text = alt.Chart().mark_text(dy=-740).encode(text=alt.Text(value='Max FP16'), x=alt.datum(13))\n",
    "    \n",
    "    x_chart = alt.Chart(df[is_x_or_grad_x]).mark_line().encode(\n",
    "        x=alt.X(\n",
    "            'scale (logâ‚‚):Q',\n",
    "            axis=alt.Axis(orient='top', values=x_range),\n",
    "            scale=alt.Scale(domain=[x_range[0], x_range[-1]]),\n",
    "        ),\n",
    "        y=alt.Y('op:O', title='', sort=op_order),\n",
    "        color=alt.Color(\n",
    "            'type',\n",
    "            legend=alt.Legend(title='', labelFontSize=12, symbolSize=100),\n",
    "            scale=alt.Scale(range=colors[:2]),\n",
    "            sort='descending'\n",
    "        ),\n",
    "    )\n",
    "    w_chart = alt.Chart(df[~is_x_or_grad_x]).mark_point(size=100).encode(\n",
    "        x=alt.X(\n",
    "            'scale (logâ‚‚):Q',\n",
    "            axis=alt.Axis(orient='top', values=x_range),\n",
    "            scale=alt.Scale(domain=[x_range[0], x_range[-1]])\n",
    "        ),\n",
    "        y=alt.Y('op:O', title='', sort=op_order),\n",
    "        color=alt.Color(\n",
    "            'type',\n",
    "            legend=alt.Legend(title='', labelFontSize=12, symbolSize=100),\n",
    "            scale=alt.Scale(range=colors[2:]),\n",
    "            sort='descending'\n",
    "        ),\n",
    "        shape=alt.Shape(\n",
    "            'type',\n",
    "            scale=alt.Scale(range=['square', 'triangle-down']),\n",
    "            sort='descending'\n",
    "        ),\n",
    "    )\n",
    "    layers = [x_chart, w_chart]\n",
    "    if subnormal:\n",
    "        layers += [fp16_min, fp16_max, fp16_min_text, fp16_max_text] \n",
    "    combined_chart = alt.layer(\n",
    "        *layers\n",
    "    ).resolve_scale(\n",
    "        color='independent', shape='independent'\n",
    "    ).configure_axis(\n",
    "        labelFontSize=12,\n",
    "        titleFontSize=16\n",
    "    ).properties(\n",
    "        width=500\n",
    "    )\n",
    "    display(combined_chart)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can analyse the scale (i.e. standard deviation) of each operation within the MLP. To do this, we provide an `instrument()` operation, which goes through and tracks the scale coming out of each operation in the forward and backward pass.\n",
    "\n",
    "Our network's weights will also use the standard glorot initialisation.\n",
    "\n",
    "Let's feed in a unit normal tensor in both directions, and examine the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_mlp(mlp, config, batch_size=64, seq_len=16):\n",
    "    stats = instrument(mlp)\n",
    "    x = torch.normal(0.0, 1.0, size=(batch_size, seq_len, config.hidden_size))\n",
    "    y = mlp(x)\n",
    "    y.backward(torch.normal(0.0, 1.0, size=y.shape))\n",
    "    visualise(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-7fb6b361d29b490a8695431ea648a810\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-7fb6b361d29b490a8695431ea648a810\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-7fb6b361d29b490a8695431ea648a810\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.17.0?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"4.17.0\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}, \"axis\": {\"labelFontSize\": 12, \"titleFontSize\": 16}}, \"layer\": [{\"data\": {\"name\": \"data-af865e44e3f4327a59c43a9a7620ab1b\"}, \"mark\": \"line\", \"encoding\": {\"color\": {\"field\": \"type\", \"legend\": {\"labelFontSize\": 12, \"symbolSize\": 100, \"title\": \"\"}, \"scale\": {\"range\": [\"#6C8EBF\", \"#FF8000\"]}, \"sort\": \"descending\", \"type\": \"nominal\"}, \"x\": {\"axis\": {\"orient\": \"top\", \"values\": [-14.0, -12.0, -10.0, -8.0, -6.0, -4.0, -2.0, 0.0, 2.0, 4.0, 6.0, 8.0, 10.0, 12.0, 14.0, 16.0]}, \"field\": \"scale (log\\u2082)\", \"scale\": {\"domain\": [-14.0, 16.0]}, \"type\": \"quantitative\"}, \"y\": {\"field\": \"op\", \"sort\": [\"ln\", \"linear_1\", \"act\", \"linear_2\", \"dropout\"], \"title\": \"\", \"type\": \"ordinal\"}}}, {\"data\": {\"name\": \"data-b1b0574336090e09ad9ceba316027b70\"}, \"mark\": {\"type\": \"point\", \"size\": 100}, \"encoding\": {\"color\": {\"field\": \"type\", \"legend\": {\"labelFontSize\": 12, \"symbolSize\": 100, \"title\": \"\"}, \"scale\": {\"range\": [\"#5D8944\", \"#ED3434\"]}, \"sort\": \"descending\", \"type\": \"nominal\"}, \"shape\": {\"field\": \"type\", \"scale\": {\"range\": [\"square\", \"triangle-down\"]}, \"sort\": \"descending\", \"type\": \"nominal\"}, \"x\": {\"axis\": {\"orient\": \"top\", \"values\": [-14.0, -12.0, -10.0, -8.0, -6.0, -4.0, -2.0, 0.0, 2.0, 4.0, 6.0, 8.0, 10.0, 12.0, 14.0, 16.0]}, \"field\": \"scale (log\\u2082)\", \"scale\": {\"domain\": [-14.0, 16.0]}, \"type\": \"quantitative\"}, \"y\": {\"field\": \"op\", \"sort\": [\"ln\", \"linear_1\", \"act\", \"linear_2\", \"dropout\"], \"title\": \"\", \"type\": \"ordinal\"}}}], \"resolve\": {\"scale\": {\"color\": \"independent\", \"shape\": \"independent\"}}, \"width\": 500, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.17.0.json\", \"datasets\": {\"data-af865e44e3f4327a59c43a9a7620ab1b\": [{\"type\": \"x\", \"op\": \"ln\", \"scale (log\\u2082)\": -5.41746367372377e-06}, {\"type\": \"x\", \"op\": \"linear_1\", \"scale (log\\u2082)\": -0.6611763129090313}, {\"type\": \"x\", \"op\": \"act\", \"scale (log\\u2082)\": -1.486183267558083}, {\"type\": \"x\", \"op\": \"linear_2\", \"scale (log\\u2082)\": -1.0485166854942438}, {\"type\": \"x\", \"op\": \"dropout\", \"scale (log\\u2082)\": -0.9730187097586517}, {\"type\": \"grad_x\", \"op\": \"ln\", \"scale (log\\u2082)\": -0.9318331541269443}, {\"type\": \"grad_x\", \"op\": \"linear_1\", \"scale (log\\u2082)\": -0.9332948837818494}, {\"type\": \"grad_x\", \"op\": \"act\", \"scale (log\\u2082)\": -1.2639267901772675}, {\"type\": \"grad_x\", \"op\": \"linear_2\", \"scale (log\\u2082)\": -0.585948384082954}, {\"type\": \"grad_x\", \"op\": \"dropout\", \"scale (log\\u2082)\": 0.07546805074658128}], \"data-b1b0574336090e09ad9ceba316027b70\": [{\"type\": \"w\", \"op\": \"ln\", \"scale (log\\u2082)\": null}, {\"type\": \"w\", \"op\": \"linear_1\", \"scale (log\\u2082)\": -4.954251668364685}, {\"type\": \"w\", \"op\": \"linear_2\", \"scale (log\\u2082)\": -4.953057203162492}, {\"type\": \"grad_w\", \"op\": \"ln\", \"scale (log\\u2082)\": 4.076348176957565}, {\"type\": \"grad_w\", \"op\": \"linear_1\", \"scale (log\\u2082)\": 3.7335717930543355}, {\"type\": \"grad_w\", \"op\": \"linear_2\", \"scale (log\\u2082)\": 3.6852586721675165}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.LayerChart(...)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def glorot_init(fan_in, fan_out):\n",
    "    return ((fan_in + fan_out) / 2) ** -0.5\n",
    "\n",
    "config = NanoGPTConfig()\n",
    "mlp = MLP(config).apply(init_weights(glorot_init))\n",
    "analyse_mlp(mlp, config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How to interpret this chart:** The x-axis shows the scale in $\\log_2$ form. The range here represents the minimum and maximum absolute values that can be represented in the FP16 or FP8 E5 number formats (excluding subnormal values). In other words, if the scale exceeds the x-axis bounds we're in the numerics \"danger zone\" where training begins to degrade.\n",
    "\n",
    "The y-axis shows the operations in the model, in the order in which they execute. We show the scale of activations (x), gradients (grad_x, grad_w) and weights (w). Activations can be said to \"flow\" forward through these layers, and grad_xs flow backwards, so we represent these by solid lines, and weights and grad_ws by symbols."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the end of both passes the x and grad_x scales have dropped by half. This is due to glorot slightly under-scaling values, and GeLU dropping the scale further.\n",
    "\n",
    "Worse, the weights are significantly under-scaled and the grad_ws over-scaled by a factor of $2^4$. This is largely because glorot scaling (along with all other weight init schemes) only accounts for the forward and grad_x scales."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unit scaling\n",
    "\n",
    "We'll now implement the equivalent layer using unit scaling, beginning with a basic linear operation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: hide\n",
    "class ScaledGrad(torch.autograd.Function):\n",
    "  @staticmethod\n",
    "  def forward(ctx, X, alpha, beta):\n",
    "    ctx.save_for_backward(\n",
    "      torch.tensor(beta, dtype=X.dtype))\n",
    "    return alpha * X\n",
    "\n",
    "  @staticmethod\n",
    "  def backward(ctx, grad_Y):\n",
    "    beta, = ctx.saved_tensors\n",
    "    return beta * grad_Y, None, None\n",
    "\n",
    "def scaled(X, alpha=1.0, beta=1.0):\n",
    "  # Forward: Y = X * alpha\n",
    "  # Backward: grad_X = grad_Y * beta\n",
    "  return ScaledGrad.apply(X, alpha, beta)\n",
    "\n",
    "def geometric_mean(xs):\n",
    "    xs = np.array(xs)\n",
    "    return xs.prod() ** (1 / xs.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnitScaledLinear(nn.Linear):\n",
    "    def __init__(self, *args, scale_for=\"fwd, grad_x\", **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.scale_for = scale_for\n",
    "    \n",
    "    def get_scales(self, input):\n",
    "        fwd_scale = self.weight.shape[1] ** -0.5\n",
    "        grad_x_scale = self.weight.shape[0] ** -0.5\n",
    "        grad_w_scale = np.prod(input.shape[:-1]) ** -0.5\n",
    "        if self.scale_for == \"fwd\":\n",
    "            grad_x_scale = fwd_scale\n",
    "        elif self.scale_for == \"grad_x\":\n",
    "            fwd_scale = grad_x_scale\n",
    "        elif self.scale_for == \"fwd, grad_x\":\n",
    "            fwd_scale = grad_x_scale = geometric_mean([fwd_scale, grad_x_scale])\n",
    "        else:\n",
    "            assert self.scale_for == \"separate\", f\"demo implementation has no {self.scale_for} scaling\"\n",
    "        return fwd_scale, grad_x_scale, grad_w_scale\n",
    "           \n",
    "    def forward(self, input):\n",
    "        fwd_scale, grad_x_scale, grad_w_scale = self.get_scales(input)\n",
    "        input = scaled(input, beta=grad_x_scale)\n",
    "        weight = scaled(self.weight, beta=grad_w_scale)\n",
    "        bias = scaled(self.bias, beta=grad_w_scale) if self.bias is not None else None\n",
    "        output = F.linear(input, weight, bias)\n",
    "        return scaled(output, alpha=fwd_scale)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's break this down a bit. The `forward()` method is still based on the fundamental `F.linear(input, weight, bias)` operation, but has each of its operands and output scaled.\n",
    "\n",
    "This is done via the `scaled(X, alpha, beta)` method. This is a special operation in that it has different dynamics in the forward and backward pass, where we have\n",
    "\n",
    "$$Y = X \\cdot \\alpha$$\n",
    "\n",
    "$$\\nabla_X = \\nabla_Y \\cdot \\beta$$\n",
    "\n",
    "So what scaling factors do we choose? This is determined in `get_scales()`. The standard approach is to set the forward and grad_x scales as a compromise between their \"ideal\" scale-preserving values (via a geometric mean). See our paper for more details on how we arrive at these ideal values.\n",
    "\n",
    "We also provide versions of the operation which select forward and grad_x scales based on only one of their ideal values. We also have a scheme which has separate forward and grad_x scales, though this is only allowed in special circumstances (again, see the paper)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the GeLU. This is pretty similar to the linear op. However, as activation functions are nonlinear, our usual approach of calculating scaling values analytically (i.e. by working through the maths) isn't always possible.\n",
    "\n",
    "Fortunately, for these elemenwise ops we can calulate them empirically, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GeLU: fwd scale=0.588, bwd scale=0.675\n",
      "Tanh: fwd scale=0.628, bwd scale=0.682\n"
     ]
    }
   ],
   "source": [
    "def analyse_elemenwise_fn(fn, num_samples=2**22):\n",
    "    x = torch.normal(0.0, 1.0, size=(num_samples,)).requires_grad_()\n",
    "    y = fn(x)\n",
    "    y.backward(torch.normal(0.0, 1.0, size=(num_samples,)))\n",
    "    print(f\"fwd scale={y.std():.3f}, bwd scale={x.grad.std():.3f}\")\n",
    "\n",
    "print(\"GeLU:\", end=\" \")\n",
    "analyse_elemenwise_fn(GELUActivation())\n",
    "print(\"Tanh:\", end=\" \")\n",
    "analyse_elemenwise_fn(nn.Tanh())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then put these scaling factors into our unit-scaled op as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnitScaledGELU(GELUActivation):\n",
    "    def forward(self, input):\n",
    "        fwd_scale = bwd_scale = geometric_mean([0.588, 0.675]) ** -1\n",
    "        input = scaled(input, beta=bwd_scale)\n",
    "        output = self.act(input)\n",
    "        return scaled(output, alpha=fwd_scale)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple! Now we just need to unit scale the layernorm and dropout and we're ready.\n",
    "\n",
    "These take the same kind of approach, so we won't go into too much detail here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnitScaledLayerNorm(nn.LayerNorm):\n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        scale = (np.prod(self.normalized_shape) / input.nelement()) ** 0.5\n",
    "        weight = scaled(self.weight, beta=scale)\n",
    "        bias = scaled(self.bias, beta=scale)\n",
    "        return F.layer_norm(input, self.normalized_shape, weight, bias, self.eps)\n",
    "\n",
    "class UnitScaledDropout(nn.Dropout):\n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        # Dropout is typically implemented with a (1-p) ** -1 scaling\n",
    "        # However, to preserve variance this ought to be (1-p) ** -0.5\n",
    "        # We correct for this by multiplying by (1-p) ** 0.5\n",
    "        scale = (1-self.p) ** 0.5\n",
    "        input = scaled(input, beta=scale)\n",
    "        output = F.dropout(input, self.p, self.training, self.inplace)\n",
    "        return scaled(output, alpha=scale)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're now ready to unit-scale the full MLP ðŸ¥³\n",
    "\n",
    "All this requires is swapping out the old layers for our new ones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnitScaledMLP(MLP):\n",
    "    def __init__(self, config: NanoGPTConfig) -> None:\n",
    "        super().__init__(config)\n",
    "        self.ln = UnitScaledLayerNorm(config.hidden_size)\n",
    "        self.linear_1 = UnitScaledLinear(config.hidden_size, config.hidden_size * 4)\n",
    "        self.act = UnitScaledGELU()\n",
    "        self.linear_2 = UnitScaledLinear(config.hidden_size * 4, config.hidden_size)\n",
    "        self.dropout = UnitScaledDropout(config.dropout)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also change our initialisation to give our weights unit scale. Let's analyse our new unit-scaled MLP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-1ce576e0cfe248a398d50ada6a1cbc8c\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-1ce576e0cfe248a398d50ada6a1cbc8c\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-1ce576e0cfe248a398d50ada6a1cbc8c\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.17.0?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"4.17.0\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}, \"axis\": {\"labelFontSize\": 12, \"titleFontSize\": 16}}, \"layer\": [{\"data\": {\"name\": \"data-88cfd94b55ea68f5fbf39ec01f8a0eda\"}, \"mark\": \"line\", \"encoding\": {\"color\": {\"field\": \"type\", \"legend\": {\"labelFontSize\": 12, \"symbolSize\": 100, \"title\": \"\"}, \"scale\": {\"range\": [\"#6C8EBF\", \"#FF8000\"]}, \"sort\": \"descending\", \"type\": \"nominal\"}, \"x\": {\"axis\": {\"orient\": \"top\", \"values\": [-14.0, -12.0, -10.0, -8.0, -6.0, -4.0, -2.0, 0.0, 2.0, 4.0, 6.0, 8.0, 10.0, 12.0, 14.0, 16.0]}, \"field\": \"scale (log\\u2082)\", \"scale\": {\"domain\": [-14.0, 16.0]}, \"type\": \"quantitative\"}, \"y\": {\"field\": \"op\", \"sort\": [\"ln\", \"linear_1\", \"act\", \"linear_2\", \"dropout\"], \"title\": \"\", \"type\": \"ordinal\"}}}, {\"data\": {\"name\": \"data-b9ffa847c238961136c7fc5898912583\"}, \"mark\": {\"type\": \"point\", \"size\": 100}, \"encoding\": {\"color\": {\"field\": \"type\", \"legend\": {\"labelFontSize\": 12, \"symbolSize\": 100, \"title\": \"\"}, \"scale\": {\"range\": [\"#5D8944\", \"#ED3434\"]}, \"sort\": \"descending\", \"type\": \"nominal\"}, \"shape\": {\"field\": \"type\", \"scale\": {\"range\": [\"square\", \"triangle-down\"]}, \"sort\": \"descending\", \"type\": \"nominal\"}, \"x\": {\"axis\": {\"orient\": \"top\", \"values\": [-14.0, -12.0, -10.0, -8.0, -6.0, -4.0, -2.0, 0.0, 2.0, 4.0, 6.0, 8.0, 10.0, 12.0, 14.0, 16.0]}, \"field\": \"scale (log\\u2082)\", \"scale\": {\"domain\": [-14.0, 16.0]}, \"type\": \"quantitative\"}, \"y\": {\"field\": \"op\", \"sort\": [\"ln\", \"linear_1\", \"act\", \"linear_2\", \"dropout\"], \"title\": \"\", \"type\": \"ordinal\"}}}], \"resolve\": {\"scale\": {\"color\": \"independent\", \"shape\": \"independent\"}}, \"width\": 500, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.17.0.json\", \"datasets\": {\"data-88cfd94b55ea68f5fbf39ec01f8a0eda\": [{\"type\": \"x\", \"op\": \"ln\", \"scale (log\\u2082)\": -5.41746367372377e-06}, {\"type\": \"x\", \"op\": \"linear_1\", \"scale (log\\u2082)\": -0.49963438945474525}, {\"type\": \"x\", \"op\": \"act\", \"scale (log\\u2082)\": -0.6391356397106588}, {\"type\": \"x\", \"op\": \"linear_2\", \"scale (log\\u2082)\": -0.03709993601574993}, {\"type\": \"x\", \"op\": \"dropout\", \"scale (log\\u2082)\": -0.037176875789001564}, {\"type\": \"grad_x\", \"op\": \"ln\", \"scale (log\\u2082)\": 0.0173147706813514}, {\"type\": \"grad_x\", \"op\": \"linear_1\", \"scale (log\\u2082)\": 0.019327387288447687}, {\"type\": \"grad_x\", \"op\": \"act\", \"scale (log\\u2082)\": -0.48110616841105064}, {\"type\": \"grad_x\", \"op\": \"linear_2\", \"scale (log\\u2082)\": -0.4999786214717071}, {\"type\": \"grad_x\", \"op\": \"dropout\", \"scale (log\\u2082)\": -0.0007167437276685347}], \"data-b9ffa847c238961136c7fc5898912583\": [{\"type\": \"w\", \"op\": \"ln\", \"scale (log\\u2082)\": null}, {\"type\": \"w\", \"op\": \"linear_1\", \"scale (log\\u2082)\": -0.0003657674697811288}, {\"type\": \"w\", \"op\": \"linear_2\", \"scale (log\\u2082)\": 0.0009261731868632599}, {\"type\": \"grad_w\", \"op\": \"ln\", \"scale (log\\u2082)\": 0.022852204182724666}, {\"type\": \"grad_w\", \"op\": \"linear_1\", \"scale (log\\u2082)\": -0.48254686164829896}, {\"type\": \"grad_w\", \"op\": \"linear_2\", \"scale (log\\u2082)\": -0.5324439506767508}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.LayerChart(...)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def unit_init(*args):\n",
    "    return 1\n",
    "\n",
    "mlp = UnitScaledMLP(config).apply(init_weights(unit_init))\n",
    "analyse_mlp(mlp, config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is much better! The final scales are almost exactly 1 in both directions, and weights and grad_ws look much better.\n",
    "\n",
    "The compromise fwd & grad_x scaling factors in our linear layers do give temporary non-unit scaling, but this is minor and the second linear layer cancels out the first."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The self-attention layer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regular scaling\n",
    "\n",
    "We start with a basic implementation of a (self-)attention module.\n",
    "\n",
    "(Note that we use [ALiBi](https://arxiv.org/abs/2108.12409) biases over positional embeddings here. This is primarily for simplicity, though these have also been [shown to perform](https://arxiv.org/abs/2210.15424) remarkably well!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: hide\n",
    "class Matmul(nn.Module):\n",
    "    def forward(self, a: Tensor, b: Tensor, scale: float=1.0) -> Tensor:\n",
    "        return (a @ b) * scale\n",
    "\n",
    "def split_heads(tensor: Tensor, num_heads: int, head_size: int) -> Tensor:\n",
    "    batch_size, seq_len, hidden_size = tensor.shape\n",
    "    tensor = tensor.view(batch_size, seq_len, num_heads, head_size)\n",
    "    return tensor.permute(0, 2, 1, 3)\n",
    "\n",
    "\n",
    "def merge_heads(tensor: Tensor, num_heads: int) -> Tensor:\n",
    "    tensor = tensor.permute(0, 2, 1, 3).contiguous()\n",
    "    batch_size, seq_len, num_heads, head_size = tensor.shape\n",
    "    return tensor.view(batch_size, seq_len, num_heads * head_size)\n",
    "\n",
    "\n",
    "def causal_mask(seq_len: int, num_heads: int) -> Tensor:\n",
    "    causal_mask = torch.tril(torch.ones((seq_len, seq_len), dtype=torch.float16))\n",
    "    causal_mask = causal_mask.view(1, 1, seq_len, seq_len)\n",
    "    alibi_mask = gen_alibi_mask(causal_mask, num_heads)\n",
    "    causal_mask = (1.0 - causal_mask) * -10_000\n",
    "    return alibi_mask + causal_mask\n",
    "\n",
    "\n",
    "# Based on https://github.com/labmlai/annotated_deep_learning_paper_implementations/blob/master/labml_nn/transformers/alibi/__init__.py\n",
    "def gen_alibi_mask(causal_mask: Tensor, num_heads: int) -> Tensor:\n",
    "    distances = causal_mask.to(torch.float32).cumsum(dim=-1)\n",
    "    slopes = gen_slopes(num_heads)\n",
    "    return distances.to(torch.float16) * slopes.view(1, num_heads, 1, 1)\n",
    "\n",
    "\n",
    "def gen_slopes(num_heads: int) -> Tensor:\n",
    "    n = 2 ** math.floor(math.log2(num_heads))\n",
    "    m_0 = 2.0 ** (-8.0 / n)\n",
    "    m = torch.pow(m_0, torch.arange(1, 1 + n))\n",
    "    if n < num_heads:\n",
    "        m_hat_0 = 2.0 ** (-4.0 / n)\n",
    "        m_hat = torch.pow(m_hat_0, torch.arange(1, 1 + 2 * (num_heads - n), 2))\n",
    "        m = torch.cat([m, m_hat])\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, config: NanoGPTConfig) -> None:\n",
    "        super().__init__()\n",
    "        self.ln = nn.LayerNorm(config.hidden_size)\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.num_heads = config.num_attention_heads\n",
    "        \n",
    "        self.w_qkv = nn.Linear(config.hidden_size, 3 * config.hidden_size)\n",
    "        self.qk_matmul = Matmul()\n",
    "        self.softmax = nn.Softmax(-1)\n",
    "        self.attn_dropout = nn.Dropout(config.dropout)\n",
    "        self.qkv_matmul = Matmul()\n",
    "        self.w_o = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.residual_dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, hidden_states: Tensor, attention_mask: Tensor) -> Tensor:\n",
    "        seq_len = hidden_states.shape[1]\n",
    "        head_size = self.hidden_size // self.num_heads\n",
    "\n",
    "        hidden_states = self.ln(hidden_states)\n",
    "        q_k_v = self.w_qkv(hidden_states)\n",
    "        q, k, v = q_k_v.split(self.hidden_size, dim=-1)\n",
    "        q, k, v = (split_heads(t, self.num_heads, head_size) for t in (q, k, v))\n",
    "\n",
    "        qk = self.qk_matmul(q, k.transpose(-1, -2), scale=1 / head_size ** 0.5).clone()\n",
    "        qk += causal_mask(seq_len, self.num_heads) + attention_mask\n",
    "        qk = self.softmax(qk)\n",
    "        qk = self.attn_dropout(qk)\n",
    "\n",
    "        qkv = self.qkv_matmul(qk, v)\n",
    "        qkv = merge_heads(qkv, self.num_heads)\n",
    "        qkvo = self.w_o(qkv)\n",
    "        return self.residual_dropout(qkvo).clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_attn(attention, config, batch_size=64, seq_len=16):\n",
    "    stats = instrument(attention)\n",
    "    x = torch.normal(0.0, 1.0, size=(batch_size, seq_len, config.hidden_size))\n",
    "    attention_mask = torch.zeros(batch_size, 1, 1, seq_len)\n",
    "    y = attention(x, attention_mask)\n",
    "    y.backward(torch.normal(0.0, 1.0, size=y.shape))\n",
    "    visualise(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-d4e7dda5b1cc4fc4849bd1603ee87d92\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-d4e7dda5b1cc4fc4849bd1603ee87d92\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-d4e7dda5b1cc4fc4849bd1603ee87d92\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.17.0?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"4.17.0\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}, \"axis\": {\"labelFontSize\": 12, \"titleFontSize\": 16}}, \"layer\": [{\"data\": {\"name\": \"data-413dc4dd2f8390fa2fea33ec27d73093\"}, \"mark\": \"line\", \"encoding\": {\"color\": {\"field\": \"type\", \"legend\": {\"labelFontSize\": 12, \"symbolSize\": 100, \"title\": \"\"}, \"scale\": {\"range\": [\"#6C8EBF\", \"#FF8000\"]}, \"sort\": \"descending\", \"type\": \"nominal\"}, \"x\": {\"axis\": {\"orient\": \"top\", \"values\": [-14.0, -12.0, -10.0, -8.0, -6.0, -4.0, -2.0, 0.0, 2.0, 4.0, 6.0, 8.0, 10.0, 12.0, 14.0, 16.0]}, \"field\": \"scale (log\\u2082)\", \"scale\": {\"domain\": [-14.0, 16.0]}, \"type\": \"quantitative\"}, \"y\": {\"field\": \"op\", \"sort\": [\"ln\", \"w_qkv\", \"qk_matmul\", \"attn_dropout\", \"qkv_matmul\", \"w_o\", \"residual_dropout\"], \"title\": \"\", \"type\": \"ordinal\"}}}, {\"data\": {\"name\": \"data-b9bd60af9deb56fe3e093618a5a750e5\"}, \"mark\": {\"type\": \"point\", \"size\": 100}, \"encoding\": {\"color\": {\"field\": \"type\", \"legend\": {\"labelFontSize\": 12, \"symbolSize\": 100, \"title\": \"\"}, \"scale\": {\"range\": [\"#5D8944\", \"#ED3434\"]}, \"sort\": \"descending\", \"type\": \"nominal\"}, \"shape\": {\"field\": \"type\", \"scale\": {\"range\": [\"square\", \"triangle-down\"]}, \"sort\": \"descending\", \"type\": \"nominal\"}, \"x\": {\"axis\": {\"orient\": \"top\", \"values\": [-14.0, -12.0, -10.0, -8.0, -6.0, -4.0, -2.0, 0.0, 2.0, 4.0, 6.0, 8.0, 10.0, 12.0, 14.0, 16.0]}, \"field\": \"scale (log\\u2082)\", \"scale\": {\"domain\": [-14.0, 16.0]}, \"type\": \"quantitative\"}, \"y\": {\"field\": \"op\", \"sort\": [\"ln\", \"w_qkv\", \"qk_matmul\", \"attn_dropout\", \"qkv_matmul\", \"w_o\", \"residual_dropout\"], \"title\": \"\", \"type\": \"ordinal\"}}}], \"resolve\": {\"scale\": {\"color\": \"independent\", \"shape\": \"independent\"}}, \"width\": 500, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.17.0.json\", \"datasets\": {\"data-413dc4dd2f8390fa2fea33ec27d73093\": [{\"type\": \"x\", \"op\": \"ln\", \"scale (log\\u2082)\": -5.41746367372377e-06}, {\"type\": \"x\", \"op\": \"w_qkv\", \"scale (log\\u2082)\": -0.5006262093637063}, {\"type\": \"x\", \"op\": \"qk_matmul\", \"scale (log\\u2082)\": -1.0028220699361186}, {\"type\": \"x\", \"op\": \"attn_dropout\", \"scale (log\\u2082)\": -3.0437592776116165}, {\"type\": \"x\", \"op\": \"qkv_matmul\", \"scale (log\\u2082)\": -1.3739407624158222}, {\"type\": \"x\", \"op\": \"w_o\", \"scale (log\\u2082)\": -1.3730572411869666}, {\"type\": \"x\", \"op\": \"residual_dropout\", \"scale (log\\u2082)\": -1.2976023846433358}, {\"type\": \"grad_x\", \"op\": \"ln\", \"scale (log\\u2082)\": -1.1344311458203291}, {\"type\": \"grad_x\", \"op\": \"w_qkv\", \"scale (log\\u2082)\": -1.139293674861106}, {\"type\": \"grad_x\", \"op\": \"qk_matmul\", \"scale (log\\u2082)\": -2.340904313840016}, {\"type\": \"grad_x\", \"op\": \"attn_dropout\", \"scale (log\\u2082)\": 2.6477826129226765}, {\"type\": \"grad_x\", \"op\": \"qkv_matmul\", \"scale (log\\u2082)\": -0.7973672206677502}, {\"type\": \"grad_x\", \"op\": \"w_o\", \"scale (log\\u2082)\": 0.07451438270026305}, {\"type\": \"grad_x\", \"op\": \"residual_dropout\", \"scale (log\\u2082)\": 0.07818013199000846}], \"data-b9bd60af9deb56fe3e093618a5a750e5\": [{\"type\": \"w\", \"op\": \"ln\", \"scale (log\\u2082)\": null}, {\"type\": \"w\", \"op\": \"w_qkv\", \"scale (log\\u2082)\": -4.7925517272318245}, {\"type\": \"w\", \"op\": \"w_o\", \"scale (log\\u2082)\": -4.296224080448264}, {\"type\": \"grad_w\", \"op\": \"ln\", \"scale (log\\u2082)\": 3.868451074458224}, {\"type\": \"grad_w\", \"op\": \"w_qkv\", \"scale (log\\u2082)\": 3.5660895850496237}, {\"type\": \"grad_w\", \"op\": \"w_o\", \"scale (log\\u2082)\": 3.7083855148383487}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.LayerChart(...)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "attention = Attention(config).apply(init_weights(glorot_init))\n",
    "analyse_attn(attention, config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again here, the activation scales and grad_x scales fall by half as they go through the layer, and fluctuate in between (Note that we omit the softmax operation here as a) its output isn't normally distributed, and b) we usually do it in higher-precision anyway). We see the same problems as before for weights and grad_ws.\n",
    "\n",
    "Let's fix this:\n",
    "\n",
    "#### Unit scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnitScaledAttention(Attention):\n",
    "    def __init__(self, config: NanoGPTConfig) -> None:\n",
    "        super().__init__(config)\n",
    "        self.ln = UnitScaledLayerNorm(config.hidden_size)\n",
    "        self.w_qkv = UnitScaledLinear(\n",
    "            config.hidden_size, 3 * config.hidden_size, scale_for=\"fwd\"\n",
    "        )\n",
    "        self.qk_matmul = UnitScaledMatmul(scale_for=\"fwd\")\n",
    "        self.softmax = UnitScaledSoftmax(-1)\n",
    "        self.attn_dropout = UnitScaledDropout(config.dropout)\n",
    "        self.qkv_matmul = UnitScaledMatmul(scale_for=\"fwd\")\n",
    "        self.w_o = UnitScaledLinear(config.hidden_size, config.hidden_size)\n",
    "        self.residual_dropout = UnitScaledDropout(config.dropout)\n",
    "\n",
    "class UnitScaledMatmul(Matmul):\n",
    "    def __init__(self, scale_for=\"fwd, grad_a, grad_b\") -> None:\n",
    "        super().__init__()\n",
    "        self.scale_for = scale_for\n",
    "    \n",
    "    def get_scales(self, a: Tensor, b: Tensor):\n",
    "        fwd_scale = a.shape[-1] ** -0.5\n",
    "        grad_a_scale = b.shape[-1] ** -0.5\n",
    "        grad_b_scale = a.shape[-2] ** -0.5\n",
    "        if self.scale_for == \"fwd\":\n",
    "            grad_a_scale = grad_b_scale = fwd_scale\n",
    "        elif self.scale_for == \"fwd, grad_a, grad_b\":\n",
    "            fwd_scale = grad_a_scale = grad_b_scale = geometric_mean([\n",
    "                fwd_scale, grad_a_scale, grad_b_scale\n",
    "            ])\n",
    "        else:\n",
    "            assert False, f\"demo implementation has no {self.scale_for} scaling\"\n",
    "        return fwd_scale, grad_a_scale, grad_b_scale\n",
    "    \n",
    "    def forward(self, a: Tensor, b: Tensor, scale: float=1.0) -> Tensor:\n",
    "        # ignores provided scale\n",
    "        fwd_scale, grad_a_scale, grad_b_scale = self.get_scales(a, b)\n",
    "        a = scaled(a, beta=grad_a_scale)\n",
    "        b = scaled(b, beta=grad_b_scale)\n",
    "        output = a @ b\n",
    "        return scaled(output, alpha=fwd_scale)\n",
    "\n",
    "class UnitScaledSoftmax(nn.Softmax):\n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        fwd_scale = bwd_scale = input.shape[-1] ** 0.5\n",
    "        input = scaled(input, fwd_scale)\n",
    "        output = F.softmax(input, self.dim, _stacklevel=5)\n",
    "        return scaled(output, bwd_scale)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To unit scale the attention layer we again swap out regular layers for unit-scaled ones. This requires altering two more operations: matmul and softmax.\n",
    "\n",
    "Like the linear layer, we provide scaling the matmul based on varying criteria. We find that scaling each matmul and linear layer here for the forward pass ensures good scaling in both directions. We see this empirically in our analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-5867f5ac681f4cbf99925597ae5c92cf\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-5867f5ac681f4cbf99925597ae5c92cf\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-5867f5ac681f4cbf99925597ae5c92cf\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.17.0?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"4.17.0\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}, \"axis\": {\"labelFontSize\": 12, \"titleFontSize\": 16}}, \"layer\": [{\"data\": {\"name\": \"data-0af0d32c84e705fc109614da4a0918b2\"}, \"mark\": \"line\", \"encoding\": {\"color\": {\"field\": \"type\", \"legend\": {\"labelFontSize\": 12, \"symbolSize\": 100, \"title\": \"\"}, \"scale\": {\"range\": [\"#6C8EBF\", \"#FF8000\"]}, \"sort\": \"descending\", \"type\": \"nominal\"}, \"x\": {\"axis\": {\"orient\": \"top\", \"values\": [-14.0, -12.0, -10.0, -8.0, -6.0, -4.0, -2.0, 0.0, 2.0, 4.0, 6.0, 8.0, 10.0, 12.0, 14.0, 16.0]}, \"field\": \"scale (log\\u2082)\", \"scale\": {\"domain\": [-14.0, 16.0]}, \"type\": \"quantitative\"}, \"y\": {\"field\": \"op\", \"sort\": [\"ln\", \"w_qkv\", \"qk_matmul\", \"attn_dropout\", \"qkv_matmul\", \"w_o\", \"residual_dropout\"], \"title\": \"\", \"type\": \"ordinal\"}}}, {\"data\": {\"name\": \"data-e01dd02b8bad112dbbd5267520fea08d\"}, \"mark\": {\"type\": \"point\", \"size\": 100}, \"encoding\": {\"color\": {\"field\": \"type\", \"legend\": {\"labelFontSize\": 12, \"symbolSize\": 100, \"title\": \"\"}, \"scale\": {\"range\": [\"#5D8944\", \"#ED3434\"]}, \"sort\": \"descending\", \"type\": \"nominal\"}, \"shape\": {\"field\": \"type\", \"scale\": {\"range\": [\"square\", \"triangle-down\"]}, \"sort\": \"descending\", \"type\": \"nominal\"}, \"x\": {\"axis\": {\"orient\": \"top\", \"values\": [-14.0, -12.0, -10.0, -8.0, -6.0, -4.0, -2.0, 0.0, 2.0, 4.0, 6.0, 8.0, 10.0, 12.0, 14.0, 16.0]}, \"field\": \"scale (log\\u2082)\", \"scale\": {\"domain\": [-14.0, 16.0]}, \"type\": \"quantitative\"}, \"y\": {\"field\": \"op\", \"sort\": [\"ln\", \"w_qkv\", \"qk_matmul\", \"attn_dropout\", \"qkv_matmul\", \"w_o\", \"residual_dropout\"], \"title\": \"\", \"type\": \"ordinal\"}}}], \"resolve\": {\"scale\": {\"color\": \"independent\", \"shape\": \"independent\"}}, \"width\": 500, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.17.0.json\", \"datasets\": {\"data-0af0d32c84e705fc109614da4a0918b2\": [{\"type\": \"x\", \"op\": \"ln\", \"scale (log\\u2082)\": -6.793330703198998e-06}, {\"type\": \"x\", \"op\": \"w_qkv\", \"scale (log\\u2082)\": 0.0007738863114838626}, {\"type\": \"x\", \"op\": \"qk_matmul\", \"scale (log\\u2082)\": 0.004909480953395364}, {\"type\": \"x\", \"op\": \"attn_dropout\", \"scale (log\\u2082)\": -0.16118501417332778}, {\"type\": \"x\", \"op\": \"qkv_matmul\", \"scale (log\\u2082)\": -0.14670158242171533}, {\"type\": \"x\", \"op\": \"w_o\", \"scale (log\\u2082)\": -0.15292324558231868}, {\"type\": \"x\", \"op\": \"residual_dropout\", \"scale (log\\u2082)\": -0.15277793044968666}, {\"type\": \"grad_x\", \"op\": \"ln\", \"scale (log\\u2082)\": -0.15557676743379045}, {\"type\": \"grad_x\", \"op\": \"w_qkv\", \"scale (log\\u2082)\": -0.15735626528386856}, {\"type\": \"grad_x\", \"op\": \"qk_matmul\", \"scale (log\\u2082)\": -5.145848788036321}, {\"type\": \"grad_x\", \"op\": \"attn_dropout\", \"scale (log\\u2082)\": -0.008765010307936503}, {\"type\": \"grad_x\", \"op\": \"qkv_matmul\", \"scale (log\\u2082)\": -0.15445271436474184}, {\"type\": \"grad_x\", \"op\": \"w_o\", \"scale (log\\u2082)\": -0.005396749957842205}, {\"type\": \"grad_x\", \"op\": \"residual_dropout\", \"scale (log\\u2082)\": -0.0007089146496972608}], \"data-e01dd02b8bad112dbbd5267520fea08d\": [{\"type\": \"w\", \"op\": \"ln\", \"scale (log\\u2082)\": null}, {\"type\": \"w\", \"op\": \"w_qkv\", \"scale (log\\u2082)\": 0.0006657631850158064}, {\"type\": \"w\", \"op\": \"w_o\", \"scale (log\\u2082)\": -0.004634284512675989}, {\"type\": \"grad_w\", \"op\": \"ln\", \"scale (log\\u2082)\": -0.09207182789544821}, {\"type\": \"grad_w\", \"op\": \"w_qkv\", \"scale (log\\u2082)\": -0.9471416121030466}, {\"type\": \"grad_w\", \"op\": \"w_o\", \"scale (log\\u2082)\": -0.15195736589380004}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.LayerChart(...)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "attention = UnitScaledAttention(config).apply(init_weights(unit_init))\n",
    "analyse_attn(attention, config, batch_size=64, seq_len=64)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, a great improvement for all four types of tensors, with each having approximate unit scale. Note that these scaling rules are robust to changes in hyperparameters too. You can experiment with the initial config to verify this.\n",
    "\n",
    "We now have our key building-blocks ðŸ§± Let's put it all together!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The transformer block\n",
    "\n",
    "#### Regular scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, config: NanoGPTConfig) -> None:\n",
    "        super().__init__()\n",
    "        self.attention_layer = Residual(Attention(config))\n",
    "        self.mlp_layer = Residual(MLP(config))\n",
    "        \n",
    "    def forward(self, hidden_states: Tensor, attention_mask: Tensor) -> Tensor:\n",
    "        hidden_states = self.attention_layer(hidden_states, attention_mask)\n",
    "        return self.mlp_layer(hidden_states)\n",
    "\n",
    "\n",
    "class Residual(nn.Module):\n",
    "    def __init__(self, f: nn.Module):\n",
    "        super().__init__()\n",
    "        self.f = f\n",
    "    \n",
    "    def forward(self, x: Tensor, *args):\n",
    "        return x + self.f(x, *args)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unit scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnitScaledTransformerBlock(TransformerBlock):\n",
    "    def __init__(self, config: NanoGPTConfig) -> None:\n",
    "        super().__init__(config)\n",
    "        self.attention_layer = UnitScaledResidual(UnitScaledAttention(config))\n",
    "        self.mlp_layer = UnitScaledResidual(UnitScaledMLP(config))\n",
    "\n",
    "class UnitScaledResidual(Residual):\n",
    "    def __init__(self, f: nn.Module, tau: float=0.2):\n",
    "        super().__init__(f)\n",
    "        self.tau = tau\n",
    "    \n",
    "    def forward(self, x: Tensor, *args):\n",
    "        y = x * (1 - self.tau) ** 0.5\n",
    "        z = scaled(x, beta=self.tau ** 0.5)\n",
    "        z = self.f(z, *args)\n",
    "        z = scaled(z, alpha=self.tau ** 0.5)\n",
    "        return y + z"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the scaling in our residual layer includes an important trick.\n",
    "\n",
    "It's necessary for unit-scaled models to use a weighted sum when doing their residual-add, to down-weight the residual/trunk branch. If this is not included, training can fail as too much signal comes from each residual (regular models avoid this as their residual implicitly reduces scale).\n",
    "\n",
    "However, the naÃ¯ve implementation of this breaks unit scale. We get around this by delaying the weighting until the end of the residual branch in the backward pass (see paper for more details)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The full transformer\n",
    "\n",
    "#### Regular scaling"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that our implementation contains a little extra boilerplate to make it ðŸ¤—-compliant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NanoGPTModel(PreTrainedModel):\n",
    "    config_class = NanoGPTConfig\n",
    "\n",
    "    def __init__(self, config: NanoGPTConfig) -> None:\n",
    "        super().__init__(config)\n",
    "        self.input_embeddings = nn.Embedding(config.vocab_size, config.hidden_size)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        self.ln = nn.LayerNorm(config.hidden_size)\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [TransformerBlock(config) for _ in range(config.num_hidden_layers)]\n",
    "        )\n",
    "        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        self.apply(init_weights(glorot_init))\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Tensor,\n",
    "        attention_mask: Tensor,\n",
    "        position_ids: Optional[Tensor] = None,\n",
    "        labels: Optional[Tensor] = None,\n",
    "        **kwargs,\n",
    "    ) -> CausalLMOutputWithCrossAttentions:\n",
    "        attention_mask = attention_mask[:, None, None, :].to(dtype=self.dtype)\n",
    "        attention_mask = (1.0 - attention_mask) * -10_000\n",
    "\n",
    "        hidden_states = self.input_embeddings(input_ids)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        for block in self.blocks:\n",
    "            hidden_states = block(hidden_states, attention_mask=attention_mask)\n",
    "        hidden_states = self.ln(hidden_states)\n",
    "        logits = self.lm_head(hidden_states)\n",
    "\n",
    "        if labels is None:\n",
    "            return CausalLMOutputWithCrossAttentions(logits=logits)\n",
    "        else:\n",
    "            labels = torch.roll(labels, -1, 1)\n",
    "            labels[:, -1] = -100  # By default, ignore_index of CrossEntropyLoss is -100\n",
    "            loss = self.loss_fn(logits.view(-1, logits.shape[-1]), labels.view(-1))\n",
    "            return CausalLMOutputWithCrossAttentions(loss=loss)\n",
    "\n",
    "    def get_input_embeddings(self) -> nn.Embedding:\n",
    "        return self.input_embeddings\n",
    "\n",
    "    def prepare_inputs_for_generation(\n",
    "        self, input_ids: Tensor, **kwargs\n",
    "    ) -> Dict[str, Tensor]:\n",
    "        attention_mask = kwargs[\"attention_mask\"]\n",
    "        position_ids = attention_mask.long().cumsum(-1) - 1\n",
    "        position_ids.masked_fill_(attention_mask == 0, 1)\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"position_ids\": position_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_full_model(model, config, batch_size=64, seq_len=16):\n",
    "    stats = instrument(model)\n",
    "    input_ids = labels = torch.randint(0, config.vocab_size, size=(batch_size, seq_len))\n",
    "    attention_mask = torch.zeros(batch_size, seq_len)\n",
    "    y = model(input_ids, attention_mask, labels=labels).loss\n",
    "    y.backward()\n",
    "    visualise(stats, subnormal=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the scaling for the entire (non-unit-scaled) transformer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-54476c997459412299a82f1682aa800b\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-54476c997459412299a82f1682aa800b\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-54476c997459412299a82f1682aa800b\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.17.0?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"4.17.0\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}, \"axis\": {\"labelFontSize\": 12, \"titleFontSize\": 16}}, \"layer\": [{\"data\": {\"name\": \"data-436d4d32b0c9190c0e14d202b1230c50\"}, \"mark\": \"line\", \"encoding\": {\"color\": {\"field\": \"type\", \"legend\": {\"labelFontSize\": 12, \"symbolSize\": 100, \"title\": \"\"}, \"scale\": {\"range\": [\"#6C8EBF\", \"#FF8000\"]}, \"sort\": \"descending\", \"type\": \"nominal\"}, \"x\": {\"axis\": {\"orient\": \"top\", \"values\": [-18.0, -16.0, -14.0, -12.0, -10.0, -8.0, -6.0, -4.0, -2.0, 0.0, 2.0, 4.0, 6.0, 8.0, 10.0, 12.0, 14.0, 16.0, 18.0]}, \"field\": \"scale (log\\u2082)\", \"scale\": {\"domain\": [-18.0, 18.0]}, \"type\": \"quantitative\"}, \"y\": {\"field\": \"op\", \"sort\": [\"input_embeddings\", \"dropout\", \"ln\", \"0.attention_layer.f.ln\", \"0.attention_layer.f.w_qkv\", \"0.attention_layer.f.qk_matmul\", \"0.attention_layer.f.attn_dropout\", \"0.attention_layer.f.qkv_matmul\", \"0.attention_layer.f.w_o\", \"0.attention_layer.f.residual_dropout\", \"0.mlp_layer.f.ln\", \"0.mlp_layer.f.linear_1\", \"0.mlp_layer.f.act\", \"0.mlp_layer.f.linear_2\", \"0.mlp_layer.f.dropout\", \"1.attention_layer.f.ln\", \"1.attention_layer.f.w_qkv\", \"1.attention_layer.f.qk_matmul\", \"1.attention_layer.f.attn_dropout\", \"1.attention_layer.f.qkv_matmul\", \"1.attention_layer.f.w_o\", \"1.attention_layer.f.residual_dropout\", \"1.mlp_layer.f.ln\", \"1.mlp_layer.f.linear_1\", \"1.mlp_layer.f.act\", \"1.mlp_layer.f.linear_2\", \"1.mlp_layer.f.dropout\", \"2.attention_layer.f.ln\", \"2.attention_layer.f.w_qkv\", \"2.attention_layer.f.qk_matmul\", \"2.attention_layer.f.attn_dropout\", \"2.attention_layer.f.qkv_matmul\", \"2.attention_layer.f.w_o\", \"2.attention_layer.f.residual_dropout\", \"2.mlp_layer.f.ln\", \"2.mlp_layer.f.linear_1\", \"2.mlp_layer.f.act\", \"2.mlp_layer.f.linear_2\", \"2.mlp_layer.f.dropout\", \"3.attention_layer.f.ln\", \"3.attention_layer.f.w_qkv\", \"3.attention_layer.f.qk_matmul\", \"3.attention_layer.f.attn_dropout\", \"3.attention_layer.f.qkv_matmul\", \"3.attention_layer.f.w_o\", \"3.attention_layer.f.residual_dropout\", \"3.mlp_layer.f.ln\", \"3.mlp_layer.f.linear_1\", \"3.mlp_layer.f.act\", \"3.mlp_layer.f.linear_2\", \"3.mlp_layer.f.dropout\", \"4.attention_layer.f.ln\", \"4.attention_layer.f.w_qkv\", \"4.attention_layer.f.qk_matmul\", \"4.attention_layer.f.attn_dropout\", \"4.attention_layer.f.qkv_matmul\", \"4.attention_layer.f.w_o\", \"4.attention_layer.f.residual_dropout\", \"4.mlp_layer.f.ln\", \"4.mlp_layer.f.linear_1\", \"4.mlp_layer.f.act\", \"4.mlp_layer.f.linear_2\", \"4.mlp_layer.f.dropout\", \"5.attention_layer.f.ln\", \"5.attention_layer.f.w_qkv\", \"5.attention_layer.f.qk_matmul\", \"5.attention_layer.f.attn_dropout\", \"5.attention_layer.f.qkv_matmul\", \"5.attention_layer.f.w_o\", \"5.attention_layer.f.residual_dropout\", \"5.mlp_layer.f.ln\", \"5.mlp_layer.f.linear_1\", \"5.mlp_layer.f.act\", \"5.mlp_layer.f.linear_2\", \"5.mlp_layer.f.dropout\", \"lm_head\"], \"title\": \"\", \"type\": \"ordinal\"}}}, {\"data\": {\"name\": \"data-f218933b5d94522484598ff4fcd17e15\"}, \"mark\": {\"type\": \"point\", \"size\": 100}, \"encoding\": {\"color\": {\"field\": \"type\", \"legend\": {\"labelFontSize\": 12, \"symbolSize\": 100, \"title\": \"\"}, \"scale\": {\"range\": [\"#5D8944\", \"#ED3434\"]}, \"sort\": \"descending\", \"type\": \"nominal\"}, \"shape\": {\"field\": \"type\", \"scale\": {\"range\": [\"square\", \"triangle-down\"]}, \"sort\": \"descending\", \"type\": \"nominal\"}, \"x\": {\"axis\": {\"orient\": \"top\", \"values\": [-18.0, -16.0, -14.0, -12.0, -10.0, -8.0, -6.0, -4.0, -2.0, 0.0, 2.0, 4.0, 6.0, 8.0, 10.0, 12.0, 14.0, 16.0, 18.0]}, \"field\": \"scale (log\\u2082)\", \"scale\": {\"domain\": [-18.0, 18.0]}, \"type\": \"quantitative\"}, \"y\": {\"field\": \"op\", \"sort\": [\"input_embeddings\", \"dropout\", \"ln\", \"0.attention_layer.f.ln\", \"0.attention_layer.f.w_qkv\", \"0.attention_layer.f.qk_matmul\", \"0.attention_layer.f.attn_dropout\", \"0.attention_layer.f.qkv_matmul\", \"0.attention_layer.f.w_o\", \"0.attention_layer.f.residual_dropout\", \"0.mlp_layer.f.ln\", \"0.mlp_layer.f.linear_1\", \"0.mlp_layer.f.act\", \"0.mlp_layer.f.linear_2\", \"0.mlp_layer.f.dropout\", \"1.attention_layer.f.ln\", \"1.attention_layer.f.w_qkv\", \"1.attention_layer.f.qk_matmul\", \"1.attention_layer.f.attn_dropout\", \"1.attention_layer.f.qkv_matmul\", \"1.attention_layer.f.w_o\", \"1.attention_layer.f.residual_dropout\", \"1.mlp_layer.f.ln\", \"1.mlp_layer.f.linear_1\", \"1.mlp_layer.f.act\", \"1.mlp_layer.f.linear_2\", \"1.mlp_layer.f.dropout\", \"2.attention_layer.f.ln\", \"2.attention_layer.f.w_qkv\", \"2.attention_layer.f.qk_matmul\", \"2.attention_layer.f.attn_dropout\", \"2.attention_layer.f.qkv_matmul\", \"2.attention_layer.f.w_o\", \"2.attention_layer.f.residual_dropout\", \"2.mlp_layer.f.ln\", \"2.mlp_layer.f.linear_1\", \"2.mlp_layer.f.act\", \"2.mlp_layer.f.linear_2\", \"2.mlp_layer.f.dropout\", \"3.attention_layer.f.ln\", \"3.attention_layer.f.w_qkv\", \"3.attention_layer.f.qk_matmul\", \"3.attention_layer.f.attn_dropout\", \"3.attention_layer.f.qkv_matmul\", \"3.attention_layer.f.w_o\", \"3.attention_layer.f.residual_dropout\", \"3.mlp_layer.f.ln\", \"3.mlp_layer.f.linear_1\", \"3.mlp_layer.f.act\", \"3.mlp_layer.f.linear_2\", \"3.mlp_layer.f.dropout\", \"4.attention_layer.f.ln\", \"4.attention_layer.f.w_qkv\", \"4.attention_layer.f.qk_matmul\", \"4.attention_layer.f.attn_dropout\", \"4.attention_layer.f.qkv_matmul\", \"4.attention_layer.f.w_o\", \"4.attention_layer.f.residual_dropout\", \"4.mlp_layer.f.ln\", \"4.mlp_layer.f.linear_1\", \"4.mlp_layer.f.act\", \"4.mlp_layer.f.linear_2\", \"4.mlp_layer.f.dropout\", \"5.attention_layer.f.ln\", \"5.attention_layer.f.w_qkv\", \"5.attention_layer.f.qk_matmul\", \"5.attention_layer.f.attn_dropout\", \"5.attention_layer.f.qkv_matmul\", \"5.attention_layer.f.w_o\", \"5.attention_layer.f.residual_dropout\", \"5.mlp_layer.f.ln\", \"5.mlp_layer.f.linear_1\", \"5.mlp_layer.f.act\", \"5.mlp_layer.f.linear_2\", \"5.mlp_layer.f.dropout\", \"lm_head\"], \"title\": \"\", \"type\": \"ordinal\"}}}, {\"data\": {\"name\": \"empty\"}, \"mark\": {\"type\": \"rule\", \"strokeDash\": [4, 4]}, \"encoding\": {\"x\": {\"datum\": -14}}}, {\"data\": {\"name\": \"empty\"}, \"mark\": {\"type\": \"rule\", \"strokeDash\": [4, 4]}, \"encoding\": {\"x\": {\"datum\": 16}}}, {\"data\": {\"name\": \"empty\"}, \"mark\": {\"type\": \"text\", \"dy\": -740}, \"encoding\": {\"text\": {\"value\": \"Min FP16 (normal)\"}, \"x\": {\"datum\": -10}}}, {\"data\": {\"name\": \"empty\"}, \"mark\": {\"type\": \"text\", \"dy\": -740}, \"encoding\": {\"text\": {\"value\": \"Max FP16\"}, \"x\": {\"datum\": 13}}}], \"resolve\": {\"scale\": {\"color\": \"independent\", \"shape\": \"independent\"}}, \"width\": 500, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.17.0.json\", \"datasets\": {\"data-436d4d32b0c9190c0e14d202b1230c50\": [{\"type\": \"x\", \"op\": \"input_embeddings\", \"scale (log\\u2082)\": -0.0035756471175330423}, {\"type\": \"x\", \"op\": \"dropout\", \"scale (log\\u2082)\": 0.07279681135903299}, {\"type\": \"x\", \"op\": \"ln\", \"scale (log\\u2082)\": -8.599132799414562e-08}, {\"type\": \"x\", \"op\": \"0.attention_layer.f.ln\", \"scale (log\\u2082)\": -4.72953065103632e-06}, {\"type\": \"x\", \"op\": \"0.attention_layer.f.w_qkv\", \"scale (log\\u2082)\": -0.5013033005776791}, {\"type\": \"x\", \"op\": \"0.attention_layer.f.qk_matmul\", \"scale (log\\u2082)\": -0.9980366068137884}, {\"type\": \"x\", \"op\": \"0.attention_layer.f.attn_dropout\", \"scale (log\\u2082)\": -3.045228073118974}, {\"type\": \"x\", \"op\": \"0.attention_layer.f.qkv_matmul\", \"scale (log\\u2082)\": -1.37087943586497}, {\"type\": \"x\", \"op\": \"0.attention_layer.f.w_o\", \"scale (log\\u2082)\": -1.363951503263564}, {\"type\": \"x\", \"op\": \"0.attention_layer.f.residual_dropout\", \"scale (log\\u2082)\": -1.2864762955248576}, {\"type\": \"x\", \"op\": \"0.mlp_layer.f.ln\", \"scale (log\\u2082)\": -3.955606392614896e-06}, {\"type\": \"x\", \"op\": \"0.mlp_layer.f.linear_1\", \"scale (log\\u2082)\": -0.6613200554005013}, {\"type\": \"x\", \"op\": \"0.mlp_layer.f.act\", \"scale (log\\u2082)\": -1.4866643085673348}, {\"type\": \"x\", \"op\": \"0.mlp_layer.f.linear_2\", \"scale (log\\u2082)\": -1.0456938024321}, {\"type\": \"x\", \"op\": \"0.mlp_layer.f.dropout\", \"scale (log\\u2082)\": -0.9686405429024201}, {\"type\": \"x\", \"op\": \"1.attention_layer.f.ln\", \"scale (log\\u2082)\": -2.92370802720058e-06}, {\"type\": \"x\", \"op\": \"1.attention_layer.f.w_qkv\", \"scale (log\\u2082)\": -0.5025250345828077}, {\"type\": \"x\", \"op\": \"1.attention_layer.f.qk_matmul\", \"scale (log\\u2082)\": -1.0034639301325634}, {\"type\": \"x\", \"op\": \"1.attention_layer.f.attn_dropout\", \"scale (log\\u2082)\": -3.057342770102914}, {\"type\": \"x\", \"op\": \"1.attention_layer.f.qkv_matmul\", \"scale (log\\u2082)\": -1.2650588919353345}, {\"type\": \"x\", \"op\": \"1.attention_layer.f.w_o\", \"scale (log\\u2082)\": -1.2712371413386625}, {\"type\": \"x\", \"op\": \"1.attention_layer.f.residual_dropout\", \"scale (log\\u2082)\": -1.1961245976903905}, {\"type\": \"x\", \"op\": \"1.mlp_layer.f.ln\", \"scale (log\\u2082)\": -2.4937505927785606e-06}, {\"type\": \"x\", \"op\": \"1.mlp_layer.f.linear_1\", \"scale (log\\u2082)\": -0.6621558678191569}, {\"type\": \"x\", \"op\": \"1.mlp_layer.f.act\", \"scale (log\\u2082)\": -1.4960397598375808}, {\"type\": \"x\", \"op\": \"1.mlp_layer.f.linear_2\", \"scale (log\\u2082)\": -1.067820578664795}, {\"type\": \"x\", \"op\": \"1.mlp_layer.f.dropout\", \"scale (log\\u2082)\": -0.9911024537833744}, {\"type\": \"x\", \"op\": \"2.attention_layer.f.ln\", \"scale (log\\u2082)\": -1.8918103998587215e-06}, {\"type\": \"x\", \"op\": \"2.attention_layer.f.w_qkv\", \"scale (log\\u2082)\": -0.5029413642824228}, {\"type\": \"x\", \"op\": \"2.attention_layer.f.qk_matmul\", \"scale (log\\u2082)\": -1.0070600219830816}, {\"type\": \"x\", \"op\": \"2.attention_layer.f.attn_dropout\", \"scale (log\\u2082)\": -3.0580675454194446}, {\"type\": \"x\", \"op\": \"2.attention_layer.f.qkv_matmul\", \"scale (log\\u2082)\": -1.1817999328618773}, {\"type\": \"x\", \"op\": \"2.attention_layer.f.w_o\", \"scale (log\\u2082)\": -1.1744164655762181}, {\"type\": \"x\", \"op\": \"2.attention_layer.f.residual_dropout\", \"scale (log\\u2082)\": -1.0985672081270086}, {\"type\": \"x\", \"op\": \"2.mlp_layer.f.ln\", \"scale (log\\u2082)\": -1.5478446880940214e-06}, {\"type\": \"x\", \"op\": \"2.mlp_layer.f.linear_1\", \"scale (log\\u2082)\": -0.6619162568413329}, {\"type\": \"x\", \"op\": \"2.mlp_layer.f.act\", \"scale (log\\u2082)\": -1.4929854874403896}, {\"type\": \"x\", \"op\": \"2.mlp_layer.f.linear_2\", \"scale (log\\u2082)\": -1.0570248001370892}, {\"type\": \"x\", \"op\": \"2.mlp_layer.f.dropout\", \"scale (log\\u2082)\": -0.9819273666376281}, {\"type\": \"x\", \"op\": \"3.attention_layer.f.ln\", \"scale (log\\u2082)\": -1.2038790583372158e-06}, {\"type\": \"x\", \"op\": \"3.attention_layer.f.w_qkv\", \"scale (log\\u2082)\": -0.5025235727059938}, {\"type\": \"x\", \"op\": \"3.attention_layer.f.qk_matmul\", \"scale (log\\u2082)\": -1.005302412278359}, {\"type\": \"x\", \"op\": \"3.attention_layer.f.attn_dropout\", \"scale (log\\u2082)\": -3.0601620236904936}, {\"type\": \"x\", \"op\": \"3.attention_layer.f.qkv_matmul\", \"scale (log\\u2082)\": -1.1244541410141837}, {\"type\": \"x\", \"op\": \"3.attention_layer.f.w_o\", \"scale (log\\u2082)\": -1.1295049910677248}, {\"type\": \"x\", \"op\": \"3.attention_layer.f.residual_dropout\", \"scale (log\\u2082)\": -1.054039713679807}, {\"type\": \"x\", \"op\": \"3.mlp_layer.f.ln\", \"scale (log\\u2082)\": -9.459048898372686e-07}, {\"type\": \"x\", \"op\": \"3.mlp_layer.f.linear_1\", \"scale (log\\u2082)\": -0.6621987325821326}, {\"type\": \"x\", \"op\": \"3.mlp_layer.f.act\", \"scale (log\\u2082)\": -1.487846208416515}, {\"type\": \"x\", \"op\": \"3.mlp_layer.f.linear_2\", \"scale (log\\u2082)\": -1.0453286075192127}, {\"type\": \"x\", \"op\": \"3.mlp_layer.f.dropout\", \"scale (log\\u2082)\": -0.9692616469361035}, {\"type\": \"x\", \"op\": \"4.attention_layer.f.ln\", \"scale (log\\u2082)\": -6.879307674667236e-07}, {\"type\": \"x\", \"op\": \"4.attention_layer.f.w_qkv\", \"scale (log\\u2082)\": -0.4994922702721279}, {\"type\": \"x\", \"op\": \"4.attention_layer.f.qk_matmul\", \"scale (log\\u2082)\": -0.9995721706070576}, {\"type\": \"x\", \"op\": \"4.attention_layer.f.attn_dropout\", \"scale (log\\u2082)\": -3.0671694442955784}, {\"type\": \"x\", \"op\": \"4.attention_layer.f.qkv_matmul\", \"scale (log\\u2082)\": -1.0746918638432705}, {\"type\": \"x\", \"op\": \"4.attention_layer.f.w_o\", \"scale (log\\u2082)\": -1.080192753283568}, {\"type\": \"x\", \"op\": \"4.attention_layer.f.residual_dropout\", \"scale (log\\u2082)\": -1.0047953558494567}, {\"type\": \"x\", \"op\": \"4.mlp_layer.f.ln\", \"scale (log\\u2082)\": -5.159480448471312e-07}, {\"type\": \"x\", \"op\": \"4.mlp_layer.f.linear_1\", \"scale (log\\u2082)\": -0.6630174898831748}, {\"type\": \"x\", \"op\": \"4.mlp_layer.f.act\", \"scale (log\\u2082)\": -1.4922245917476784}, {\"type\": \"x\", \"op\": \"4.mlp_layer.f.linear_2\", \"scale (log\\u2082)\": -1.0626280657422424}, {\"type\": \"x\", \"op\": \"4.mlp_layer.f.dropout\", \"scale (log\\u2082)\": -0.9871923726641583}, {\"type\": \"x\", \"op\": \"5.attention_layer.f.ln\", \"scale (log\\u2082)\": -3.4396534272948306e-07}, {\"type\": \"x\", \"op\": \"5.attention_layer.f.w_qkv\", \"scale (log\\u2082)\": -0.49990176707535705}, {\"type\": \"x\", \"op\": \"5.attention_layer.f.qk_matmul\", \"scale (log\\u2082)\": -0.9910949330922089}, {\"type\": \"x\", \"op\": \"5.attention_layer.f.attn_dropout\", \"scale (log\\u2082)\": -3.0655095077342596}, {\"type\": \"x\", \"op\": \"5.attention_layer.f.qkv_matmul\", \"scale (log\\u2082)\": -1.0408064387671092}, {\"type\": \"x\", \"op\": \"5.attention_layer.f.w_o\", \"scale (log\\u2082)\": -1.0346776383693617}, {\"type\": \"x\", \"op\": \"5.attention_layer.f.residual_dropout\", \"scale (log\\u2082)\": -0.9578076035944022}, {\"type\": \"x\", \"op\": \"5.mlp_layer.f.ln\", \"scale (log\\u2082)\": -1.7198266111377426e-07}, {\"type\": \"x\", \"op\": \"5.mlp_layer.f.linear_1\", \"scale (log\\u2082)\": -0.6596183671880178}, {\"type\": \"x\", \"op\": \"5.mlp_layer.f.act\", \"scale (log\\u2082)\": -1.479171776670123}, {\"type\": \"x\", \"op\": \"5.mlp_layer.f.linear_2\", \"scale (log\\u2082)\": -1.0509833744151158}, {\"type\": \"x\", \"op\": \"5.mlp_layer.f.dropout\", \"scale (log\\u2082)\": -0.9747110390779149}, {\"type\": \"x\", \"op\": \"lm_head\", \"scale (log\\u2082)\": -0.00817042632627266}, {\"type\": \"grad_x\", \"op\": \"dropout\", \"scale (log\\u2082)\": -14.16867893422051}, {\"type\": \"grad_x\", \"op\": \"ln\", \"scale (log\\u2082)\": -15.223988428571662}, {\"type\": \"grad_x\", \"op\": \"0.attention_layer.f.ln\", \"scale (log\\u2082)\": -15.390935140447665}, {\"type\": \"grad_x\", \"op\": \"0.attention_layer.f.w_qkv\", \"scale (log\\u2082)\": -15.32294979705157}, {\"type\": \"grad_x\", \"op\": \"0.attention_layer.f.attn_dropout\", \"scale (log\\u2082)\": -11.763930610425652}, {\"type\": \"grad_x\", \"op\": \"0.attention_layer.f.w_o\", \"scale (log\\u2082)\": -14.330220138421073}, {\"type\": \"grad_x\", \"op\": \"0.attention_layer.f.residual_dropout\", \"scale (log\\u2082)\": -14.332090117501206}, {\"type\": \"grad_x\", \"op\": \"0.mlp_layer.f.ln\", \"scale (log\\u2082)\": -15.649316780840874}, {\"type\": \"grad_x\", \"op\": \"0.mlp_layer.f.linear_1\", \"scale (log\\u2082)\": -15.474991124493858}, {\"type\": \"grad_x\", \"op\": \"0.mlp_layer.f.act\", \"scale (log\\u2082)\": -15.811361053536329}, {\"type\": \"grad_x\", \"op\": \"0.mlp_layer.f.linear_2\", \"scale (log\\u2082)\": -15.13292557394234}, {\"type\": \"grad_x\", \"op\": \"0.mlp_layer.f.dropout\", \"scale (log\\u2082)\": -14.47403312987295}, {\"type\": \"grad_x\", \"op\": \"1.attention_layer.f.ln\", \"scale (log\\u2082)\": -16.031424867887498}, {\"type\": \"grad_x\", \"op\": \"1.attention_layer.f.w_qkv\", \"scale (log\\u2082)\": -15.655385096890157}, {\"type\": \"grad_x\", \"op\": \"1.attention_layer.f.attn_dropout\", \"scale (log\\u2082)\": -12.0099009713935}, {\"type\": \"grad_x\", \"op\": \"1.attention_layer.f.w_o\", \"scale (log\\u2082)\": -14.573041849920827}, {\"type\": \"grad_x\", \"op\": \"1.attention_layer.f.residual_dropout\", \"scale (log\\u2082)\": -14.572984540394806}, {\"type\": \"grad_x\", \"op\": \"1.mlp_layer.f.ln\", \"scale (log\\u2082)\": -16.069980273557587}, {\"type\": \"grad_x\", \"op\": \"1.mlp_layer.f.linear_1\", \"scale (log\\u2082)\": -15.688830290565555}, {\"type\": \"grad_x\", \"op\": \"1.mlp_layer.f.act\", \"scale (log\\u2082)\": -16.028868364242054}, {\"type\": \"grad_x\", \"op\": \"1.mlp_layer.f.linear_2\", \"scale (log\\u2082)\": -15.341831564637099}, {\"type\": \"grad_x\", \"op\": \"1.mlp_layer.f.dropout\", \"scale (log\\u2082)\": -14.679670440423838}, {\"type\": \"grad_x\", \"op\": \"2.attention_layer.f.ln\", \"scale (log\\u2082)\": -16.482983304366623}, {\"type\": \"grad_x\", \"op\": \"2.attention_layer.f.w_qkv\", \"scale (log\\u2082)\": -15.911105516871984}, {\"type\": \"grad_x\", \"op\": \"2.attention_layer.f.attn_dropout\", \"scale (log\\u2082)\": -12.171194200335192}, {\"type\": \"grad_x\", \"op\": \"2.attention_layer.f.w_o\", \"scale (log\\u2082)\": -14.741874236987}, {\"type\": \"grad_x\", \"op\": \"2.attention_layer.f.residual_dropout\", \"scale (log\\u2082)\": -14.745527682570806}, {\"type\": \"grad_x\", \"op\": \"2.mlp_layer.f.ln\", \"scale (log\\u2082)\": -16.378523558899072}, {\"type\": \"grad_x\", \"op\": \"2.mlp_layer.f.linear_1\", \"scale (log\\u2082)\": -15.835670763039698}, {\"type\": \"grad_x\", \"op\": \"2.mlp_layer.f.act\", \"scale (log\\u2082)\": -16.174248495849138}, {\"type\": \"grad_x\", \"op\": \"2.mlp_layer.f.linear_2\", \"scale (log\\u2082)\": -15.490334917770971}, {\"type\": \"grad_x\", \"op\": \"2.mlp_layer.f.dropout\", \"scale (log\\u2082)\": -14.831569499023837}, {\"type\": \"grad_x\", \"op\": \"3.attention_layer.f.ln\", \"scale (log\\u2082)\": -16.838953329200194}, {\"type\": \"grad_x\", \"op\": \"3.attention_layer.f.w_qkv\", \"scale (log\\u2082)\": -16.115569345734773}, {\"type\": \"grad_x\", \"op\": \"3.attention_layer.f.attn_dropout\", \"scale (log\\u2082)\": -12.319171354750257}, {\"type\": \"grad_x\", \"op\": \"3.attention_layer.f.w_o\", \"scale (log\\u2082)\": -14.885505429137556}, {\"type\": \"grad_x\", \"op\": \"3.attention_layer.f.residual_dropout\", \"scale (log\\u2082)\": -14.88335922042543}, {\"type\": \"grad_x\", \"op\": \"3.mlp_layer.f.ln\", \"scale (log\\u2082)\": -16.637975399819492}, {\"type\": \"grad_x\", \"op\": \"3.mlp_layer.f.linear_1\", \"scale (log\\u2082)\": -15.95735737240062}, {\"type\": \"grad_x\", \"op\": \"3.mlp_layer.f.act\", \"scale (log\\u2082)\": -16.297726155219276}, {\"type\": \"grad_x\", \"op\": \"3.mlp_layer.f.linear_2\", \"scale (log\\u2082)\": -15.620409311438635}, {\"type\": \"grad_x\", \"op\": \"3.mlp_layer.f.dropout\", \"scale (log\\u2082)\": -14.95744370035805}, {\"type\": \"grad_x\", \"op\": \"4.attention_layer.f.ln\", \"scale (log\\u2082)\": -17.13924149175071}, {\"type\": \"grad_x\", \"op\": \"4.attention_layer.f.w_qkv\", \"scale (log\\u2082)\": -16.285118738689587}, {\"type\": \"grad_x\", \"op\": \"4.attention_layer.f.attn_dropout\", \"scale (log\\u2082)\": -12.427641511779855}, {\"type\": \"grad_x\", \"op\": \"4.attention_layer.f.w_o\", \"scale (log\\u2082)\": -14.999596585108959}, {\"type\": \"grad_x\", \"op\": \"4.attention_layer.f.residual_dropout\", \"scale (log\\u2082)\": -14.998076624828796}, {\"type\": \"grad_x\", \"op\": \"4.mlp_layer.f.ln\", \"scale (log\\u2082)\": -16.86561253736478}, {\"type\": \"grad_x\", \"op\": \"4.mlp_layer.f.linear_1\", \"scale (log\\u2082)\": -16.070730031352923}, {\"type\": \"grad_x\", \"op\": \"4.mlp_layer.f.act\", \"scale (log\\u2082)\": -16.407214618385723}, {\"type\": \"grad_x\", \"op\": \"4.mlp_layer.f.linear_2\", \"scale (log\\u2082)\": -15.721168169507225}, {\"type\": \"grad_x\", \"op\": \"4.mlp_layer.f.dropout\", \"scale (log\\u2082)\": -15.059823085210262}, {\"type\": \"grad_x\", \"op\": \"5.attention_layer.f.ln\", \"scale (log\\u2082)\": -17.37984171368871}, {\"type\": \"grad_x\", \"op\": \"5.attention_layer.f.w_qkv\", \"scale (log\\u2082)\": -16.424627986795997}, {\"type\": \"grad_x\", \"op\": \"5.attention_layer.f.attn_dropout\", \"scale (log\\u2082)\": -12.51206161269945}, {\"type\": \"grad_x\", \"op\": \"5.attention_layer.f.w_o\", \"scale (log\\u2082)\": -15.09154333223278}, {\"type\": \"grad_x\", \"op\": \"5.attention_layer.f.residual_dropout\", \"scale (log\\u2082)\": -15.091453726189723}, {\"type\": \"grad_x\", \"op\": \"5.mlp_layer.f.ln\", \"scale (log\\u2082)\": -17.049298167542176}, {\"type\": \"grad_x\", \"op\": \"5.mlp_layer.f.linear_1\", \"scale (log\\u2082)\": -16.144869662285547}, {\"type\": \"grad_x\", \"op\": \"5.mlp_layer.f.act\", \"scale (log\\u2082)\": -16.487137675507586}, {\"type\": \"grad_x\", \"op\": \"5.mlp_layer.f.linear_2\", \"scale (log\\u2082)\": -15.812644755006314}, {\"type\": \"grad_x\", \"op\": \"5.mlp_layer.f.dropout\", \"scale (log\\u2082)\": -15.149231619719371}, {\"type\": \"grad_x\", \"op\": \"lm_head\", \"scale (log\\u2082)\": -14.2479630390473}], \"data-f218933b5d94522484598ff4fcd17e15\": [{\"type\": \"w\", \"op\": \"input_embeddings\", \"scale (log\\u2082)\": -0.0029901773017736405}, {\"type\": \"w\", \"op\": \"ln\", \"scale (log\\u2082)\": null}, {\"type\": \"w\", \"op\": \"0.attention_layer.f.ln\", \"scale (log\\u2082)\": null}, {\"type\": \"w\", \"op\": \"0.attention_layer.f.w_qkv\", \"scale (log\\u2082)\": -4.793146303681486}, {\"type\": \"w\", \"op\": \"0.attention_layer.f.w_o\", \"scale (log\\u2082)\": -4.287830931859984}, {\"type\": \"w\", \"op\": \"0.mlp_layer.f.ln\", \"scale (log\\u2082)\": null}, {\"type\": \"w\", \"op\": \"0.mlp_layer.f.linear_1\", \"scale (log\\u2082)\": -4.956619036930442}, {\"type\": \"w\", \"op\": \"0.mlp_layer.f.linear_2\", \"scale (log\\u2082)\": -4.952337537454559}, {\"type\": \"w\", \"op\": \"1.attention_layer.f.ln\", \"scale (log\\u2082)\": null}, {\"type\": \"w\", \"op\": \"1.attention_layer.f.w_qkv\", \"scale (log\\u2082)\": -4.792635438785294}, {\"type\": \"w\", \"op\": \"1.attention_layer.f.w_o\", \"scale (log\\u2082)\": -4.294319842210186}, {\"type\": \"w\", \"op\": \"1.mlp_layer.f.ln\", \"scale (log\\u2082)\": null}, {\"type\": \"w\", \"op\": \"1.mlp_layer.f.linear_1\", \"scale (log\\u2082)\": -4.9528201598218615}, {\"type\": \"w\", \"op\": \"1.mlp_layer.f.linear_2\", \"scale (log\\u2082)\": -4.953975281242726}, {\"type\": \"w\", \"op\": \"2.attention_layer.f.ln\", \"scale (log\\u2082)\": null}, {\"type\": \"w\", \"op\": \"2.attention_layer.f.w_qkv\", \"scale (log\\u2082)\": -4.794934936177019}, {\"type\": \"w\", \"op\": \"2.attention_layer.f.w_o\", \"scale (log\\u2082)\": -4.290839391123795}, {\"type\": \"w\", \"op\": \"2.mlp_layer.f.ln\", \"scale (log\\u2082)\": null}, {\"type\": \"w\", \"op\": \"2.mlp_layer.f.linear_1\", \"scale (log\\u2082)\": -4.953919810280163}, {\"type\": \"w\", \"op\": \"2.mlp_layer.f.linear_2\", \"scale (log\\u2082)\": -4.95385634611921}, {\"type\": \"w\", \"op\": \"3.attention_layer.f.ln\", \"scale (log\\u2082)\": null}, {\"type\": \"w\", \"op\": \"3.attention_layer.f.w_qkv\", \"scale (log\\u2082)\": -4.793684926425247}, {\"type\": \"w\", \"op\": \"3.attention_layer.f.w_o\", \"scale (log\\u2082)\": -4.291376419850721}, {\"type\": \"w\", \"op\": \"3.mlp_layer.f.ln\", \"scale (log\\u2082)\": null}, {\"type\": \"w\", \"op\": \"3.mlp_layer.f.linear_1\", \"scale (log\\u2082)\": -4.954128378839141}, {\"type\": \"w\", \"op\": \"3.mlp_layer.f.linear_2\", \"scale (log\\u2082)\": -4.953546894224289}, {\"type\": \"w\", \"op\": \"4.attention_layer.f.ln\", \"scale (log\\u2082)\": null}, {\"type\": \"w\", \"op\": \"4.attention_layer.f.w_qkv\", \"scale (log\\u2082)\": -4.792766527132705}, {\"type\": \"w\", \"op\": \"4.attention_layer.f.w_o\", \"scale (log\\u2082)\": -4.291753006311795}, {\"type\": \"w\", \"op\": \"4.mlp_layer.f.ln\", \"scale (log\\u2082)\": null}, {\"type\": \"w\", \"op\": \"4.mlp_layer.f.linear_1\", \"scale (log\\u2082)\": -4.953563547641678}, {\"type\": \"w\", \"op\": \"4.mlp_layer.f.linear_2\", \"scale (log\\u2082)\": -4.951893833767392}, {\"type\": \"w\", \"op\": \"5.attention_layer.f.ln\", \"scale (log\\u2082)\": null}, {\"type\": \"w\", \"op\": \"5.attention_layer.f.w_qkv\", \"scale (log\\u2082)\": -4.7925231293800845}, {\"type\": \"w\", \"op\": \"5.attention_layer.f.w_o\", \"scale (log\\u2082)\": -4.290793630873316}, {\"type\": \"w\", \"op\": \"5.mlp_layer.f.ln\", \"scale (log\\u2082)\": null}, {\"type\": \"w\", \"op\": \"5.mlp_layer.f.linear_1\", \"scale (log\\u2082)\": -4.953701445323673}, {\"type\": \"w\", \"op\": \"5.mlp_layer.f.linear_2\", \"scale (log\\u2082)\": -4.953320593337882}, {\"type\": \"w\", \"op\": \"lm_head\", \"scale (log\\u2082)\": -4.297858393245872}, {\"type\": \"grad_w\", \"op\": \"ln\", \"scale (log\\u2082)\": -9.15547083545152}, {\"type\": \"grad_w\", \"op\": \"0.attention_layer.f.ln\", \"scale (log\\u2082)\": -10.26649512940284}, {\"type\": \"grad_w\", \"op\": \"0.attention_layer.f.w_qkv\", \"scale (log\\u2082)\": -10.592628752009396}, {\"type\": \"grad_w\", \"op\": \"0.attention_layer.f.w_o\", \"scale (log\\u2082)\": -10.405504649878555}, {\"type\": \"grad_w\", \"op\": \"0.mlp_layer.f.ln\", \"scale (log\\u2082)\": -10.452250905249953}, {\"type\": \"grad_w\", \"op\": \"0.mlp_layer.f.linear_1\", \"scale (log\\u2082)\": -10.784173611775014}, {\"type\": \"grad_w\", \"op\": \"0.mlp_layer.f.linear_2\", \"scale (log\\u2082)\": -10.74319398513618}, {\"type\": \"grad_w\", \"op\": \"1.attention_layer.f.ln\", \"scale (log\\u2082)\": -10.462135611538704}, {\"type\": \"grad_w\", \"op\": \"1.attention_layer.f.w_qkv\", \"scale (log\\u2082)\": -10.800497485742518}, {\"type\": \"grad_w\", \"op\": \"1.attention_layer.f.w_o\", \"scale (log\\u2082)\": -10.607429309956792}, {\"type\": \"grad_w\", \"op\": \"1.mlp_layer.f.ln\", \"scale (log\\u2082)\": -10.640308742189054}, {\"type\": \"grad_w\", \"op\": \"1.mlp_layer.f.linear_1\", \"scale (log\\u2082)\": -10.987852213874534}, {\"type\": \"grad_w\", \"op\": \"1.mlp_layer.f.linear_2\", \"scale (log\\u2082)\": -10.98368323878445}, {\"type\": \"grad_w\", \"op\": \"2.attention_layer.f.ln\", \"scale (log\\u2082)\": -10.69029565068821}, {\"type\": \"grad_w\", \"op\": \"2.attention_layer.f.w_qkv\", \"scale (log\\u2082)\": -10.979083455393015}, {\"type\": \"grad_w\", \"op\": \"2.attention_layer.f.w_o\", \"scale (log\\u2082)\": -10.76121101832343}, {\"type\": \"grad_w\", \"op\": \"2.mlp_layer.f.ln\", \"scale (log\\u2082)\": -10.75209623846119}, {\"type\": \"grad_w\", \"op\": \"2.mlp_layer.f.linear_1\", \"scale (log\\u2082)\": -11.13446286095072}, {\"type\": \"grad_w\", \"op\": \"2.mlp_layer.f.linear_2\", \"scale (log\\u2082)\": -11.152916266297263}, {\"type\": \"grad_w\", \"op\": \"3.attention_layer.f.ln\", \"scale (log\\u2082)\": -10.89261305110137}, {\"type\": \"grad_w\", \"op\": \"3.attention_layer.f.w_qkv\", \"scale (log\\u2082)\": -11.134125168381006}, {\"type\": \"grad_w\", \"op\": \"3.attention_layer.f.w_o\", \"scale (log\\u2082)\": -10.91368654152057}, {\"type\": \"grad_w\", \"op\": \"3.mlp_layer.f.ln\", \"scale (log\\u2082)\": -10.944421815689738}, {\"type\": \"grad_w\", \"op\": \"3.mlp_layer.f.linear_1\", \"scale (log\\u2082)\": -11.266281546945505}, {\"type\": \"grad_w\", \"op\": \"3.mlp_layer.f.linear_2\", \"scale (log\\u2082)\": -11.275867665412257}, {\"type\": \"grad_w\", \"op\": \"4.attention_layer.f.ln\", \"scale (log\\u2082)\": -10.89438956721706}, {\"type\": \"grad_w\", \"op\": \"4.attention_layer.f.w_qkv\", \"scale (log\\u2082)\": -11.237948015628959}, {\"type\": \"grad_w\", \"op\": \"4.attention_layer.f.w_o\", \"scale (log\\u2082)\": -11.010883769032844}, {\"type\": \"grad_w\", \"op\": \"4.mlp_layer.f.ln\", \"scale (log\\u2082)\": -11.055442881394471}, {\"type\": \"grad_w\", \"op\": \"4.mlp_layer.f.linear_1\", \"scale (log\\u2082)\": -11.38030893774402}, {\"type\": \"grad_w\", \"op\": \"4.mlp_layer.f.linear_2\", \"scale (log\\u2082)\": -11.402368180578009}, {\"type\": \"grad_w\", \"op\": \"5.attention_layer.f.ln\", \"scale (log\\u2082)\": -11.086627994214632}, {\"type\": \"grad_w\", \"op\": \"5.attention_layer.f.w_qkv\", \"scale (log\\u2082)\": -11.362514665181955}, {\"type\": \"grad_w\", \"op\": \"5.attention_layer.f.w_o\", \"scale (log\\u2082)\": -11.139688660878985}, {\"type\": \"grad_w\", \"op\": \"5.mlp_layer.f.ln\", \"scale (log\\u2082)\": -11.160644622793223}, {\"type\": \"grad_w\", \"op\": \"5.mlp_layer.f.linear_1\", \"scale (log\\u2082)\": -11.463697046795843}, {\"type\": \"grad_w\", \"op\": \"5.mlp_layer.f.linear_2\", \"scale (log\\u2082)\": -11.48592733189783}, {\"type\": \"grad_w\", \"op\": \"lm_head\", \"scale (log\\u2082)\": -9.2256998386851}], \"empty\": [{}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.LayerChart(...)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = NanoGPTModel(config)\n",
    "analyse_full_model(model, config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results clearly demonstrates the inadiquacy of the standard approach to model design when it comes to scale. Grad_x and grad_w values are very far from having unit scale, with the former now relying on subnormal values (dangerous, as these begin to lose precision and disappear altogether at $2^{-24}$).\n",
    "\n",
    "A key issue introduced by this layer is the cross-entropy loss definition. This typically uses a `1 / batch_size` term to average over labels, which has the effect of dramatically under-scaling gradients in the backward pass.\n",
    "\n",
    "We can also see here that a single loss scaling factor for all gradients isn't ideal. This would shift all grad_xs and grad_ws to the right by the same amount, which would still leave us far short of our unit-scale ideal.\n",
    "\n",
    "What we really need is per-op scaling! Again, we'll fix this for the unit-scaled implementation:\n",
    "\n",
    "#### Unit scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnitScaledNanoGPTModel(NanoGPTModel):\n",
    "    config_class = NanoGPTConfig\n",
    "\n",
    "    def __init__(self, config: NanoGPTConfig) -> None:\n",
    "        super().__init__(config)\n",
    "        self.input_embeddings = UnitScaledEmbedding(config.vocab_size, config.hidden_size)\n",
    "        self.dropout = UnitScaledDropout(config.dropout)\n",
    "        self.ln = UnitScaledLayerNorm(config.hidden_size)\n",
    "        self.blocks = nn.ModuleList([\n",
    "            UnitScaledTransformerBlock(config)\n",
    "            for _ in range(config.num_hidden_layers)\n",
    "        ])\n",
    "        self.lm_head = UnitScaledLinear(\n",
    "            config.hidden_size, config.vocab_size, bias=False, scale_for=\"separate\"\n",
    "        )\n",
    "        self.loss_fn = UnitScaledCrossEntropyLoss()\n",
    "        self.apply(init_weights(unit_init))\n",
    "\n",
    "class UnitScaledEmbedding(nn.Embedding):\n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        batch_size = np.prod(input.shape)\n",
    "        weight = scaled(self.weight, beta=self.num_embeddings / batch_size)\n",
    "        return F.embedding(input, weight, self.padding_idx, self.max_norm,\n",
    "                           self.norm_type, self.scale_grad_by_freq, self.sparse)\n",
    "\n",
    "class UnitScaledCrossEntropyLoss(nn.CrossEntropyLoss):    \n",
    "    def forward(self, input: Tensor, target: Tensor) -> Tensor:\n",
    "        batch_size, seq_len = input.shape\n",
    "        input = scaled(input, beta=seq_len / (seq_len - 1) ** 0.5)\n",
    "        loss = F.cross_entropy(input, target, reduction='sum')\n",
    "        return scaled(loss, alpha=1 / batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-3c9276ca450b4e6297711377533bcd6c\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-3c9276ca450b4e6297711377533bcd6c\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-3c9276ca450b4e6297711377533bcd6c\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.17.0?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"4.17.0\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}, \"axis\": {\"labelFontSize\": 12, \"titleFontSize\": 16}}, \"layer\": [{\"data\": {\"name\": \"data-da2f5635f07ab4dceb082e877ff24c22\"}, \"mark\": \"line\", \"encoding\": {\"color\": {\"field\": \"type\", \"legend\": {\"labelFontSize\": 12, \"symbolSize\": 100, \"title\": \"\"}, \"scale\": {\"range\": [\"#6C8EBF\", \"#FF8000\"]}, \"sort\": \"descending\", \"type\": \"nominal\"}, \"x\": {\"axis\": {\"orient\": \"top\", \"values\": [-18.0, -16.0, -14.0, -12.0, -10.0, -8.0, -6.0, -4.0, -2.0, 0.0, 2.0, 4.0, 6.0, 8.0, 10.0, 12.0, 14.0, 16.0, 18.0]}, \"field\": \"scale (log\\u2082)\", \"scale\": {\"domain\": [-18.0, 18.0]}, \"type\": \"quantitative\"}, \"y\": {\"field\": \"op\", \"sort\": [\"input_embeddings\", \"dropout\", \"ln\", \"0.attention_layer.f.ln\", \"0.attention_layer.f.w_qkv\", \"0.attention_layer.f.qk_matmul\", \"0.attention_layer.f.attn_dropout\", \"0.attention_layer.f.qkv_matmul\", \"0.attention_layer.f.w_o\", \"0.attention_layer.f.residual_dropout\", \"0.mlp_layer.f.ln\", \"0.mlp_layer.f.linear_1\", \"0.mlp_layer.f.act\", \"0.mlp_layer.f.linear_2\", \"0.mlp_layer.f.dropout\", \"1.attention_layer.f.ln\", \"1.attention_layer.f.w_qkv\", \"1.attention_layer.f.qk_matmul\", \"1.attention_layer.f.attn_dropout\", \"1.attention_layer.f.qkv_matmul\", \"1.attention_layer.f.w_o\", \"1.attention_layer.f.residual_dropout\", \"1.mlp_layer.f.ln\", \"1.mlp_layer.f.linear_1\", \"1.mlp_layer.f.act\", \"1.mlp_layer.f.linear_2\", \"1.mlp_layer.f.dropout\", \"2.attention_layer.f.ln\", \"2.attention_layer.f.w_qkv\", \"2.attention_layer.f.qk_matmul\", \"2.attention_layer.f.attn_dropout\", \"2.attention_layer.f.qkv_matmul\", \"2.attention_layer.f.w_o\", \"2.attention_layer.f.residual_dropout\", \"2.mlp_layer.f.ln\", \"2.mlp_layer.f.linear_1\", \"2.mlp_layer.f.act\", \"2.mlp_layer.f.linear_2\", \"2.mlp_layer.f.dropout\", \"3.attention_layer.f.ln\", \"3.attention_layer.f.w_qkv\", \"3.attention_layer.f.qk_matmul\", \"3.attention_layer.f.attn_dropout\", \"3.attention_layer.f.qkv_matmul\", \"3.attention_layer.f.w_o\", \"3.attention_layer.f.residual_dropout\", \"3.mlp_layer.f.ln\", \"3.mlp_layer.f.linear_1\", \"3.mlp_layer.f.act\", \"3.mlp_layer.f.linear_2\", \"3.mlp_layer.f.dropout\", \"4.attention_layer.f.ln\", \"4.attention_layer.f.w_qkv\", \"4.attention_layer.f.qk_matmul\", \"4.attention_layer.f.attn_dropout\", \"4.attention_layer.f.qkv_matmul\", \"4.attention_layer.f.w_o\", \"4.attention_layer.f.residual_dropout\", \"4.mlp_layer.f.ln\", \"4.mlp_layer.f.linear_1\", \"4.mlp_layer.f.act\", \"4.mlp_layer.f.linear_2\", \"4.mlp_layer.f.dropout\", \"5.attention_layer.f.ln\", \"5.attention_layer.f.w_qkv\", \"5.attention_layer.f.qk_matmul\", \"5.attention_layer.f.attn_dropout\", \"5.attention_layer.f.qkv_matmul\", \"5.attention_layer.f.w_o\", \"5.attention_layer.f.residual_dropout\", \"5.mlp_layer.f.ln\", \"5.mlp_layer.f.linear_1\", \"5.mlp_layer.f.act\", \"5.mlp_layer.f.linear_2\", \"5.mlp_layer.f.dropout\", \"lm_head\"], \"title\": \"\", \"type\": \"ordinal\"}}}, {\"data\": {\"name\": \"data-4626a376f4cdbc775e5c8bb237eb002a\"}, \"mark\": {\"type\": \"point\", \"size\": 100}, \"encoding\": {\"color\": {\"field\": \"type\", \"legend\": {\"labelFontSize\": 12, \"symbolSize\": 100, \"title\": \"\"}, \"scale\": {\"range\": [\"#5D8944\", \"#ED3434\"]}, \"sort\": \"descending\", \"type\": \"nominal\"}, \"shape\": {\"field\": \"type\", \"scale\": {\"range\": [\"square\", \"triangle-down\"]}, \"sort\": \"descending\", \"type\": \"nominal\"}, \"x\": {\"axis\": {\"orient\": \"top\", \"values\": [-18.0, -16.0, -14.0, -12.0, -10.0, -8.0, -6.0, -4.0, -2.0, 0.0, 2.0, 4.0, 6.0, 8.0, 10.0, 12.0, 14.0, 16.0, 18.0]}, \"field\": \"scale (log\\u2082)\", \"scale\": {\"domain\": [-18.0, 18.0]}, \"type\": \"quantitative\"}, \"y\": {\"field\": \"op\", \"sort\": [\"input_embeddings\", \"dropout\", \"ln\", \"0.attention_layer.f.ln\", \"0.attention_layer.f.w_qkv\", \"0.attention_layer.f.qk_matmul\", \"0.attention_layer.f.attn_dropout\", \"0.attention_layer.f.qkv_matmul\", \"0.attention_layer.f.w_o\", \"0.attention_layer.f.residual_dropout\", \"0.mlp_layer.f.ln\", \"0.mlp_layer.f.linear_1\", \"0.mlp_layer.f.act\", \"0.mlp_layer.f.linear_2\", \"0.mlp_layer.f.dropout\", \"1.attention_layer.f.ln\", \"1.attention_layer.f.w_qkv\", \"1.attention_layer.f.qk_matmul\", \"1.attention_layer.f.attn_dropout\", \"1.attention_layer.f.qkv_matmul\", \"1.attention_layer.f.w_o\", \"1.attention_layer.f.residual_dropout\", \"1.mlp_layer.f.ln\", \"1.mlp_layer.f.linear_1\", \"1.mlp_layer.f.act\", \"1.mlp_layer.f.linear_2\", \"1.mlp_layer.f.dropout\", \"2.attention_layer.f.ln\", \"2.attention_layer.f.w_qkv\", \"2.attention_layer.f.qk_matmul\", \"2.attention_layer.f.attn_dropout\", \"2.attention_layer.f.qkv_matmul\", \"2.attention_layer.f.w_o\", \"2.attention_layer.f.residual_dropout\", \"2.mlp_layer.f.ln\", \"2.mlp_layer.f.linear_1\", \"2.mlp_layer.f.act\", \"2.mlp_layer.f.linear_2\", \"2.mlp_layer.f.dropout\", \"3.attention_layer.f.ln\", \"3.attention_layer.f.w_qkv\", \"3.attention_layer.f.qk_matmul\", \"3.attention_layer.f.attn_dropout\", \"3.attention_layer.f.qkv_matmul\", \"3.attention_layer.f.w_o\", \"3.attention_layer.f.residual_dropout\", \"3.mlp_layer.f.ln\", \"3.mlp_layer.f.linear_1\", \"3.mlp_layer.f.act\", \"3.mlp_layer.f.linear_2\", \"3.mlp_layer.f.dropout\", \"4.attention_layer.f.ln\", \"4.attention_layer.f.w_qkv\", \"4.attention_layer.f.qk_matmul\", \"4.attention_layer.f.attn_dropout\", \"4.attention_layer.f.qkv_matmul\", \"4.attention_layer.f.w_o\", \"4.attention_layer.f.residual_dropout\", \"4.mlp_layer.f.ln\", \"4.mlp_layer.f.linear_1\", \"4.mlp_layer.f.act\", \"4.mlp_layer.f.linear_2\", \"4.mlp_layer.f.dropout\", \"5.attention_layer.f.ln\", \"5.attention_layer.f.w_qkv\", \"5.attention_layer.f.qk_matmul\", \"5.attention_layer.f.attn_dropout\", \"5.attention_layer.f.qkv_matmul\", \"5.attention_layer.f.w_o\", \"5.attention_layer.f.residual_dropout\", \"5.mlp_layer.f.ln\", \"5.mlp_layer.f.linear_1\", \"5.mlp_layer.f.act\", \"5.mlp_layer.f.linear_2\", \"5.mlp_layer.f.dropout\", \"lm_head\"], \"title\": \"\", \"type\": \"ordinal\"}}}, {\"data\": {\"name\": \"empty\"}, \"mark\": {\"type\": \"rule\", \"strokeDash\": [4, 4]}, \"encoding\": {\"x\": {\"datum\": -14}}}, {\"data\": {\"name\": \"empty\"}, \"mark\": {\"type\": \"rule\", \"strokeDash\": [4, 4]}, \"encoding\": {\"x\": {\"datum\": 16}}}, {\"data\": {\"name\": \"empty\"}, \"mark\": {\"type\": \"text\", \"dy\": -740}, \"encoding\": {\"text\": {\"value\": \"Min FP16 (normal)\"}, \"x\": {\"datum\": -10}}}, {\"data\": {\"name\": \"empty\"}, \"mark\": {\"type\": \"text\", \"dy\": -740}, \"encoding\": {\"text\": {\"value\": \"Max FP16\"}, \"x\": {\"datum\": 13}}}], \"resolve\": {\"scale\": {\"color\": \"independent\", \"shape\": \"independent\"}}, \"width\": 500, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.17.0.json\", \"datasets\": {\"data-da2f5635f07ab4dceb082e877ff24c22\": [{\"type\": \"x\", \"op\": \"input_embeddings\", \"scale (log\\u2082)\": -0.0026752612796985265}, {\"type\": \"x\", \"op\": \"dropout\", \"scale (log\\u2082)\": -0.002388407198037546}, {\"type\": \"x\", \"op\": \"ln\", \"scale (log\\u2082)\": -6.53535553522462e-06}, {\"type\": \"x\", \"op\": \"0.attention_layer.f.ln\", \"scale (log\\u2082)\": -5.5034553246245395e-06}, {\"type\": \"x\", \"op\": \"0.attention_layer.f.w_qkv\", \"scale (log\\u2082)\": 0.002416395274701302}, {\"type\": \"x\", \"op\": \"0.attention_layer.f.qk_matmul\", \"scale (log\\u2082)\": 0.0014398079812092152}, {\"type\": \"x\", \"op\": \"0.attention_layer.f.attn_dropout\", \"scale (log\\u2082)\": -0.30056151531791864}, {\"type\": \"x\", \"op\": \"0.attention_layer.f.qkv_matmul\", \"scale (log\\u2082)\": -0.2373927970359962}, {\"type\": \"x\", \"op\": \"0.attention_layer.f.w_o\", \"scale (log\\u2082)\": -0.2298482987317782}, {\"type\": \"x\", \"op\": \"0.attention_layer.f.residual_dropout\", \"scale (log\\u2082)\": -0.22885483283395122}, {\"type\": \"x\", \"op\": \"0.mlp_layer.f.ln\", \"scale (log\\u2082)\": -5.933413656011219e-06}, {\"type\": \"x\", \"op\": \"0.mlp_layer.f.linear_1\", \"scale (log\\u2082)\": -0.49909941822586773}, {\"type\": \"x\", \"op\": \"0.mlp_layer.f.act\", \"scale (log\\u2082)\": -0.6387537432084995}, {\"type\": \"x\", \"op\": \"0.mlp_layer.f.linear_2\", \"scale (log\\u2082)\": -0.02781511936553658}, {\"type\": \"x\", \"op\": \"0.mlp_layer.f.dropout\", \"scale (log\\u2082)\": -0.027587558139697994}, {\"type\": \"x\", \"op\": \"1.attention_layer.f.ln\", \"scale (log\\u2082)\": -5.847421979482836e-06}, {\"type\": \"x\", \"op\": \"1.attention_layer.f.w_qkv\", \"scale (log\\u2082)\": -0.0003712723205801338}, {\"type\": \"x\", \"op\": \"1.attention_layer.f.qk_matmul\", \"scale (log\\u2082)\": -0.005457774963110059}, {\"type\": \"x\", \"op\": \"1.attention_layer.f.attn_dropout\", \"scale (log\\u2082)\": -0.3064747581750652}, {\"type\": \"x\", \"op\": \"1.attention_layer.f.qkv_matmul\", \"scale (log\\u2082)\": -0.23357583125530287}, {\"type\": \"x\", \"op\": \"1.attention_layer.f.w_o\", \"scale (log\\u2082)\": -0.23461828584585512}, {\"type\": \"x\", \"op\": \"1.attention_layer.f.residual_dropout\", \"scale (log\\u2082)\": -0.2354939349082911}, {\"type\": \"x\", \"op\": \"1.mlp_layer.f.ln\", \"scale (log\\u2082)\": -6.277380413380001e-06}, {\"type\": \"x\", \"op\": \"1.mlp_layer.f.linear_1\", \"scale (log\\u2082)\": -0.5048155430293578}, {\"type\": \"x\", \"op\": \"1.mlp_layer.f.act\", \"scale (log\\u2082)\": -0.6488469668982891}, {\"type\": \"x\", \"op\": \"1.mlp_layer.f.linear_2\", \"scale (log\\u2082)\": -0.0476907407164028}, {\"type\": \"x\", \"op\": \"1.mlp_layer.f.dropout\", \"scale (log\\u2082)\": -0.04714164692619024}, {\"type\": \"x\", \"op\": \"2.attention_layer.f.ln\", \"scale (log\\u2082)\": -6.191388716349517e-06}, {\"type\": \"x\", \"op\": \"2.attention_layer.f.w_qkv\", \"scale (log\\u2082)\": 0.0015014868469069031}, {\"type\": \"x\", \"op\": \"2.attention_layer.f.qk_matmul\", \"scale (log\\u2082)\": 0.003032991339907099}, {\"type\": \"x\", \"op\": \"2.attention_layer.f.attn_dropout\", \"scale (log\\u2082)\": -0.3007122316303179}, {\"type\": \"x\", \"op\": \"2.attention_layer.f.qkv_matmul\", \"scale (log\\u2082)\": -0.21832382307800108}, {\"type\": \"x\", \"op\": \"2.attention_layer.f.w_o\", \"scale (log\\u2082)\": -0.2223749508712784}, {\"type\": \"x\", \"op\": \"2.attention_layer.f.residual_dropout\", \"scale (log\\u2082)\": -0.22312093743898817}, {\"type\": \"x\", \"op\": \"2.mlp_layer.f.ln\", \"scale (log\\u2082)\": -6.53535553522462e-06}, {\"type\": \"x\", \"op\": \"2.mlp_layer.f.linear_1\", \"scale (log\\u2082)\": -0.5044597862084382}, {\"type\": \"x\", \"op\": \"2.mlp_layer.f.act\", \"scale (log\\u2082)\": -0.6560453798956452}, {\"type\": \"x\", \"op\": \"2.mlp_layer.f.linear_2\", \"scale (log\\u2082)\": -0.047622570277563864}, {\"type\": \"x\", \"op\": \"2.mlp_layer.f.dropout\", \"scale (log\\u2082)\": -0.047280344694047245}, {\"type\": \"x\", \"op\": \"3.attention_layer.f.ln\", \"scale (log\\u2082)\": -6.449363822817553e-06}, {\"type\": \"x\", \"op\": \"3.attention_layer.f.w_qkv\", \"scale (log\\u2082)\": -0.0008492422601230015}, {\"type\": \"x\", \"op\": \"3.attention_layer.f.qk_matmul\", \"scale (log\\u2082)\": -0.008280346215350622}, {\"type\": \"x\", \"op\": \"3.attention_layer.f.attn_dropout\", \"scale (log\\u2082)\": -0.31365539436849366}, {\"type\": \"x\", \"op\": \"3.attention_layer.f.qkv_matmul\", \"scale (log\\u2082)\": -0.2159810727045912}, {\"type\": \"x\", \"op\": \"3.attention_layer.f.w_o\", \"scale (log\\u2082)\": -0.2203068913305145}, {\"type\": \"x\", \"op\": \"3.attention_layer.f.residual_dropout\", \"scale (log\\u2082)\": -0.21986066605269583}, {\"type\": \"x\", \"op\": \"3.mlp_layer.f.ln\", \"scale (log\\u2082)\": -6.7073389754153415e-06}, {\"type\": \"x\", \"op\": \"3.mlp_layer.f.linear_1\", \"scale (log\\u2082)\": -0.5008551971050431}, {\"type\": \"x\", \"op\": \"3.mlp_layer.f.act\", \"scale (log\\u2082)\": -0.6283218478419356}, {\"type\": \"x\", \"op\": \"3.mlp_layer.f.linear_2\", \"scale (log\\u2082)\": -0.001308779501528422}, {\"type\": \"x\", \"op\": \"3.mlp_layer.f.dropout\", \"scale (log\\u2082)\": -0.0021316651760685825}, {\"type\": \"x\", \"op\": \"4.attention_layer.f.ln\", \"scale (log\\u2082)\": -6.363372115536013e-06}, {\"type\": \"x\", \"op\": \"4.attention_layer.f.w_qkv\", \"scale (log\\u2082)\": 5.778501342788239e-05}, {\"type\": \"x\", \"op\": \"4.attention_layer.f.qk_matmul\", \"scale (log\\u2082)\": -0.0030812615908310096}, {\"type\": \"x\", \"op\": \"4.attention_layer.f.attn_dropout\", \"scale (log\\u2082)\": -0.31441440334292287}, {\"type\": \"x\", \"op\": \"4.attention_layer.f.qkv_matmul\", \"scale (log\\u2082)\": -0.1990659445753048}, {\"type\": \"x\", \"op\": \"4.attention_layer.f.w_o\", \"scale (log\\u2082)\": -0.20352780726559422}, {\"type\": \"x\", \"op\": \"4.attention_layer.f.residual_dropout\", \"scale (log\\u2082)\": -0.20408995389822107}, {\"type\": \"x\", \"op\": \"4.mlp_layer.f.ln\", \"scale (log\\u2082)\": -6.6213472527572165e-06}, {\"type\": \"x\", \"op\": \"4.mlp_layer.f.linear_1\", \"scale (log\\u2082)\": -0.4954527785995013}, {\"type\": \"x\", \"op\": \"4.mlp_layer.f.act\", \"scale (log\\u2082)\": -0.642428800567893}, {\"type\": \"x\", \"op\": \"4.mlp_layer.f.linear_2\", \"scale (log\\u2082)\": -0.002342584742283486}, {\"type\": \"x\", \"op\": \"4.mlp_layer.f.dropout\", \"scale (log\\u2082)\": -0.0025345836677572713}, {\"type\": \"x\", \"op\": \"5.attention_layer.f.ln\", \"scale (log\\u2082)\": -6.449363822817553e-06}, {\"type\": \"x\", \"op\": \"5.attention_layer.f.w_qkv\", \"scale (log\\u2082)\": 0.0009416416089438373}, {\"type\": \"x\", \"op\": \"5.attention_layer.f.qk_matmul\", \"scale (log\\u2082)\": -0.008543721012320303}, {\"type\": \"x\", \"op\": \"5.attention_layer.f.attn_dropout\", \"scale (log\\u2082)\": -0.32182144808786173}, {\"type\": \"x\", \"op\": \"5.attention_layer.f.qkv_matmul\", \"scale (log\\u2082)\": -0.19045420919639486}, {\"type\": \"x\", \"op\": \"5.attention_layer.f.w_o\", \"scale (log\\u2082)\": -0.1838235466264482}, {\"type\": \"x\", \"op\": \"5.attention_layer.f.residual_dropout\", \"scale (log\\u2082)\": -0.1850219514592163}, {\"type\": \"x\", \"op\": \"5.mlp_layer.f.ln\", \"scale (log\\u2082)\": -6.6213472527572165e-06}, {\"type\": \"x\", \"op\": \"5.mlp_layer.f.linear_1\", \"scale (log\\u2082)\": -0.500358210898188}, {\"type\": \"x\", \"op\": \"5.mlp_layer.f.act\", \"scale (log\\u2082)\": -0.6372006859545888}, {\"type\": \"x\", \"op\": \"5.mlp_layer.f.linear_2\", \"scale (log\\u2082)\": -0.02091258508360714}, {\"type\": \"x\", \"op\": \"5.mlp_layer.f.dropout\", \"scale (log\\u2082)\": -0.020429579912504173}, {\"type\": \"x\", \"op\": \"lm_head\", \"scale (log\\u2082)\": 0.0006949864507229543}, {\"type\": \"grad_x\", \"op\": \"dropout\", \"scale (log\\u2082)\": 0.0195186197642741}, {\"type\": \"grad_x\", \"op\": \"ln\", \"scale (log\\u2082)\": 0.05404251049981862}, {\"type\": \"grad_x\", \"op\": \"0.attention_layer.f.ln\", \"scale (log\\u2082)\": -0.12175455565960182}, {\"type\": \"grad_x\", \"op\": \"0.attention_layer.f.w_qkv\", \"scale (log\\u2082)\": -0.12674149107214486}, {\"type\": \"grad_x\", \"op\": \"0.attention_layer.f.attn_dropout\", \"scale (log\\u2082)\": 1.0515725685288955}, {\"type\": \"grad_x\", \"op\": \"0.attention_layer.f.w_o\", \"scale (log\\u2082)\": 0.0535703037896171}, {\"type\": \"grad_x\", \"op\": \"0.attention_layer.f.residual_dropout\", \"scale (log\\u2082)\": 0.051535728643711275}, {\"type\": \"grad_x\", \"op\": \"0.mlp_layer.f.ln\", \"scale (log\\u2082)\": 0.09713148655377864}, {\"type\": \"grad_x\", \"op\": \"0.mlp_layer.f.linear_1\", \"scale (log\\u2082)\": 0.05461557837365756}, {\"type\": \"grad_x\", \"op\": \"0.mlp_layer.f.act\", \"scale (log\\u2082)\": -0.4459148599956813}, {\"type\": \"grad_x\", \"op\": \"0.mlp_layer.f.linear_2\", \"scale (log\\u2082)\": -0.4626197198624782}, {\"type\": \"grad_x\", \"op\": \"0.mlp_layer.f.dropout\", \"scale (log\\u2082)\": 0.03796312401869181}, {\"type\": \"grad_x\", \"op\": \"1.attention_layer.f.ln\", \"scale (log\\u2082)\": -0.09658231064118157}, {\"type\": \"grad_x\", \"op\": \"1.attention_layer.f.w_qkv\", \"scale (log\\u2082)\": -0.12548066402382496}, {\"type\": \"grad_x\", \"op\": \"1.attention_layer.f.attn_dropout\", \"scale (log\\u2082)\": 1.0586897169698775}, {\"type\": \"grad_x\", \"op\": \"1.attention_layer.f.w_o\", \"scale (log\\u2082)\": 0.059023399416956675}, {\"type\": \"grad_x\", \"op\": \"1.attention_layer.f.residual_dropout\", \"scale (log\\u2082)\": 0.06421795618086122}, {\"type\": \"grad_x\", \"op\": \"1.mlp_layer.f.ln\", \"scale (log\\u2082)\": 0.1377902685048015}, {\"type\": \"grad_x\", \"op\": \"1.mlp_layer.f.linear_1\", \"scale (log\\u2082)\": 0.06583584547069997}, {\"type\": \"grad_x\", \"op\": \"1.mlp_layer.f.act\", \"scale (log\\u2082)\": -0.43494825633394124}, {\"type\": \"grad_x\", \"op\": \"1.mlp_layer.f.linear_2\", \"scale (log\\u2082)\": -0.453665057576719}, {\"type\": \"grad_x\", \"op\": \"1.mlp_layer.f.dropout\", \"scale (log\\u2082)\": 0.04838692506192431}, {\"type\": \"grad_x\", \"op\": \"2.attention_layer.f.ln\", \"scale (log\\u2082)\": -0.06697603488845891}, {\"type\": \"grad_x\", \"op\": \"2.attention_layer.f.w_qkv\", \"scale (log\\u2082)\": -0.1210702074987476}, {\"type\": \"grad_x\", \"op\": \"2.attention_layer.f.attn_dropout\", \"scale (log\\u2082)\": 1.0734368497759161}, {\"type\": \"grad_x\", \"op\": \"2.attention_layer.f.w_o\", \"scale (log\\u2082)\": 0.07689239863657463}, {\"type\": \"grad_x\", \"op\": \"2.attention_layer.f.residual_dropout\", \"scale (log\\u2082)\": 0.0768579933361794}, {\"type\": \"grad_x\", \"op\": \"2.mlp_layer.f.ln\", \"scale (log\\u2082)\": 0.16478803202779072}, {\"type\": \"grad_x\", \"op\": \"2.mlp_layer.f.linear_1\", \"scale (log\\u2082)\": 0.06554597212392345}, {\"type\": \"grad_x\", \"op\": \"2.mlp_layer.f.act\", \"scale (log\\u2082)\": -0.43513496340135605}, {\"type\": \"grad_x\", \"op\": \"2.mlp_layer.f.linear_2\", \"scale (log\\u2082)\": -0.44599603715399183}, {\"type\": \"grad_x\", \"op\": \"2.mlp_layer.f.dropout\", \"scale (log\\u2082)\": 0.05213135736724056}, {\"type\": \"grad_x\", \"op\": \"3.attention_layer.f.ln\", \"scale (log\\u2082)\": -0.06273475975984952}, {\"type\": \"grad_x\", \"op\": \"3.attention_layer.f.w_qkv\", \"scale (log\\u2082)\": -0.1404323236524465}, {\"type\": \"grad_x\", \"op\": \"3.attention_layer.f.attn_dropout\", \"scale (log\\u2082)\": 1.0743842065304516}, {\"type\": \"grad_x\", \"op\": \"3.attention_layer.f.w_o\", \"scale (log\\u2082)\": 0.07352527209482707}, {\"type\": \"grad_x\", \"op\": \"3.attention_layer.f.residual_dropout\", \"scale (log\\u2082)\": 0.08027088066107284}, {\"type\": \"grad_x\", \"op\": \"3.mlp_layer.f.ln\", \"scale (log\\u2082)\": 0.19597693856575696}, {\"type\": \"grad_x\", \"op\": \"3.mlp_layer.f.linear_1\", \"scale (log\\u2082)\": 0.08050999302888516}, {\"type\": \"grad_x\", \"op\": \"3.mlp_layer.f.act\", \"scale (log\\u2082)\": -0.4189125376994725}, {\"type\": \"grad_x\", \"op\": \"3.mlp_layer.f.linear_2\", \"scale (log\\u2082)\": -0.4498740780649521}, {\"type\": \"grad_x\", \"op\": \"3.mlp_layer.f.dropout\", \"scale (log\\u2082)\": 0.048628719738532916}, {\"type\": \"grad_x\", \"op\": \"4.attention_layer.f.ln\", \"scale (log\\u2082)\": -0.0780007539574688}, {\"type\": \"grad_x\", \"op\": \"4.attention_layer.f.w_qkv\", \"scale (log\\u2082)\": -0.15676515610125152}, {\"type\": \"grad_x\", \"op\": \"4.attention_layer.f.attn_dropout\", \"scale (log\\u2082)\": 1.0731519328730497}, {\"type\": \"grad_x\", \"op\": \"4.attention_layer.f.w_o\", \"scale (log\\u2082)\": 0.07417021515733516}, {\"type\": \"grad_x\", \"op\": \"4.attention_layer.f.residual_dropout\", \"scale (log\\u2082)\": 0.07658157975124916}, {\"type\": \"grad_x\", \"op\": \"4.mlp_layer.f.ln\", \"scale (log\\u2082)\": 0.17710560787417182}, {\"type\": \"grad_x\", \"op\": \"4.mlp_layer.f.linear_1\", \"scale (log\\u2082)\": 0.06289826614055445}, {\"type\": \"grad_x\", \"op\": \"4.mlp_layer.f.act\", \"scale (log\\u2082)\": -0.43451029871294194}, {\"type\": \"grad_x\", \"op\": \"4.mlp_layer.f.linear_2\", \"scale (log\\u2082)\": -0.4487830769019847}, {\"type\": \"grad_x\", \"op\": \"4.mlp_layer.f.dropout\", \"scale (log\\u2082)\": 0.04874328367126092}, {\"type\": \"grad_x\", \"op\": \"5.attention_layer.f.ln\", \"scale (log\\u2082)\": -0.10538638731610048}, {\"type\": \"grad_x\", \"op\": \"5.attention_layer.f.w_qkv\", \"scale (log\\u2082)\": -0.18509908449296517}, {\"type\": \"grad_x\", \"op\": \"5.attention_layer.f.attn_dropout\", \"scale (log\\u2082)\": 1.0623520329831595}, {\"type\": \"grad_x\", \"op\": \"5.attention_layer.f.w_o\", \"scale (log\\u2082)\": 0.07474057056599041}, {\"type\": \"grad_x\", \"op\": \"5.attention_layer.f.residual_dropout\", \"scale (log\\u2082)\": 0.08061619826077719}, {\"type\": \"grad_x\", \"op\": \"5.mlp_layer.f.ln\", \"scale (log\\u2082)\": 0.18672981376183762}, {\"type\": \"grad_x\", \"op\": \"5.mlp_layer.f.linear_1\", \"scale (log\\u2082)\": 0.07141031416927439}, {\"type\": \"grad_x\", \"op\": \"5.mlp_layer.f.act\", \"scale (log\\u2082)\": -0.42785573216253087}, {\"type\": \"grad_x\", \"op\": \"5.mlp_layer.f.linear_2\", \"scale (log\\u2082)\": -0.449665723754572}, {\"type\": \"grad_x\", \"op\": \"5.mlp_layer.f.dropout\", \"scale (log\\u2082)\": 0.05348230714019229}, {\"type\": \"grad_x\", \"op\": \"lm_head\", \"scale (log\\u2082)\": -0.041643054553981325}], \"data-4626a376f4cdbc775e5c8bb237eb002a\": [{\"type\": \"w\", \"op\": \"input_embeddings\", \"scale (log\\u2082)\": -0.0012849384829846198}, {\"type\": \"w\", \"op\": \"ln\", \"scale (log\\u2082)\": null}, {\"type\": \"w\", \"op\": \"0.attention_layer.f.ln\", \"scale (log\\u2082)\": null}, {\"type\": \"w\", \"op\": \"0.attention_layer.f.w_qkv\", \"scale (log\\u2082)\": 0.0008055138022986543}, {\"type\": \"w\", \"op\": \"0.attention_layer.f.w_o\", \"scale (log\\u2082)\": 0.002401629443311008}, {\"type\": \"w\", \"op\": \"0.mlp_layer.f.ln\", \"scale (log\\u2082)\": null}, {\"type\": \"w\", \"op\": \"0.mlp_layer.f.linear_1\", \"scale (log\\u2082)\": 0.00030196993010657007}, {\"type\": \"w\", \"op\": \"0.mlp_layer.f.linear_2\", \"scale (log\\u2082)\": -0.0008861547327968484}, {\"type\": \"w\", \"op\": \"1.attention_layer.f.ln\", \"scale (log\\u2082)\": null}, {\"type\": \"w\", \"op\": \"1.attention_layer.f.w_qkv\", \"scale (log\\u2082)\": -0.0032049282731348113}, {\"type\": \"w\", \"op\": \"1.attention_layer.f.w_o\", \"scale (log\\u2082)\": -0.0020570885029925863}, {\"type\": \"w\", \"op\": \"1.mlp_layer.f.ln\", \"scale (log\\u2082)\": null}, {\"type\": \"w\", \"op\": \"1.mlp_layer.f.linear_1\", \"scale (log\\u2082)\": -0.002002062690453638}, {\"type\": \"w\", \"op\": \"1.mlp_layer.f.linear_2\", \"scale (log\\u2082)\": -0.002490479365954445}, {\"type\": \"w\", \"op\": \"2.attention_layer.f.ln\", \"scale (log\\u2082)\": null}, {\"type\": \"w\", \"op\": \"2.attention_layer.f.w_qkv\", \"scale (log\\u2082)\": 0.0016274135017457032}, {\"type\": \"w\", \"op\": \"2.attention_layer.f.w_o\", \"scale (log\\u2082)\": -0.002410716025626946}, {\"type\": \"w\", \"op\": \"2.mlp_layer.f.ln\", \"scale (log\\u2082)\": null}, {\"type\": \"w\", \"op\": \"2.mlp_layer.f.linear_1\", \"scale (log\\u2082)\": 0.000534251163492664}, {\"type\": \"w\", \"op\": \"2.mlp_layer.f.linear_2\", \"scale (log\\u2082)\": 0.001949309719532497}, {\"type\": \"w\", \"op\": \"3.attention_layer.f.ln\", \"scale (log\\u2082)\": null}, {\"type\": \"w\", \"op\": \"3.attention_layer.f.w_qkv\", \"scale (log\\u2082)\": -0.000498921939056515}, {\"type\": \"w\", \"op\": \"3.attention_layer.f.w_o\", \"scale (log\\u2082)\": -0.003037226746296972}, {\"type\": \"w\", \"op\": \"3.mlp_layer.f.ln\", \"scale (log\\u2082)\": null}, {\"type\": \"w\", \"op\": \"3.mlp_layer.f.linear_1\", \"scale (log\\u2082)\": 0.0011968466314778597}, {\"type\": \"w\", \"op\": \"3.mlp_layer.f.linear_2\", \"scale (log\\u2082)\": 0.00256868055591959}, {\"type\": \"w\", \"op\": \"4.attention_layer.f.ln\", \"scale (log\\u2082)\": null}, {\"type\": \"w\", \"op\": \"4.attention_layer.f.w_qkv\", \"scale (log\\u2082)\": 0.0007503371315881887}, {\"type\": \"w\", \"op\": \"4.attention_layer.f.w_o\", \"scale (log\\u2082)\": 0.0015446089443926024}, {\"type\": \"w\", \"op\": \"4.mlp_layer.f.ln\", \"scale (log\\u2082)\": null}, {\"type\": \"w\", \"op\": \"4.mlp_layer.f.linear_1\", \"scale (log\\u2082)\": -0.00044903058192860286}, {\"type\": \"w\", \"op\": \"4.mlp_layer.f.linear_2\", \"scale (log\\u2082)\": -0.00018411917604885069}, {\"type\": \"w\", \"op\": \"5.attention_layer.f.ln\", \"scale (log\\u2082)\": null}, {\"type\": \"w\", \"op\": \"5.attention_layer.f.w_qkv\", \"scale (log\\u2082)\": -0.0019423891814239636}, {\"type\": \"w\", \"op\": \"5.attention_layer.f.w_o\", \"scale (log\\u2082)\": -0.0012910493202238254}, {\"type\": \"w\", \"op\": \"5.mlp_layer.f.ln\", \"scale (log\\u2082)\": null}, {\"type\": \"w\", \"op\": \"5.mlp_layer.f.linear_1\", \"scale (log\\u2082)\": 0.0014832755341695485}, {\"type\": \"w\", \"op\": \"5.mlp_layer.f.linear_2\", \"scale (log\\u2082)\": -0.0020731919155250766}, {\"type\": \"w\", \"op\": \"lm_head\", \"scale (log\\u2082)\": -0.0004590087153274933}, {\"type\": \"grad_w\", \"op\": \"ln\", \"scale (log\\u2082)\": -0.00902708910915083}, {\"type\": \"grad_w\", \"op\": \"0.attention_layer.f.ln\", \"scale (log\\u2082)\": -0.12293684041829143}, {\"type\": \"grad_w\", \"op\": \"0.attention_layer.f.w_qkv\", \"scale (log\\u2082)\": -0.9140172112752449}, {\"type\": \"grad_w\", \"op\": \"0.attention_layer.f.w_o\", \"scale (log\\u2082)\": -0.12312113609718932}, {\"type\": \"grad_w\", \"op\": \"0.mlp_layer.f.ln\", \"scale (log\\u2082)\": 0.07769099063439702}, {\"type\": \"grad_w\", \"op\": \"0.mlp_layer.f.linear_1\", \"scale (log\\u2082)\": -0.44518341735689104}, {\"type\": \"grad_w\", \"op\": \"0.mlp_layer.f.linear_2\", \"scale (log\\u2082)\": -0.4687164743772653}, {\"type\": \"grad_w\", \"op\": \"1.attention_layer.f.ln\", \"scale (log\\u2082)\": -0.14337225324457314}, {\"type\": \"grad_w\", \"op\": \"1.attention_layer.f.w_qkv\", \"scale (log\\u2082)\": -0.9014438593268351}, {\"type\": \"grad_w\", \"op\": \"1.attention_layer.f.w_o\", \"scale (log\\u2082)\": -0.10795141423123758}, {\"type\": \"grad_w\", \"op\": \"1.mlp_layer.f.ln\", \"scale (log\\u2082)\": 0.14617631684116458}, {\"type\": \"grad_w\", \"op\": \"1.mlp_layer.f.linear_1\", \"scale (log\\u2082)\": -0.4262001686007185}, {\"type\": \"grad_w\", \"op\": \"1.mlp_layer.f.linear_2\", \"scale (log\\u2082)\": -0.4259529220529244}, {\"type\": \"grad_w\", \"op\": \"2.attention_layer.f.ln\", \"scale (log\\u2082)\": -0.05272513661972931}, {\"type\": \"grad_w\", \"op\": \"2.attention_layer.f.w_qkv\", \"scale (log\\u2082)\": -0.8668737834529171}, {\"type\": \"grad_w\", \"op\": \"2.attention_layer.f.w_o\", \"scale (log\\u2082)\": -0.07729175382809361}, {\"type\": \"grad_w\", \"op\": \"2.mlp_layer.f.ln\", \"scale (log\\u2082)\": 0.14136736057171442}, {\"type\": \"grad_w\", \"op\": \"2.mlp_layer.f.linear_1\", \"scale (log\\u2082)\": -0.41592979184148615}, {\"type\": \"grad_w\", \"op\": \"2.mlp_layer.f.linear_2\", \"scale (log\\u2082)\": -0.4377707375165417}, {\"type\": \"grad_w\", \"op\": \"3.attention_layer.f.ln\", \"scale (log\\u2082)\": -0.07243315058984288}, {\"type\": \"grad_w\", \"op\": \"3.attention_layer.f.w_qkv\", \"scale (log\\u2082)\": -0.8921531843769408}, {\"type\": \"grad_w\", \"op\": \"3.attention_layer.f.w_o\", \"scale (log\\u2082)\": -0.08545829335562806}, {\"type\": \"grad_w\", \"op\": \"3.mlp_layer.f.ln\", \"scale (log\\u2082)\": 0.17954482985031356}, {\"type\": \"grad_w\", \"op\": \"3.mlp_layer.f.linear_1\", \"scale (log\\u2082)\": -0.39329456132476037}, {\"type\": \"grad_w\", \"op\": \"3.mlp_layer.f.linear_2\", \"scale (log\\u2082)\": -0.4124012369066156}, {\"type\": \"grad_w\", \"op\": \"4.attention_layer.f.ln\", \"scale (log\\u2082)\": -0.0884351126247761}, {\"type\": \"grad_w\", \"op\": \"4.attention_layer.f.w_qkv\", \"scale (log\\u2082)\": -0.8644129787146132}, {\"type\": \"grad_w\", \"op\": \"4.attention_layer.f.w_o\", \"scale (log\\u2082)\": -0.07011748333012581}, {\"type\": \"grad_w\", \"op\": \"4.mlp_layer.f.ln\", \"scale (log\\u2082)\": 0.183038179075814}, {\"type\": \"grad_w\", \"op\": \"4.mlp_layer.f.linear_1\", \"scale (log\\u2082)\": -0.4110493502667407}, {\"type\": \"grad_w\", \"op\": \"4.mlp_layer.f.linear_2\", \"scale (log\\u2082)\": -0.4316711016851706}, {\"type\": \"grad_w\", \"op\": \"5.attention_layer.f.ln\", \"scale (log\\u2082)\": -0.06500144623616053}, {\"type\": \"grad_w\", \"op\": \"5.attention_layer.f.w_qkv\", \"scale (log\\u2082)\": -0.8750630675919389}, {\"type\": \"grad_w\", \"op\": \"5.attention_layer.f.w_o\", \"scale (log\\u2082)\": -0.07206844686967354}, {\"type\": \"grad_w\", \"op\": \"5.mlp_layer.f.ln\", \"scale (log\\u2082)\": 0.14121173434494705}, {\"type\": \"grad_w\", \"op\": \"5.mlp_layer.f.linear_1\", \"scale (log\\u2082)\": -0.40541057154996546}, {\"type\": \"grad_w\", \"op\": \"5.mlp_layer.f.linear_2\", \"scale (log\\u2082)\": -0.42794665820394606}, {\"type\": \"grad_w\", \"op\": \"lm_head\", \"scale (log\\u2082)\": 0.016387329616919712}], \"empty\": [{}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.LayerChart(...)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = UnitScaledNanoGPTModel(config)\n",
    "analyse_full_model(model, config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Success! ðŸ¥‚ ðŸŽŠ ðŸ¾ We've managed to keep every tensor in the model at unit scale for the forward and backward pass. All that was required was to re-implement standard layers with the right scaling factors.\n",
    "\n",
    "Of course, this only gives us the right scaling *at initialisation*. Scales inevitably drift throughout training, but we've given ourselves headroom by starting in the ideal place. The results in our paper show that this is sufficient to enable accurate, out-the-box training of many models, up to the size of BERT Large (we haven't tested anything largerâ€”yet!)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "#### Regular scaling\n",
    "\n",
    "To show that this really does work, let's train both our models.\n",
    "\n",
    "As in Karpathy's original NanoGPT, we'll train on the small Shakespeare dataset for a few minutes and see if we can get something that captures the rough style. Starting with the regular (non-unit-scaled) model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: hide\n",
    "from optimum.graphcore.generation_utils import IPUGenerationMixin\n",
    "from optimum.graphcore.modeling_utils import (\n",
    "    PipelineMixin,\n",
    "    outline_attribute,\n",
    "    register,\n",
    "    tied_weight_model,\n",
    ")\n",
    "\n",
    "@tied_weight_model(NanoGPTModel)\n",
    "@register(NanoGPTModel)\n",
    "class PipelinedNanoGPTModel(NanoGPTModel, PipelineMixin, IPUGenerationMixin):\n",
    "    def parallelize(self):\n",
    "        self._hooks = [outline_attribute(self.ln, \"LayerNorm\")]\n",
    "\n",
    "    def deparallelize(self):\n",
    "        pass\n",
    "\n",
    "@tied_weight_model(UnitScaledNanoGPTModel)\n",
    "@register(UnitScaledNanoGPTModel)\n",
    "class PipelinedUnitScaledNanoGPTModel(\n",
    "    UnitScaledNanoGPTModel, PipelineMixin, IPUGenerationMixin\n",
    "):\n",
    "    def parallelize(self):\n",
    "        self._hooks = [outline_attribute(self.ln, \"UnitScaledLayerNorm\")]\n",
    "\n",
    "    def deparallelize(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import poptorch_experimental_addons as pea\n",
    "\n",
    "def scaled(X, alpha=1.0, beta=1.0):\n",
    "  # Forward: Y = X * alpha\n",
    "  # Backward: grad_X = grad_Y * beta\n",
    "  return pea.autograd_proxy(X * alpha, X * beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "from datasets.load import load_dataset\n",
    "from optimum.graphcore import (\n",
    "    IPUConfig,\n",
    "    IPUTrainer,\n",
    "    IPUTrainingArguments,\n",
    "    pipeline,\n",
    "    pipelines,\n",
    ")\n",
    "from transformers import AutoTokenizer, DataCollatorForLanguageModeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset tiny_shakespeare (/nethome/charlieb/.cache/huggingface/datasets/tiny_shakespeare/default/1.0.0/b5b13969f09fe8707337f6cb296314fbe06960bd9a868dca39e713e163d27b5e)\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 668.41it/s]\n",
      "Loading cached processed dataset at /nethome/charlieb/.cache/huggingface/datasets/tiny_shakespeare/default/1.0.0/b5b13969f09fe8707337f6cb296314fbe06960bd9a868dca39e713e163d27b5e/cache-bebf307cc6e838af.arrow\n",
      "Loading cached processed dataset at /nethome/charlieb/.cache/huggingface/datasets/tiny_shakespeare/default/1.0.0/b5b13969f09fe8707337f6cb296314fbe06960bd9a868dca39e713e163d27b5e/cache-c2f9b4618a37a1c8.arrow\n",
      "Loading cached processed dataset at /nethome/charlieb/.cache/huggingface/datasets/tiny_shakespeare/default/1.0.0/b5b13969f09fe8707337f6cb296314fbe06960bd9a868dca39e713e163d27b5e/cache-6df33a0afb6a9efa.arrow\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"tiny_shakespeare\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/byt5-small\")\n",
    "\n",
    "config = NanoGPTConfig(vocab_size=len(tokenizer), eos_token_id=tokenizer.eos_token_id)\n",
    "batch_sz = 16\n",
    "seq_len = 128\n",
    "ipu_config = IPUConfig(\n",
    "    gradient_accumulation_steps=5 * 64 // batch_sz,\n",
    "    layers_per_ipu=[config.num_hidden_layers],\n",
    "    executable_cache_dir=\"./exe_cache\",\n",
    ")\n",
    "\n",
    "def split_and_tokenize(data, seq_len, batch_sz):\n",
    "    tokens = tokenizer(data[\"text\"])\n",
    "    seqs = [\n",
    "        tokens[\"input_ids\"][0][i : i + seq_len]\n",
    "        for i in range(0, len(tokens[\"input_ids\"][0]), seq_len)\n",
    "    ]\n",
    "    seqs = seqs[: int(len(seqs) / batch_sz) * batch_sz]  # make divisible by batch size\n",
    "    return {\"input_ids\": seqs}\n",
    "\n",
    "\n",
    "prep_data = partial(split_and_tokenize, seq_len=seq_len, batch_sz=batch_sz)\n",
    "tokenized_dataset = dataset.map(\n",
    "    prep_data, batched=True, remove_columns=dataset[\"train\"].column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-1c80317fa3b1799d\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-1c80317fa3b1799d\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compiling Model...\n",
      "Graph compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:01<00:00]\n",
      "Compiled/Loaded model in 5.3185042690020055 secs\n",
      "***** Running training *****\n",
      "  Num examples = 7840\n",
      "  Num Epochs = 42\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 320\n",
      "  Gradient Accumulation steps = 20\n",
      "  Total optimization steps = 1000\n",
      "  2%|â–         | 21/1000 [00:02<02:16,  7.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.4369, 'learning_rate': 0.0002, 'epoch': 0.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|â–         | 41/1000 [00:05<02:02,  7.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.7566, 'learning_rate': 0.0004, 'epoch': 1.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|â–Œ         | 61/1000 [00:08<02:00,  7.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4258, 'learning_rate': 0.0006, 'epoch': 2.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|â–Š         | 81/1000 [00:11<02:07,  7.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.2702, 'learning_rate': 0.0008, 'epoch': 3.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|â–ˆ         | 101/1000 [00:13<02:14,  6.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1507, 'learning_rate': 0.001, 'epoch': 4.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|â–ˆâ–        | 121/1000 [00:16<02:02,  7.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0396, 'learning_rate': 0.0009777777777777777, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|â–ˆâ–        | 141/1000 [00:19<02:01,  7.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9657, 'learning_rate': 0.0009555555555555556, 'epoch': 5.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|â–ˆâ–Œ        | 161/1000 [00:21<01:56,  7.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8925, 'learning_rate': 0.0009333333333333333, 'epoch': 6.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|â–ˆâ–Š        | 181/1000 [00:24<02:00,  6.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8139, 'learning_rate': 0.0009111111111111111, 'epoch': 7.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|â–ˆâ–ˆ        | 201/1000 [00:27<01:42,  7.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8059, 'learning_rate': 0.0008888888888888888, 'epoch': 8.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|â–ˆâ–ˆâ–       | 221/1000 [00:30<01:44,  7.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7422, 'learning_rate': 0.0008666666666666667, 'epoch': 9.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|â–ˆâ–ˆâ–       | 241/1000 [00:32<01:46,  7.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.733, 'learning_rate': 0.0008444444444444444, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|â–ˆâ–ˆâ–Œ       | 250/1000 [00:33<01:38,  7.61it/s]Compiling Model...\n",
      "Graph compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00]\n",
      "Compiled/Loaded model in 4.082931288983673 secs\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 432\n",
      "  Batch size = 16\n",
      "\n",
      " 25%|â–ˆâ–ˆâ–Œ       | 250/1000 [00:40<01:38,  7.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7421875, 'eval_runtime': 0.4696, 'eval_samples_per_second': 919.964, 'eval_steps_per_second': 57.498, 'epoch': 10.42}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Graph compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:01<00:00]\n",
      " 26%|â–ˆâ–ˆâ–Œ       | 261/1000 [00:47<03:01,  4.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6966, 'learning_rate': 0.0008222222222222222, 'epoch': 10.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|â–ˆâ–ˆâ–Š       | 281/1000 [00:50<01:42,  7.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6454, 'learning_rate': 0.0008, 'epoch': 11.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|â–ˆâ–ˆâ–ˆ       | 301/1000 [00:53<01:31,  7.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6402, 'learning_rate': 0.0007777777777777778, 'epoch': 12.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|â–ˆâ–ˆâ–ˆâ–      | 321/1000 [00:56<01:42,  6.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6077, 'learning_rate': 0.0007555555555555555, 'epoch': 13.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|â–ˆâ–ˆâ–ˆâ–      | 341/1000 [00:58<01:27,  7.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5972, 'learning_rate': 0.0007333333333333333, 'epoch': 14.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 361/1000 [01:01<01:25,  7.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5686, 'learning_rate': 0.0007111111111111111, 'epoch': 15.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|â–ˆâ–ˆâ–ˆâ–Š      | 381/1000 [01:04<01:28,  7.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5528, 'learning_rate': 0.000688888888888889, 'epoch': 15.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 401/1000 [01:07<01:19,  7.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5522, 'learning_rate': 0.0006666666666666666, 'epoch': 16.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 421/1000 [01:09<01:19,  7.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5131, 'learning_rate': 0.0006444444444444444, 'epoch': 17.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 441/1000 [01:12<01:19,  7.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5112, 'learning_rate': 0.0006222222222222223, 'epoch': 18.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 461/1000 [01:15<01:10,  7.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5121, 'learning_rate': 0.0006, 'epoch': 19.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 481/1000 [01:17<01:09,  7.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4964, 'learning_rate': 0.0005777777777777778, 'epoch': 20.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 500/1000 [01:20<01:04,  7.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4818, 'learning_rate': 0.0005555555555555556, 'epoch': 20.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compiling Model...\n",
      "Graph compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00]\n",
      "Compiled/Loaded model in 3.906453931995202 secs\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 432\n",
      "  Batch size = 16\n",
      "\n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 500/1000 [01:29<01:04,  7.71it/s]Saving model checkpoint to out/checkpoint-500\n",
      "/nethome/charlieb/projects/unit-scaling-notebook/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1428: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Configuration saved in out/checkpoint-500/ipu_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.56640625, 'eval_runtime': 0.4722, 'eval_samples_per_second': 914.934, 'eval_steps_per_second': 57.183, 'epoch': 20.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Graph compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:01<00:00]\n",
      " 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 521/1000 [01:38<01:14,  6.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4603, 'learning_rate': 0.0005333333333333334, 'epoch': 21.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 541/1000 [01:41<01:08,  6.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4605, 'learning_rate': 0.0005111111111111111, 'epoch': 22.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 561/1000 [01:44<01:05,  6.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.444, 'learning_rate': 0.0004888888888888889, 'epoch': 23.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 581/1000 [01:46<00:54,  7.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4429, 'learning_rate': 0.00046666666666666666, 'epoch': 24.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 601/1000 [01:49<00:59,  6.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4514, 'learning_rate': 0.0004444444444444444, 'epoch': 25.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 621/1000 [01:52<00:54,  7.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4254, 'learning_rate': 0.0004222222222222222, 'epoch': 25.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 641/1000 [01:54<00:52,  6.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4112, 'learning_rate': 0.0004, 'epoch': 26.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 661/1000 [01:57<00:50,  6.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4068, 'learning_rate': 0.00037777777777777777, 'epoch': 27.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 681/1000 [02:00<00:44,  7.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4147, 'learning_rate': 0.00035555555555555557, 'epoch': 28.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 701/1000 [02:03<00:42,  7.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3746, 'learning_rate': 0.0003333333333333333, 'epoch': 29.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 721/1000 [02:06<00:43,  6.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3948, 'learning_rate': 0.0003111111111111111, 'epoch': 30.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 741/1000 [02:09<00:36,  7.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3934, 'learning_rate': 0.0002888888888888889, 'epoch': 30.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 750/1000 [02:10<00:35,  7.02it/s]Compiling Model...\n",
      "Graph compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00]\n",
      "Compiled/Loaded model in 3.861648246005643 secs\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 432\n",
      "  Batch size = 16\n",
      "\n",
      " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 750/1000 [02:16<00:35,  7.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5185546875, 'eval_runtime': 0.4588, 'eval_samples_per_second': 941.633, 'eval_steps_per_second': 58.852, 'epoch': 31.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Graph compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:01<00:00]\n",
      " 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 761/1000 [02:24<01:02,  3.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3722, 'learning_rate': 0.0002666666666666667, 'epoch': 31.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 781/1000 [02:27<00:31,  7.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.359, 'learning_rate': 0.00024444444444444443, 'epoch': 32.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 801/1000 [02:29<00:27,  7.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3687, 'learning_rate': 0.0002222222222222222, 'epoch': 33.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 821/1000 [02:32<00:23,  7.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3617, 'learning_rate': 0.0002, 'epoch': 34.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 841/1000 [02:35<00:25,  6.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3544, 'learning_rate': 0.00017777777777777779, 'epoch': 35.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 861/1000 [02:38<00:18,  7.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3524, 'learning_rate': 0.00015555555555555556, 'epoch': 35.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 881/1000 [02:41<00:16,  7.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.33, 'learning_rate': 0.00013333333333333334, 'epoch': 36.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 901/1000 [02:44<00:13,  7.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3366, 'learning_rate': 0.0001111111111111111, 'epoch': 37.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 921/1000 [02:47<00:11,  7.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3277, 'learning_rate': 8.888888888888889e-05, 'epoch': 38.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 941/1000 [02:50<00:08,  6.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.342, 'learning_rate': 6.666666666666667e-05, 'epoch': 39.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 961/1000 [02:53<00:06,  6.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3299, 'learning_rate': 4.4444444444444447e-05, 'epoch': 40.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 981/1000 [02:56<00:02,  7.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3312, 'learning_rate': 2.2222222222222223e-05, 'epoch': 40.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [02:58<00:00,  7.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3361, 'learning_rate': 0.0, 'epoch': 41.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compiling Model...\n",
      "Graph compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00]\n",
      "Compiled/Loaded model in 4.049473897961434 secs\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 432\n",
      "  Batch size = 16\n",
      "\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [03:08<00:00,  7.21it/s]Saving model checkpoint to out/checkpoint-1000\n",
      "/nethome/charlieb/projects/unit-scaling-notebook/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1428: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Configuration saved in out/checkpoint-1000/ipu_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5078125, 'eval_runtime': 0.4775, 'eval_samples_per_second': 904.799, 'eval_steps_per_second': 56.55, 'epoch': 41.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [03:08<00:00,  5.31it/s]\n",
      "Saving model checkpoint to trained_model/\n",
      "Configuration saved in trained_model/ipu_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 188.3129, 'train_samples_per_second': 1699.3, 'train_steps_per_second': 5.31, 'train_loss': 1.6398095703125, 'epoch': 41.67}\n"
     ]
    }
   ],
   "source": [
    "train_args = IPUTrainingArguments(\n",
    "    output_dir=\"out\",\n",
    "    per_device_train_batch_size=batch_sz,\n",
    "    per_device_eval_batch_size=batch_sz,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=250,\n",
    "    logging_steps=20,\n",
    "    max_steps=1000,\n",
    "    weight_decay=0.1,\n",
    "    warmup_steps=100,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    learning_rate=1e-3,\n",
    "    loss_scaling=2**6,\n",
    ")\n",
    "\n",
    "model = NanoGPTModel(config)\n",
    "\n",
    "trainer = IPUTrainer(\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    "    model=model,\n",
    "    args=train_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    ipu_config=ipu_config,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(\"trained_model/\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we use loss scaling here, though this model is sufficiently small that it can get away without loss scaling. This certainly doesn't hold for larger models though.\n",
    "\n",
    "To really get a sense of how the model's doing, we ought to get it to write some Shakespeare. Here's two attempts at *The Tempest*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IPUConfig {\n",
      "  \"auto_loss_scaling\": false,\n",
      "  \"device_iterations\": 1,\n",
      "  \"embedding_serialization_factor\": 1,\n",
      "  \"enable_half_partials\": true,\n",
      "  \"executable_cache_dir\": \"./exe_cache\",\n",
      "  \"execute_encoder_on_cpu_for_generation\": false,\n",
      "  \"gradient_accumulation_steps\": 20,\n",
      "  \"inference_device_iterations\": 1,\n",
      "  \"inference_replication_factor\": 1,\n",
      "  \"ipus_per_replica\": 1,\n",
      "  \"layers_per_ipu\": [\n",
      "    6\n",
      "  ],\n",
      "  \"matmul_proportion\": 0.6,\n",
      "  \"optimizer_state_offchip\": true,\n",
      "  \"optimum_version\": \"1.6.1\",\n",
      "  \"output_mode\": \"final\",\n",
      "  \"recompute_checkpoint_every_layer\": false,\n",
      "  \"replicated_tensor_sharding\": false,\n",
      "  \"replication_factor\": 1,\n",
      "  \"seed\": null,\n",
      "  \"transformers_version\": \"4.25.1\"\n",
      "}\n",
      "\n",
      "Graph compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Completion [1] =====\n",
      "\n",
      "PROSPERO:\n",
      "Our revels now are ended. These our actors,\n",
      "As I foretold you, were all spirits him\n",
      "And here is a part to the scorns of him.\n",
      "\n",
      "BRUTUS:\n",
      "Well, here's a very foul of the house to the common\n",
      "To be secret the trumpets of the people's revenge,\n",
      "That he presently presence.\n",
      "\n",
      "CORIOLANUS:\n",
      "The gods of the slanders of the children of the world\n",
      "The consuls of the cause of the love.\n",
      "\n",
      "POLIXENES:\n",
      "The gods and Capulet, good Camillo's such as as the\n",
      "That resign as her for the country.\n",
      "\n",
      "BUCKINGHAM:\n",
      "So shall be the sta\n",
      "\n",
      "===== Completion [2] =====\n",
      "\n",
      "PROSPERO:\n",
      "Our revels now are ended. These our actors,\n",
      "As I foretold you, were all spirits in the state.\n",
      "\n",
      "DUKE OF YORK:\n",
      "I have stay a fair is the strew and parts the fair\n",
      "And therefore I would not the people.\n",
      "\n",
      "GLOUCESTER:\n",
      "The king is the heads of a king, and like thee\n",
      "Will be proud in the seasons of your father.\n",
      "\n",
      "KING RICHARD III:\n",
      "Why, then, then, the heavens it was a state?\n",
      "\n",
      "LADY ANNE:\n",
      "Why, what says the hath the prince of the grants\n",
      "That hath should be the death of the prince,\n",
      "And the duke livery with him \n"
     ]
    }
   ],
   "source": [
    "pipelines.check_model_type = lambda self, supported_models: ...\n",
    "\n",
    "final_model = NanoGPTModel.from_pretrained(\"trained_model/\")\n",
    "\n",
    "TEST_INPUT = \"\"\"PROSPERO:\n",
    "Our revels now are ended. These our actors,\n",
    "As I foretold you, were all spirits\"\"\"\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    ipu_config=ipu_config.to_dict(),  # TODO: feature request -> no to_dict()\n",
    "    model=final_model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=512,\n",
    "    do_sample=True,\n",
    ")\n",
    "\n",
    "outputs = pipe(TEST_INPUT, num_return_sequences=2, temperature=0.4)\n",
    "for i, output in enumerate(outputs):\n",
    "    print(f\"\\n===== Completion [{i+1}] =====\\n\")\n",
    "    print(output[\"generated_text\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's not going to win any literary awards, but pretty good for a few minutes of training. Now let's see if our unit-scaled model can do the same..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unit scaling"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All we need to do is swap in our new model, remove the loss scaling (of course!), and increase the learning rate (as our weights are now larger due to unit-initialisation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: hide?\n",
    "from types import MethodType\n",
    "\n",
    "class _IPUConfig(IPUConfig):\n",
    "    def to_options(self, *args, **kwargs):\n",
    "        options = super().to_options(*args, **kwargs)\n",
    "        options._popart.setPatterns(dict(AutogradProxyOpPattern=True))\n",
    "        return options\n",
    "\n",
    "    def for_pod_type(self, *args, **kwargs):\n",
    "        config = super().for_pod_type(*args, **kwargs)\n",
    "        config.__class__ = _IPUConfig\n",
    "        return config\n",
    "\n",
    "ipu_config.__class__ = _IPUConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UnitScaledNanoGPTModel(config)\n",
    "\n",
    "train_args.loss_scaling = 1.0\n",
    "train_args.learning_rate = 2e-2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ¤ž The moment of truth ... let's train our unit-scaled model ðŸ¤ž"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "Compiling Model...\n",
      "Graph compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:02<00:00]\n",
      "Compiled/Loaded model in 12.797130857012235 secs\n",
      "***** Running training *****\n",
      "  Num examples = 7840\n",
      "  Num Epochs = 42\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 320\n",
      "  Gradient Accumulation steps = 20\n",
      "  Total optimization steps = 1000\n",
      "  2%|â–         | 21/1000 [00:03<02:30,  6.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.7835, 'learning_rate': 0.004, 'epoch': 0.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|â–         | 41/1000 [00:06<02:23,  6.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.8256, 'learning_rate': 0.008, 'epoch': 1.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|â–Œ         | 61/1000 [00:09<02:26,  6.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4451, 'learning_rate': 0.012, 'epoch': 2.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|â–Š         | 81/1000 [00:12<02:04,  7.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.2794, 'learning_rate': 0.016, 'epoch': 3.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|â–ˆ         | 101/1000 [00:15<02:15,  6.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.15, 'learning_rate': 0.02, 'epoch': 4.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|â–ˆâ–        | 121/1000 [00:18<02:02,  7.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.056, 'learning_rate': 0.019555555555555555, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|â–ˆâ–        | 141/1000 [00:21<02:02,  6.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9731, 'learning_rate': 0.019111111111111113, 'epoch': 5.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|â–ˆâ–Œ        | 161/1000 [00:24<02:03,  6.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9058, 'learning_rate': 0.018666666666666668, 'epoch': 6.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|â–ˆâ–Š        | 181/1000 [00:26<01:55,  7.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8505, 'learning_rate': 0.018222222222222223, 'epoch': 7.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|â–ˆâ–ˆ        | 201/1000 [00:29<01:54,  6.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7972, 'learning_rate': 0.017777777777777778, 'epoch': 8.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|â–ˆâ–ˆâ–       | 221/1000 [00:32<01:51,  6.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7667, 'learning_rate': 0.017333333333333336, 'epoch': 9.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|â–ˆâ–ˆâ–       | 241/1000 [00:35<01:40,  7.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7384, 'learning_rate': 0.01688888888888889, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|â–ˆâ–ˆâ–Œ       | 250/1000 [00:36<01:46,  7.05it/s]Compiling Model...\n",
      "Graph compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:01<00:00]\n",
      "Compiled/Loaded model in 10.73408320802264 secs\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 432\n",
      "  Batch size = 16\n",
      "\n",
      " 25%|â–ˆâ–ˆâ–Œ       | 250/1000 [00:50<01:46,  7.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7333984375, 'eval_runtime': 0.6275, 'eval_samples_per_second': 688.487, 'eval_steps_per_second': 43.03, 'epoch': 10.42}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Graph compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:02<00:00]\n",
      " 26%|â–ˆâ–ˆâ–Œ       | 261/1000 [01:04<04:29,  2.74it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7074, 'learning_rate': 0.016444444444444446, 'epoch': 10.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|â–ˆâ–ˆâ–Š       | 281/1000 [01:07<01:45,  6.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6881, 'learning_rate': 0.016, 'epoch': 11.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|â–ˆâ–ˆâ–ˆ       | 301/1000 [01:10<01:44,  6.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6322, 'learning_rate': 0.015555555555555557, 'epoch': 12.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|â–ˆâ–ˆâ–ˆâ–      | 321/1000 [01:13<01:48,  6.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6246, 'learning_rate': 0.015111111111111112, 'epoch': 13.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|â–ˆâ–ˆâ–ˆâ–      | 341/1000 [01:16<01:31,  7.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6069, 'learning_rate': 0.014666666666666666, 'epoch': 14.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 361/1000 [01:19<01:40,  6.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5772, 'learning_rate': 0.014222222222222223, 'epoch': 15.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|â–ˆâ–ˆâ–ˆâ–Š      | 381/1000 [01:22<01:23,  7.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5472, 'learning_rate': 0.013777777777777778, 'epoch': 15.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 401/1000 [01:25<01:32,  6.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5531, 'learning_rate': 0.013333333333333332, 'epoch': 16.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 421/1000 [01:27<01:27,  6.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5266, 'learning_rate': 0.01288888888888889, 'epoch': 17.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 441/1000 [01:30<01:13,  7.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5199, 'learning_rate': 0.012444444444444445, 'epoch': 18.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 461/1000 [01:33<01:11,  7.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5043, 'learning_rate': 0.012, 'epoch': 19.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 481/1000 [01:36<01:15,  6.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4862, 'learning_rate': 0.011555555555555555, 'epoch': 20.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 500/1000 [01:38<01:05,  7.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4942, 'learning_rate': 0.011111111111111112, 'epoch': 20.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compiling Model...\n",
      "Graph compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:01<00:00]\n",
      "Compiled/Loaded model in 10.706514268007595 secs\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 432\n",
      "  Batch size = 16\n",
      "\n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 500/1000 [01:52<01:05,  7.65it/s]Saving model checkpoint to out/checkpoint-500\n",
      "/nethome/charlieb/projects/unit-scaling-notebook/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1428: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Configuration saved in out/checkpoint-500/ipu_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5400390625, 'eval_runtime': 0.6162, 'eval_samples_per_second': 701.023, 'eval_steps_per_second': 43.814, 'epoch': 20.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Graph compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:02<00:00]\n",
      " 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 521/1000 [02:07<01:17,  6.16it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4525, 'learning_rate': 0.010666666666666666, 'epoch': 21.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 541/1000 [02:10<01:10,  6.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.455, 'learning_rate': 0.010222222222222221, 'epoch': 22.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 561/1000 [02:13<01:07,  6.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4438, 'learning_rate': 0.009777777777777778, 'epoch': 23.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 581/1000 [02:16<00:59,  7.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4358, 'learning_rate': 0.009333333333333334, 'epoch': 24.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 601/1000 [02:19<00:56,  7.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4157, 'learning_rate': 0.008888888888888889, 'epoch': 25.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 621/1000 [02:22<00:53,  7.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4046, 'learning_rate': 0.008444444444444445, 'epoch': 25.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 641/1000 [02:25<00:55,  6.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3781, 'learning_rate': 0.008, 'epoch': 26.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 661/1000 [02:27<00:47,  7.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3857, 'learning_rate': 0.007555555555555556, 'epoch': 27.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 681/1000 [02:30<00:45,  7.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3919, 'learning_rate': 0.0071111111111111115, 'epoch': 28.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 701/1000 [02:33<00:42,  7.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3537, 'learning_rate': 0.006666666666666666, 'epoch': 29.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 721/1000 [02:36<00:37,  7.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3511, 'learning_rate': 0.006222222222222223, 'epoch': 30.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 741/1000 [02:39<00:37,  6.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3359, 'learning_rate': 0.0057777777777777775, 'epoch': 30.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 750/1000 [02:40<00:33,  7.53it/s]Compiling Model...\n",
      "Graph compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:01<00:00]\n",
      "Compiled/Loaded model in 11.012903372000437 secs\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 432\n",
      "  Batch size = 16\n",
      "\n",
      " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 750/1000 [02:57<00:33,  7.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.4873046875, 'eval_runtime': 0.612, 'eval_samples_per_second': 705.875, 'eval_steps_per_second': 44.117, 'epoch': 31.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Graph compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:02<00:00]\n",
      " 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 761/1000 [03:13<01:39,  2.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3398, 'learning_rate': 0.005333333333333333, 'epoch': 31.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 781/1000 [03:16<00:33,  6.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3367, 'learning_rate': 0.004888888888888889, 'epoch': 32.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 801/1000 [03:19<00:27,  7.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.327, 'learning_rate': 0.0044444444444444444, 'epoch': 33.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 821/1000 [03:22<00:28,  6.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.322, 'learning_rate': 0.004, 'epoch': 34.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 840/1000 [03:25<00:24,  6.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2932, 'learning_rate': 0.0035555555555555557, 'epoch': 35.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 861/1000 [03:28<00:19,  6.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2955, 'learning_rate': 0.0031111111111111114, 'epoch': 35.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 881/1000 [03:31<00:17,  6.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2822, 'learning_rate': 0.0026666666666666666, 'epoch': 36.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 901/1000 [03:34<00:14,  6.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2897, 'learning_rate': 0.0022222222222222222, 'epoch': 37.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 921/1000 [03:37<00:11,  6.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2786, 'learning_rate': 0.0017777777777777779, 'epoch': 38.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 941/1000 [03:40<00:07,  7.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2674, 'learning_rate': 0.0013333333333333333, 'epoch': 39.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 961/1000 [03:42<00:05,  6.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2678, 'learning_rate': 0.0008888888888888889, 'epoch': 40.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 981/1000 [03:45<00:02,  7.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2452, 'learning_rate': 0.00044444444444444447, 'epoch': 40.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [03:48<00:00,  6.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2518, 'learning_rate': 0.0, 'epoch': 41.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compiling Model...\n",
      "Graph compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:01<00:00]\n",
      "Compiled/Loaded model in 11.062077922048047 secs\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 432\n",
      "  Batch size = 16\n",
      "\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [04:03<00:00,  6.79it/s]Saving model checkpoint to out/checkpoint-1000\n",
      "/nethome/charlieb/projects/unit-scaling-notebook/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1428: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Configuration saved in out/checkpoint-1000/ipu_config.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [04:03<00:00,  4.10it/s]\n",
      "Saving model checkpoint to trained_model/\n",
      "Configuration saved in trained_model/ipu_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.466796875, 'eval_runtime': 0.7101, 'eval_samples_per_second': 608.345, 'eval_steps_per_second': 38.022, 'epoch': 41.67}\n",
      "{'train_runtime': 243.6652, 'train_samples_per_second': 1313.277, 'train_steps_per_second': 4.104, 'train_loss': 1.632796875, 'epoch': 41.67}\n"
     ]
    }
   ],
   "source": [
    "unit_scale_trainer = IPUTrainer(\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    "    model=model,\n",
    "    args=train_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    ipu_config=ipu_config,\n",
    ")\n",
    "\n",
    "unit_scale_trainer.train()\n",
    "unit_scale_trainer.save_model(\"trained_model/\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get a similar evaluation loss as the regular modelâ€”a success! In fact after sweeping the learning rate, unit scaling appears to be slightly better here.\n",
    "\n",
    "Let's celebrate with some unit-scaled Shakespeare..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IPUConfig {\n",
      "  \"auto_loss_scaling\": false,\n",
      "  \"device_iterations\": 1,\n",
      "  \"embedding_serialization_factor\": 1,\n",
      "  \"enable_half_partials\": true,\n",
      "  \"executable_cache_dir\": \"./exe_cache\",\n",
      "  \"execute_encoder_on_cpu_for_generation\": false,\n",
      "  \"gradient_accumulation_steps\": 20,\n",
      "  \"inference_device_iterations\": 1,\n",
      "  \"inference_replication_factor\": 1,\n",
      "  \"ipus_per_replica\": 1,\n",
      "  \"layers_per_ipu\": [\n",
      "    6\n",
      "  ],\n",
      "  \"matmul_proportion\": 0.6,\n",
      "  \"optimizer_state_offchip\": true,\n",
      "  \"optimum_version\": \"1.6.1\",\n",
      "  \"output_mode\": \"final\",\n",
      "  \"recompute_checkpoint_every_layer\": false,\n",
      "  \"replicated_tensor_sharding\": false,\n",
      "  \"replication_factor\": 1,\n",
      "  \"seed\": null,\n",
      "  \"transformers_version\": \"4.25.1\"\n",
      "}\n",
      "\n",
      "Graph compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:01<00:00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Completion [1] =====\n",
      "\n",
      "PROSPERO:\n",
      "Our revels now are ended. These our actors,\n",
      "As I foretold you, were all spirits of the\n",
      "provost: the strength of your honours and strength\n",
      "To meet the world in the violence of your father's country's life\n",
      "Than the ground of the court-wings of all the death.\n",
      "\n",
      "KING HENRY VI:\n",
      "My lord, the last is the father for the court?\n",
      "\n",
      "KING RICHARD III:\n",
      "No more than I see thee to be a peace\n",
      "And more than my brother's love in the gods\n",
      "And the root of my counsel: therefore I mean not\n",
      "The sister of the charge of the \n",
      "\n",
      "===== Completion [2] =====\n",
      "\n",
      "PROSPERO:\n",
      "Our revels now are ended. These our actors,\n",
      "As I foretold you, were all spirits for the head\n",
      "To see him that live and dear at his country's head,\n",
      "The sea of his crown, sir, the last of the ground\n",
      "Of our highness and the sun of his presence\n",
      "Of the singer of the steed service of her sense,\n",
      "That she was not her of your grace was for his son.\n",
      "\n",
      "KING RICHARD III:\n",
      "What is the done? what thou art thou dost deserve?\n",
      "\n",
      "DUKE OF AUMERLE:\n",
      "Pray, the good man! what says he lightning how\n",
      "That he was before his des\n"
     ]
    }
   ],
   "source": [
    "pipelines.check_model_type = lambda self, supported_models: ...\n",
    "\n",
    "final_model = UnitScaledNanoGPTModel.from_pretrained(\"trained_model/\")\n",
    "\n",
    "\n",
    "TEST_INPUT = \"\"\"PROSPERO:\n",
    "Our revels now are ended. These our actors,\n",
    "As I foretold you, were all spirits\"\"\"\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    ipu_config=ipu_config.to_dict(),  # TODO: feature request -> no to_dict()\n",
    "    model=final_model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=512,\n",
    "    do_sample=True,\n",
    ")\n",
    "\n",
    "outputs = pipe(TEST_INPUT, num_return_sequences=2, temperature=0.4)\n",
    "for i, output in enumerate(outputs):\n",
    "    print(f\"\\n===== Completion [{i+1}] =====\\n\")\n",
    "    print(output[\"generated_text\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some neat verse, from a neatly-scaled model. From which we can only conclude:\n",
    "\n",
    "*Enter: unit scaling*\n",
    "\n",
    "*Exeunt: loss scaling, automatic loss scaling, etc.*\n",
    "\n",
    "FIN\n",
    "\n",
    "---\n",
    "\n",
    "We hope that practitioners will consider using unit scaling for future projects, particularly those having difficulties with loss scaling or automatic mixed precision. With FP8 on the horizon, these issues are likely to become more prevalent. We hope unit scaling can help.\n",
    "\n",
    "If you're interested in using unit scaling yourself, or have questions, please do reach out ðŸ™â˜Žï¸ We're keen to hear from anyone that has a problem unit scaling might help solve.\n",
    "\n",
    "The definitions provided here are the closest we have to an \"official\" PyTorch implementation, but if there's demand for a library tell us and we'll make one!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
