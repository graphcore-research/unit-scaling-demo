{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) 2023 Graphcore Ltd. All rights reserved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unit Scaling: A How-To Guide\n",
    "\n",
    "In our paper [Unit Scaling: Out-of-the-Box Low-Precision Training](TODO), we describe a scheme for designing neural networks that have approximate unit variance after every operation in the forward and backward pass.\n",
    "\n",
    "This can be seen as an alternative to (static) loss scaling, or its automatic variant, as used in Automatic Mixed Precision. Whereas both of those schemes rely on a single, global scaling factor for all the gradients, unit scaling is more fine-grained.\n",
    "\n",
    "A unit-scaled model adds scaling factors (constant scalar multiplications) to each operation in the computational graph to achieve this unit variance property. The result is a model which naturally produces tensors in the middle of the dynamic range provided by floating-point formats. There's no extra loss-scale hyperparameter—it works out-of-the-box!\n",
    "\n",
    "## Implementing unit scaling\n",
    "\n",
    "Here we demonstrate how to go about unit scaling in practice. This involves re-implementing common neural network layers to add variance-preserving scaling factors.\n",
    "\n",
    "As explained in the paper, we can sometimes justify using different scaling factors in the forward and backward pass. We introduce a special `scaled()` op which allows us to do just that.\n",
    "\n",
    "Below we implement two models. The first is a simple implementation of a transformer-decoder. The design and hyperparameters are inspired by Andrej Karpathy's [popular NanoGPT implementation](https://github.com/karpathy/nanoGPT) (though some differ). It's also 🤗-compatible!\n",
    "\n",
    "The second model is the same, but unit-scaled. Let's get stuck in...\n",
    "\n",
    "## Building a unit-scaled NanoGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2023-03-20T16:49:00.038233Z",
     "iopub.status.busy": "2023-03-20T16:49:00.037951Z",
     "iopub.status.idle": "2023-03-20T16:50:13.931705Z",
     "shell.execute_reply": "2023-03-20T16:50:13.930690Z",
     "shell.execute_reply.started": "2023-03-20T16:49:00.038207Z"
    },
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/huggingface/optimum-graphcore.git\n",
      "  Cloning https://github.com/huggingface/optimum-graphcore.git to /tmp/pip-req-build-0ay1rt57\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/optimum-graphcore.git /tmp/pip-req-build-0ay1rt57\n",
      "  Resolved https://github.com/huggingface/optimum-graphcore.git to commit 78756c7e2a7ed3bbce034708598ddab5619cb653\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting datasets\n",
      "  Downloading datasets-2.10.1-py3-none-any.whl (469 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m469.0/469.0 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (from optimum-graphcore==0.6.0.dev0) (1.13.0+cpu)\n",
      "Collecting scipy\n",
      "  Downloading scipy-1.10.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.5/34.5 MB\u001b[0m \u001b[31m62.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting pillow\n",
      "  Downloading Pillow-9.4.0-cp38-cp38-manylinux_2_28_x86_64.whl (3.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m81.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hCollecting transformers==4.25.1\n",
      "  Downloading transformers-4.25.1-py3-none-any.whl (5.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m76.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting tokenizers\n",
      "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m82.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.97-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m68.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting optimum\n",
      "  Downloading optimum-1.7.1-py3-none-any.whl (279 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m279.1/279.1 kB\u001b[0m \u001b[31m52.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers==4.25.1->optimum-graphcore==0.6.0.dev0) (2.28.2)\n",
      "Collecting filelock\n",
      "  Downloading filelock-3.10.0-py3-none-any.whl (9.9 kB)\n",
      "Collecting numpy>=1.17\n",
      "  Downloading numpy-1.24.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m61.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers==4.25.1->optimum-graphcore==0.6.0.dev0) (23.0)\n",
      "Collecting regex!=2019.12.17\n",
      "  Downloading regex-2022.10.31-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (772 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m772.3/772.3 kB\u001b[0m \u001b[31m77.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting huggingface-hub<1.0,>=0.10.0\n",
      "  Downloading huggingface_hub-0.13.3-py3-none-any.whl (199 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.8/199.8 kB\u001b[0m \u001b[31m42.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers==4.25.1->optimum-graphcore==0.6.0.dev0) (4.64.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers==4.25.1->optimum-graphcore==0.6.0.dev0) (5.4.1)\n",
      "Collecting multiprocess\n",
      "  Downloading multiprocess-0.70.14-py38-none-any.whl (132 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.0/132.0 kB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting responses<0.19\n",
      "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Collecting aiohttp\n",
      "  Downloading aiohttp-3.8.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m78.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting xxhash\n",
      "  Downloading xxhash-3.2.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (213 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m213.0/213.0 kB\u001b[0m \u001b[31m44.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pandas\n",
      "  Downloading pandas-1.5.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.2/12.2 MB\u001b[0m \u001b[31m78.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting fsspec[http]>=2021.11.1\n",
      "  Downloading fsspec-2023.3.0-py3-none-any.whl (145 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m145.4/145.4 kB\u001b[0m \u001b[31m35.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pyarrow>=6.0.0\n",
      "  Downloading pyarrow-11.0.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (35.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.0/35.0 MB\u001b[0m \u001b[31m58.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting dill<0.3.7,>=0.3.0\n",
      "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting transformers[sentencepiece]>=4.26.0\n",
      "  Downloading transformers-4.27.1-py3-none-any.whl (6.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m76.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting coloredlogs\n",
      "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting sympy\n",
      "  Downloading sympy-1.11.1-py3-none-any.whl (6.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m66.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch->optimum-graphcore==0.6.0.dev0) (4.5.0)\n",
      "Collecting yarl<2.0,>=1.0\n",
      "  Downloading yarl-1.8.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (262 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m262.1/262.1 kB\u001b[0m \u001b[31m51.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->optimum-graphcore==0.6.0.dev0) (22.2.0)\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.3.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (161 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m161.3/161.3 kB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.0.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (121 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.3/121.3 kB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3\n",
      "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->optimum-graphcore==0.6.0.dev0) (3.0.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->transformers==4.25.1->optimum-graphcore==0.6.0.dev0) (2019.11.28)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/lib/python3/dist-packages (from requests->transformers==4.25.1->optimum-graphcore==0.6.0.dev0) (1.25.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->transformers==4.25.1->optimum-graphcore==0.6.0.dev0) (2.8)\n",
      "Collecting urllib3<1.27,>=1.21.1\n",
      "  Downloading urllib3-1.26.15-py2.py3-none-any.whl (140 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.9/140.9 kB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting transformers[sentencepiece]>=4.26.0\n",
      "  Downloading transformers-4.27.0-py3-none-any.whl (6.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m77.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Downloading transformers-4.26.1-py3-none-any.whl (6.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m76.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Downloading transformers-4.26.0-py3-none-any.whl (6.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hINFO: pip is looking at multiple versions of tqdm to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting tqdm>=4.27\n",
      "  Downloading tqdm-4.65.0-py3-none-any.whl (77 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.1/77.1 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hINFO: pip is looking at multiple versions of responses to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting responses<0.19\n",
      "  Downloading responses-0.17.0-py2.py3-none-any.whl (38 kB)\n",
      "INFO: pip is looking at multiple versions of requests to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting requests\n",
      "  Downloading requests-2.28.2-py3-none-any.whl (62 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hINFO: pip is looking at multiple versions of regex to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting regex!=2019.12.17\n",
      "  Downloading regex-2022.9.13-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (772 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m772.3/772.3 kB\u001b[0m \u001b[31m61.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hINFO: pip is looking at multiple versions of pyyaml to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting pyyaml>=5.1\n",
      "  Downloading PyYAML-6.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (701 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m701.2/701.2 kB\u001b[0m \u001b[31m77.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hINFO: pip is looking at multiple versions of pyarrow to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting pyarrow>=6.0.0\n",
      "  Downloading pyarrow-10.0.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.0/36.0 MB\u001b[0m \u001b[31m58.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hINFO: pip is looking at multiple versions of packaging to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting packaging>=20.0\n",
      "  Downloading packaging-23.0-py3-none-any.whl (42 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.7/42.7 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hINFO: pip is looking at multiple versions of numpy to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting numpy>=1.17\n",
      "  Downloading numpy-1.24.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m71.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hINFO: pip is looking at multiple versions of huggingface-hub to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting huggingface-hub<1.0,>=0.10.0\n",
      "  Downloading huggingface_hub-0.13.2-py3-none-any.whl (199 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.2/199.2 kB\u001b[0m \u001b[31m44.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hINFO: pip is looking at multiple versions of aiohttp to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting aiohttp\n",
      "  Downloading aiohttp-3.8.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m63.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hINFO: pip is looking at multiple versions of fsspec[http] to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting fsspec[http]>=2021.11.1\n",
      "  Downloading fsspec-2023.1.0-py3-none-any.whl (143 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.0/143.0 kB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hINFO: pip is looking at multiple versions of dill to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting dill<0.3.7,>=0.3.0\n",
      "  Downloading dill-0.3.5.1-py2.py3-none-any.whl (95 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.8/95.8 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hINFO: pip is looking at multiple versions of sentencepiece to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.96-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m68.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hINFO: pip is looking at multiple versions of scipy to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting scipy\n",
      "  Downloading scipy-1.10.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.5/34.5 MB\u001b[0m \u001b[31m58.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hINFO: pip is looking at multiple versions of pillow to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting pillow\n",
      "  Downloading Pillow-9.3.0-cp38-cp38-manylinux_2_28_x86_64.whl (3.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m77.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hINFO: pip is looking at multiple versions of torch to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting torch\n",
      "  Downloading torch-2.0.0-cp38-cp38-manylinux1_x86_64.whl (619.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m619.9/619.9 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hINFO: pip is looking at multiple versions of optimum to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting optimum\n",
      "  Downloading optimum-1.7.0-py3-none-any.whl (279 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m279.0/279.0 kB\u001b[0m \u001b[31m51.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading optimum-1.6.4-py3-none-any.whl (227 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.8/227.8 kB\u001b[0m \u001b[31m48.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting numpy>=1.17\n",
      "  Downloading numpy-1.23.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m75.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting optimum\n",
      "  Downloading optimum-1.6.3-py3-none-any.whl (227 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.4/227.4 kB\u001b[0m \u001b[31m45.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading optimum-1.6.2-py3-none-any.whl (227 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.3/227.3 kB\u001b[0m \u001b[31m39.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading optimum-1.6.1-py3-none-any.whl (222 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m222.6/222.6 kB\u001b[0m \u001b[31m49.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting protobuf<=3.20.2\n",
      "  Downloading protobuf-3.20.2-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m76.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting humanfriendly>=9.1\n",
      "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets->optimum-graphcore==0.6.0.dev0) (2022.7.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets->optimum-graphcore==0.6.0.dev0) (2.8.2)\n",
      "Collecting mpmath>=0.19\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m71.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.1->pandas->datasets->optimum-graphcore==0.6.0.dev0) (1.14.0)\n",
      "Building wheels for collected packages: optimum-graphcore\n",
      "  Building wheel for optimum-graphcore (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for optimum-graphcore: filename=optimum_graphcore-0.6.0.dev0-py3-none-any.whl size=163372 sha256=655efec6ef367c86767e64a41f159d1ce45d0cb2a19dce57cd9aadb479605639\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-vdbut82n/wheels/ee/d4/6d/d7f5a3cead7a39c4f637049180f1cfdb2b988987ede78bf858\n",
      "Successfully built optimum-graphcore\n",
      "Installing collected packages: tokenizers, sentencepiece, mpmath, xxhash, urllib3, sympy, regex, protobuf, pillow, numpy, multidict, humanfriendly, fsspec, frozenlist, filelock, dill, async-timeout, yarl, scipy, pyarrow, pandas, multiprocess, coloredlogs, aiosignal, responses, huggingface-hub, aiohttp, transformers, datasets, optimum, optimum-graphcore\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.25.8\n",
      "    Uninstalling urllib3-1.25.8:\n",
      "      Successfully uninstalled urllib3-1.25.8\n",
      "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 coloredlogs-15.0.1 datasets-2.10.1 dill-0.3.6 filelock-3.10.0 frozenlist-1.3.3 fsspec-2023.3.0 huggingface-hub-0.13.3 humanfriendly-10.0 mpmath-1.3.0 multidict-6.0.4 multiprocess-0.70.14 numpy-1.23.5 optimum-1.6.1 optimum-graphcore-0.6.0.dev0 pandas-1.5.3 pillow-9.4.0 protobuf-3.20.2 pyarrow-11.0.0 regex-2022.10.31 responses-0.18.0 scipy-1.10.1 sentencepiece-0.1.97 sympy-1.11.1 tokenizers-0.13.2 transformers-4.25.1 urllib3-1.26.15 xxhash-3.2.0 yarl-1.8.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting git+https://github.com/graphcore-research/poptorch-experimental-addons\n",
      "  Cloning https://github.com/graphcore-research/poptorch-experimental-addons to /tmp/pip-req-build-fidyluv4\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/graphcore-research/poptorch-experimental-addons /tmp/pip-req-build-fidyluv4\n",
      "  Resolved https://github.com/graphcore-research/poptorch-experimental-addons to commit 27812acef2fe9e951eb76e81495709c54b0e8c65\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from poptorch-experimental-addons==0.1) (1.23.5)\n",
      "Collecting einops\n",
      "  Downloading einops-0.6.0-py3-none-any.whl (41 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.6/41.6 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: poptorch-experimental-addons\n",
      "  Building wheel for poptorch-experimental-addons (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for poptorch-experimental-addons: filename=poptorch_experimental_addons-0.1-cp38-cp38-linux_x86_64.whl size=7030890 sha256=aadcc512a797f6954cad1a6b224b8b40f0fcfe775689b82c5b9e1774835a993b\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-c3aeybyg/wheels/93/ac/ba/7e2d0a8a20fe42df8743ad3df26d32436183ee92605d203506\n",
      "Successfully built poptorch-experimental-addons\n",
      "Installing collected packages: einops, poptorch-experimental-addons\n",
      "Successfully installed einops-0.6.0 poptorch-experimental-addons-0.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting altair\n",
      "  Downloading altair-4.2.2-py3-none-any.whl (813 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m813.6/813.6 kB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pandas>=0.18 in /usr/local/lib/python3.8/dist-packages (from altair) (1.5.3)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from altair) (1.23.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from altair) (3.1.2)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.8/dist-packages (from altair) (4.17.3)\n",
      "Collecting entrypoints\n",
      "  Downloading entrypoints-0.4-py3-none-any.whl (5.3 kB)\n",
      "Collecting toolz\n",
      "  Downloading toolz-0.12.0-py3-none-any.whl (55 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.8/55.8 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pkgutil-resolve-name>=1.3.10 in /usr/local/lib/python3.8/dist-packages (from jsonschema>=3.0->altair) (1.3.10)\n",
      "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema>=3.0->altair) (5.12.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema>=3.0->altair) (22.2.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema>=3.0->altair) (0.19.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.8/dist-packages (from pandas>=0.18->altair) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.8/dist-packages (from pandas>=0.18->altair) (2022.7.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.8/dist-packages (from jinja2->altair) (2.1.2)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.8/dist-packages (from importlib-resources>=1.4.0->jsonschema>=3.0->altair) (3.14.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.1->pandas>=0.18->altair) (1.14.0)\n",
      "Installing collected packages: toolz, entrypoints, altair\n",
      "Successfully installed altair-4.2.2 entrypoints-0.4 toolz-0.12.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/huggingface/optimum-graphcore.git\n",
    "!pip install git+https://github.com/graphcore-research/poptorch-experimental-addons\n",
    "!pip install altair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-20T16:50:13.933274Z",
     "iopub.status.busy": "2023-03-20T16:50:13.933081Z",
     "iopub.status.idle": "2023-03-20T16:50:15.769715Z",
     "shell.execute_reply": "2023-03-20T16:50:15.768894Z",
     "shell.execute_reply.started": "2023-03-20T16:50:13.933252Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import Dict, Optional\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, Tensor\n",
    "from transformers.activations import GELUActivation\n",
    "from transformers.configuration_utils import PretrainedConfig\n",
    "from transformers.modeling_outputs import CausalLMOutputWithCrossAttentions\n",
    "from transformers.modeling_utils import PreTrainedModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The MLP layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regular scaling\n",
    "\n",
    "We'll start by setting up a basic config for a reasonably small transformer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-20T16:50:15.771522Z",
     "iopub.status.busy": "2023-03-20T16:50:15.771288Z",
     "iopub.status.idle": "2023-03-20T16:50:15.776401Z",
     "shell.execute_reply": "2023-03-20T16:50:15.775807Z",
     "shell.execute_reply.started": "2023-03-20T16:50:15.771502Z"
    }
   },
   "outputs": [],
   "source": [
    "class NanoGPTConfig(PretrainedConfig):\n",
    "    model_type = \"nano-gpt\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_size: int = 384,\n",
    "        num_hidden_layers: int = 6,\n",
    "        num_attention_heads: int = 6,\n",
    "        dropout: float = 0.1,\n",
    "        vocab_size: int = 384,\n",
    "        eos_token_id: int = 1,\n",
    "        **kwargs,\n",
    "    ) -> None:\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "        self.dropout = dropout\n",
    "        self.vocab_size = vocab_size\n",
    "        self.eos_token_id = eos_token_id\n",
    "        super().__init__(**kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Along with a standard (pre-norm) MLP module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-20T16:50:15.777639Z",
     "iopub.status.busy": "2023-03-20T16:50:15.777459Z",
     "iopub.status.idle": "2023-03-20T16:50:15.783811Z",
     "shell.execute_reply": "2023-03-20T16:50:15.783104Z",
     "shell.execute_reply.started": "2023-03-20T16:50:15.777620Z"
    }
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, config: NanoGPTConfig) -> None:\n",
    "        super().__init__()\n",
    "        self.ln = nn.LayerNorm(config.hidden_size)\n",
    "        self.linear_1 = nn.Linear(config.hidden_size, config.hidden_size * 4)\n",
    "        self.act = GELUActivation()\n",
    "        self.linear_2 = nn.Linear(config.hidden_size * 4, config.hidden_size)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, hidden_states: Tensor) -> Tensor:\n",
    "        hidden_states = self.ln(hidden_states)\n",
    "        hidden_states = self.linear_1(hidden_states)\n",
    "        hidden_states = self.act(hidden_states)\n",
    "        hidden_states = self.linear_2(hidden_states)\n",
    "        return self.dropout(hidden_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-20T16:50:15.784687Z",
     "iopub.status.busy": "2023-03-20T16:50:15.784513Z",
     "iopub.status.idle": "2023-03-20T16:50:15.790132Z",
     "shell.execute_reply": "2023-03-20T16:50:15.789501Z",
     "shell.execute_reply.started": "2023-03-20T16:50:15.784668Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: hide\n",
    "def init_weights(init_fn) -> None:\n",
    "    def inner_fn(module: nn.Module) -> None:\n",
    "        if isinstance(module, nn.Linear):\n",
    "            fan_out, fan_in = module.weight.shape  # TODO: correct base impl\n",
    "            module.weight.data.normal_(mean=0.0, std=init_fn(fan_in, fan_out))\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            module.weight.data.normal_(mean=0.0, std=1)\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "    return inner_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-20T16:50:15.791132Z",
     "iopub.status.busy": "2023-03-20T16:50:15.790923Z",
     "iopub.status.idle": "2023-03-20T16:50:15.996596Z",
     "shell.execute_reply": "2023-03-20T16:50:15.995892Z",
     "shell.execute_reply.started": "2023-03-20T16:50:15.791109Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: hide\n",
    "import pandas as pd\n",
    "\n",
    "np.seterr(divide = 'ignore')\n",
    "\n",
    "def instrument(module):\n",
    "    stats = {}\n",
    "    instrument_recursive(module, stats)\n",
    "    return stats\n",
    "\n",
    "def instrument_recursive(module, stats, name=''):\n",
    "    children = list(module.named_children())\n",
    "    if children:\n",
    "        for c_name, c in children:\n",
    "            _name = f'{name}.{c_name}' if name and name != 'blocks' else c_name\n",
    "            instrument_recursive(c, stats, _name)\n",
    "    else:\n",
    "        instrument_terminal(module, stats, name)\n",
    "\n",
    "def instrument_terminal(module, stats, name):\n",
    "    module_stats = {}\n",
    "    def require_input_grads(_module, input):\n",
    "        for i in input:\n",
    "            if isinstance(i, Tensor) and i.is_floating_point():\n",
    "                i.requires_grad_()\n",
    "    \n",
    "    module.register_forward_pre_hook(require_input_grads)\n",
    "    \n",
    "    if name.split('.')[-1] == 'softmax':\n",
    "        return\n",
    "    \n",
    "    def record_fwd_scale(_module, input, output):\n",
    "        if isinstance(output, Tensor) and output.is_floating_point():\n",
    "            module_stats['x'] = np.log2(output.std().item())\n",
    "    \n",
    "    module.register_forward_hook(record_fwd_scale)\n",
    "    \n",
    "    def record_bwd_scales(_module, grad_input, grad_output):\n",
    "        grad_input = list(grad_input)\n",
    "        for g in grad_input:\n",
    "            if g is not None and isinstance(g, Tensor) \\\n",
    "                and g.is_floating_point() and len(grad_input) == 1:\n",
    "                module_stats['grad_x'] = np.log2(g.std().item())\n",
    "        \n",
    "        for param_name, param in _module.named_parameters():\n",
    "            if param_name == \"weight\":\n",
    "                module_stats['w'] = np.log2(param.std().item())\n",
    "                if param.grad is not None:\n",
    "                    module_stats['grad_w'] = np.log2(param.grad.std().item())\n",
    "    \n",
    "    module.register_full_backward_hook(record_bwd_scales)\n",
    "    \n",
    "    stats[name] = module_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-20T16:50:15.998689Z",
     "iopub.status.busy": "2023-03-20T16:50:15.998397Z",
     "iopub.status.idle": "2023-03-20T16:50:16.166139Z",
     "shell.execute_reply": "2023-03-20T16:50:16.165419Z",
     "shell.execute_reply.started": "2023-03-20T16:50:15.998667Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: hide\n",
    "import altair as alt\n",
    "\n",
    "def visualise(stats, subnormal=False):\n",
    "    df = pd.DataFrame(stats)\n",
    "    df = df.stack().to_frame('scale (log₂)').reset_index(names=['type', 'op'])\n",
    "    plot(df, subnormal)\n",
    "\n",
    "def plot(df, subnormal=False):\n",
    "    is_x_or_grad_x = (df['type'] == 'x') | (df['type'] == 'grad_x')\n",
    "    op_order = df[df['type'] == 'x']['op'].tolist()\n",
    "    colors = ['#6C8EBF', '#FF8000', '#5D8944', '#ED3434']\n",
    "    x_range = np.arange(-18 if subnormal else -14, 18+1 if subnormal else 16+1, 2)\n",
    "    \n",
    "    fp16_min = alt.Chart().mark_rule(strokeDash=(4, 4)).encode(x=alt.datum(-14))\n",
    "    fp16_min_text = alt.Chart().mark_text(dy=-740).encode(text=alt.Text(value='Min FP16 (normal)'), x=alt.datum(-10))\n",
    "    fp16_max = alt.Chart().mark_rule(strokeDash=(4, 4)).encode(x=alt.datum(16))\n",
    "    fp16_max_text = alt.Chart().mark_text(dy=-740).encode(text=alt.Text(value='Max FP16'), x=alt.datum(13))\n",
    "    \n",
    "    x_chart = alt.Chart(df[is_x_or_grad_x]).mark_line().encode(\n",
    "        x=alt.X(\n",
    "            'scale (log₂):Q',\n",
    "            axis=alt.Axis(orient='top', values=x_range),\n",
    "            scale=alt.Scale(domain=[x_range[0], x_range[-1]]),\n",
    "        ),\n",
    "        y=alt.Y('op:O', title='', sort=op_order),\n",
    "        color=alt.Color(\n",
    "            'type',\n",
    "            legend=alt.Legend(title='', labelFontSize=12, symbolSize=100),\n",
    "            scale=alt.Scale(range=colors[:2]),\n",
    "            sort='descending'\n",
    "        ),\n",
    "    )\n",
    "    w_chart = alt.Chart(df[~is_x_or_grad_x]).mark_point(size=100).encode(\n",
    "        x=alt.X(\n",
    "            'scale (log₂):Q',\n",
    "            axis=alt.Axis(orient='top', values=x_range),\n",
    "            scale=alt.Scale(domain=[x_range[0], x_range[-1]])\n",
    "        ),\n",
    "        y=alt.Y('op:O', title='', sort=op_order),\n",
    "        color=alt.Color(\n",
    "            'type',\n",
    "            legend=alt.Legend(title='', labelFontSize=12, symbolSize=100),\n",
    "            scale=alt.Scale(range=colors[2:]),\n",
    "            sort='descending'\n",
    "        ),\n",
    "        shape=alt.Shape(\n",
    "            'type',\n",
    "            scale=alt.Scale(range=['square', 'triangle-down']),\n",
    "            sort='descending'\n",
    "        ),\n",
    "    )\n",
    "    layers = [x_chart, w_chart]\n",
    "    if subnormal:\n",
    "        layers += [fp16_min, fp16_max, fp16_min_text, fp16_max_text] \n",
    "    combined_chart = alt.layer(\n",
    "        *layers\n",
    "    ).resolve_scale(\n",
    "        color='independent', shape='independent'\n",
    "    ).configure_axis(\n",
    "        labelFontSize=12,\n",
    "        titleFontSize=16\n",
    "    ).properties(\n",
    "        width=500\n",
    "    )\n",
    "    display(combined_chart)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can analyse the scale (i.e. standard deviation) of each operation within the MLP. To do this, we provide an `instrument()` operation, which goes through and tracks the scale coming out of each operation in the forward and backward pass.\n",
    "\n",
    "Our network's weights will also use the standard glorot initialisation.\n",
    "\n",
    "Let's feed in a unit normal tensor in both directions, and examine the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-20T16:50:16.168778Z",
     "iopub.status.busy": "2023-03-20T16:50:16.168504Z",
     "iopub.status.idle": "2023-03-20T16:50:16.173161Z",
     "shell.execute_reply": "2023-03-20T16:50:16.172445Z",
     "shell.execute_reply.started": "2023-03-20T16:50:16.168758Z"
    }
   },
   "outputs": [],
   "source": [
    "def analyse_mlp(mlp, config, batch_size=64, seq_len=16):\n",
    "    stats = instrument(mlp)\n",
    "    x = torch.normal(0.0, 1.0, size=(batch_size, seq_len, config.hidden_size))\n",
    "    y = mlp(x)\n",
    "    y.backward(torch.normal(0.0, 1.0, size=y.shape))\n",
    "    visualise(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-20T16:50:16.174835Z",
     "iopub.status.busy": "2023-03-20T16:50:16.174632Z",
     "iopub.status.idle": "2023-03-20T16:50:16.609266Z",
     "shell.execute_reply": "2023-03-20T16:50:16.608348Z",
     "shell.execute_reply.started": "2023-03-20T16:50:16.174816Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-ced282a3c00145ea9a912feb24a4062a\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-ced282a3c00145ea9a912feb24a4062a\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-ced282a3c00145ea9a912feb24a4062a\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.17.0?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"4.17.0\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}, \"axis\": {\"labelFontSize\": 12, \"titleFontSize\": 16}}, \"layer\": [{\"data\": {\"name\": \"data-1717f52aa16e5a47a6f3d2e47a5f6bb8\"}, \"mark\": \"line\", \"encoding\": {\"color\": {\"field\": \"type\", \"legend\": {\"labelFontSize\": 12, \"symbolSize\": 100, \"title\": \"\"}, \"scale\": {\"range\": [\"#6C8EBF\", \"#FF8000\"]}, \"sort\": \"descending\", \"type\": \"nominal\"}, \"x\": {\"axis\": {\"orient\": \"top\", \"values\": [-14.0, -12.0, -10.0, -8.0, -6.0, -4.0, -2.0, 0.0, 2.0, 4.0, 6.0, 8.0, 10.0, 12.0, 14.0, 16.0]}, \"field\": \"scale (log\\u2082)\", \"scale\": {\"domain\": [-14.0, 16.0]}, \"type\": \"quantitative\"}, \"y\": {\"field\": \"op\", \"sort\": [\"ln\", \"linear_1\", \"act\", \"linear_2\", \"dropout\"], \"title\": \"\", \"type\": \"ordinal\"}}}, {\"data\": {\"name\": \"data-e935a03ccd14661c74c881670507a11e\"}, \"mark\": {\"type\": \"point\", \"size\": 100}, \"encoding\": {\"color\": {\"field\": \"type\", \"legend\": {\"labelFontSize\": 12, \"symbolSize\": 100, \"title\": \"\"}, \"scale\": {\"range\": [\"#5D8944\", \"#ED3434\"]}, \"sort\": \"descending\", \"type\": \"nominal\"}, \"shape\": {\"field\": \"type\", \"scale\": {\"range\": [\"square\", \"triangle-down\"]}, \"sort\": \"descending\", \"type\": \"nominal\"}, \"x\": {\"axis\": {\"orient\": \"top\", \"values\": [-14.0, -12.0, -10.0, -8.0, -6.0, -4.0, -2.0, 0.0, 2.0, 4.0, 6.0, 8.0, 10.0, 12.0, 14.0, 16.0]}, \"field\": \"scale (log\\u2082)\", \"scale\": {\"domain\": [-14.0, 16.0]}, \"type\": \"quantitative\"}, \"y\": {\"field\": \"op\", \"sort\": [\"ln\", \"linear_1\", \"act\", \"linear_2\", \"dropout\"], \"title\": \"\", \"type\": \"ordinal\"}}}], \"resolve\": {\"scale\": {\"color\": \"independent\", \"shape\": \"independent\"}}, \"width\": 500, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.17.0.json\", \"datasets\": {\"data-1717f52aa16e5a47a6f3d2e47a5f6bb8\": [{\"type\": \"x\", \"op\": \"ln\", \"scale (log\\u2082)\": -5.41746367372377e-06}, {\"type\": \"x\", \"op\": \"linear_1\", \"scale (log\\u2082)\": -0.6617055245982282}, {\"type\": \"x\", \"op\": \"act\", \"scale (log\\u2082)\": -1.4867560052567084}, {\"type\": \"x\", \"op\": \"linear_2\", \"scale (log\\u2082)\": -1.0457129743908093}, {\"type\": \"x\", \"op\": \"dropout\", \"scale (log\\u2082)\": -0.9702834312991335}, {\"type\": \"grad_x\", \"op\": \"ln\", \"scale (log\\u2082)\": -0.9186007591961166}, {\"type\": \"grad_x\", \"op\": \"linear_1\", \"scale (log\\u2082)\": -0.9185960453183812}, {\"type\": \"grad_x\", \"op\": \"act\", \"scale (log\\u2082)\": -1.258722200489453}, {\"type\": \"grad_x\", \"op\": \"linear_2\", \"scale (log\\u2082)\": -0.581215354705674}, {\"type\": \"grad_x\", \"op\": \"dropout\", \"scale (log\\u2082)\": 0.07796360719074065}], \"data-e935a03ccd14661c74c881670507a11e\": [{\"type\": \"w\", \"op\": \"ln\", \"scale (log\\u2082)\": null}, {\"type\": \"w\", \"op\": \"linear_1\", \"scale (log\\u2082)\": -4.953225520920785}, {\"type\": \"w\", \"op\": \"linear_2\", \"scale (log\\u2082)\": -4.949914551892651}, {\"type\": \"grad_w\", \"op\": \"ln\", \"scale (log\\u2082)\": 4.174220307429304}, {\"type\": \"grad_w\", \"op\": \"linear_1\", \"scale (log\\u2082)\": 3.739120405815755}, {\"type\": \"grad_w\", \"op\": \"linear_2\", \"scale (log\\u2082)\": 3.681978807123629}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.LayerChart(...)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def glorot_init(fan_in, fan_out):\n",
    "    return ((fan_in + fan_out) / 2) ** -0.5\n",
    "\n",
    "config = NanoGPTConfig()\n",
    "mlp = MLP(config).apply(init_weights(glorot_init))\n",
    "analyse_mlp(mlp, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How to interpret this chart:** The x-axis shows the scale in $\\log_2$ form. The range here represents the minimum and maximum absolute values that can be represented in the FP16 or FP8 E5 number formats (excluding subnormal values). In other words, if the scale exceeds the x-axis bounds we're in the numerics \"danger zone\" where training begins to degrade.\n",
    "\n",
    "The y-axis shows the operations in the model, in the order in which they execute. We show the scale of activations (x), gradients (grad_x, grad_w) and weights (w). Activations can be said to \"flow\" forward through these layers, and grad_xs flow backwards, so we represent these by solid lines, and weights and grad_ws by symbols."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the end of both passes the x and grad_x scales have dropped by half. This is due to glorot slightly under-scaling values, and GeLU dropping the scale further.\n",
    "\n",
    "Worse, the weights are significantly under-scaled and the grad_ws over-scaled by a factor of $2^4$. This is largely because glorot scaling (along with all other weight init schemes) only accounts for the forward and grad_x scales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unit scaling\n",
    "\n",
    "We'll now implement the equivalent layer using unit scaling, beginning with a basic linear operation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-20T16:50:16.610713Z",
     "iopub.status.busy": "2023-03-20T16:50:16.610507Z",
     "iopub.status.idle": "2023-03-20T16:50:16.616716Z",
     "shell.execute_reply": "2023-03-20T16:50:16.616022Z",
     "shell.execute_reply.started": "2023-03-20T16:50:16.610680Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: hide\n",
    "class ScaledGrad(torch.autograd.Function):\n",
    "  @staticmethod\n",
    "  def forward(ctx, X, alpha, beta):\n",
    "    ctx.save_for_backward(\n",
    "      torch.tensor(beta, dtype=X.dtype))\n",
    "    return alpha * X\n",
    "\n",
    "  @staticmethod\n",
    "  def backward(ctx, grad_Y):\n",
    "    beta, = ctx.saved_tensors\n",
    "    return beta * grad_Y, None, None\n",
    "\n",
    "def scaled(X, alpha=1.0, beta=1.0):\n",
    "  # Forward: Y = X * alpha\n",
    "  # Backward: grad_X = grad_Y * beta\n",
    "  return ScaledGrad.apply(X, alpha, beta)\n",
    "\n",
    "def geometric_mean(xs):\n",
    "    xs = np.array(xs)\n",
    "    return xs.prod() ** (1 / xs.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-20T16:50:16.617635Z",
     "iopub.status.busy": "2023-03-20T16:50:16.617460Z",
     "iopub.status.idle": "2023-03-20T16:50:16.625189Z",
     "shell.execute_reply": "2023-03-20T16:50:16.624493Z",
     "shell.execute_reply.started": "2023-03-20T16:50:16.617617Z"
    }
   },
   "outputs": [],
   "source": [
    "class UnitScaledLinear(nn.Linear):\n",
    "    def __init__(self, *args, scale_for=\"fwd, grad_x\", **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.scale_for = scale_for\n",
    "    \n",
    "    def get_scales(self, input):\n",
    "        fwd_scale = self.weight.shape[1] ** -0.5\n",
    "        grad_x_scale = self.weight.shape[0] ** -0.5\n",
    "        grad_w_scale = np.prod(input.shape[:-1]) ** -0.5\n",
    "        if self.scale_for == \"fwd\":\n",
    "            grad_x_scale = fwd_scale\n",
    "        elif self.scale_for == \"grad_x\":\n",
    "            fwd_scale = grad_x_scale\n",
    "        elif self.scale_for == \"fwd, grad_x\":\n",
    "            fwd_scale = grad_x_scale = geometric_mean([fwd_scale, grad_x_scale])\n",
    "        else:\n",
    "            assert self.scale_for == \"separate\", f\"demo implementation has no {self.scale_for} scaling\"\n",
    "        return fwd_scale, grad_x_scale, grad_w_scale\n",
    "           \n",
    "    def forward(self, input):\n",
    "        fwd_scale, grad_x_scale, grad_w_scale = self.get_scales(input)\n",
    "        input = scaled(input, beta=grad_x_scale)\n",
    "        weight = scaled(self.weight, beta=grad_w_scale)\n",
    "        bias = scaled(self.bias, beta=grad_w_scale) if self.bias is not None else None\n",
    "        output = F.linear(input, weight, bias)\n",
    "        return scaled(output, alpha=fwd_scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's break this down a bit. The `forward()` method is still based on the fundamental `F.linear(input, weight, bias)` operation, but has each of its operands and output scaled.\n",
    "\n",
    "This is done via the `scaled(X, alpha, beta)` method. This is a special operation in that it has different dynamics in the forward and backward pass, where we have\n",
    "\n",
    "$$Y = X \\cdot \\alpha$$\n",
    "\n",
    "$$\\nabla_X = \\nabla_Y \\cdot \\beta$$\n",
    "\n",
    "So what scaling factors do we choose? This is determined in `get_scales()`. The standard approach is to set the forward and grad_x scales as a compromise between their \"ideal\" scale-preserving values (via a geometric mean). See our paper for more details on how we arrive at these ideal values.\n",
    "\n",
    "We also provide versions of the operation which select forward and grad_x scales based on only one of their ideal values. We also have a scheme which has separate forward and grad_x scales, though this is only allowed in special circumstances (again, see the paper)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the GeLU. This is pretty similar to the linear op. However, as activation functions are nonlinear, our usual approach of calculating scaling values analytically (i.e. by working through the maths) isn't always possible.\n",
    "\n",
    "Fortunately, for these elemenwise ops we can calulate them empirically, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-20T16:50:16.626271Z",
     "iopub.status.busy": "2023-03-20T16:50:16.626096Z",
     "iopub.status.idle": "2023-03-20T16:50:16.790230Z",
     "shell.execute_reply": "2023-03-20T16:50:16.789417Z",
     "shell.execute_reply.started": "2023-03-20T16:50:16.626253Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GeLU: fwd scale=0.587, bwd scale=0.675\n",
      "Tanh: fwd scale=0.628, bwd scale=0.682\n"
     ]
    }
   ],
   "source": [
    "def analyse_elemenwise_fn(fn, num_samples=2**22):\n",
    "    x = torch.normal(0.0, 1.0, size=(num_samples,)).requires_grad_()\n",
    "    y = fn(x)\n",
    "    y.backward(torch.normal(0.0, 1.0, size=(num_samples,)))\n",
    "    print(f\"fwd scale={y.std():.3f}, bwd scale={x.grad.std():.3f}\")\n",
    "\n",
    "print(\"GeLU:\", end=\" \")\n",
    "analyse_elemenwise_fn(GELUActivation())\n",
    "print(\"Tanh:\", end=\" \")\n",
    "analyse_elemenwise_fn(nn.Tanh())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then put these scaling factors into our unit-scaled op as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-20T16:50:16.791779Z",
     "iopub.status.busy": "2023-03-20T16:50:16.791387Z",
     "iopub.status.idle": "2023-03-20T16:50:16.796115Z",
     "shell.execute_reply": "2023-03-20T16:50:16.795532Z",
     "shell.execute_reply.started": "2023-03-20T16:50:16.791752Z"
    }
   },
   "outputs": [],
   "source": [
    "class UnitScaledGELU(GELUActivation):\n",
    "    def forward(self, input):\n",
    "        fwd_scale = bwd_scale = geometric_mean([0.588, 0.675]) ** -1\n",
    "        input = scaled(input, beta=bwd_scale)\n",
    "        output = self.act(input)\n",
    "        return scaled(output, alpha=fwd_scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple! Now we just need to unit scale the layernorm and dropout and we're ready.\n",
    "\n",
    "These take the same kind of approach, so we won't go into too much detail here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-20T16:50:16.797381Z",
     "iopub.status.busy": "2023-03-20T16:50:16.797015Z",
     "iopub.status.idle": "2023-03-20T16:50:16.803870Z",
     "shell.execute_reply": "2023-03-20T16:50:16.803368Z",
     "shell.execute_reply.started": "2023-03-20T16:50:16.797357Z"
    }
   },
   "outputs": [],
   "source": [
    "class UnitScaledLayerNorm(nn.LayerNorm):\n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        scale = (np.prod(self.normalized_shape) / input.nelement()) ** 0.5\n",
    "        weight = scaled(self.weight, beta=scale)\n",
    "        bias = scaled(self.bias, beta=scale)\n",
    "        return F.layer_norm(input, self.normalized_shape, weight, bias, self.eps)\n",
    "\n",
    "class UnitScaledDropout(nn.Dropout):\n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        # Dropout is typically implemented with a (1-p) ** -1 scaling\n",
    "        # However, to preserve variance this ought to be (1-p) ** -0.5\n",
    "        # We correct for this by multiplying by (1-p) ** 0.5\n",
    "        scale = (1-self.p) ** 0.5\n",
    "        input = scaled(input, beta=scale)\n",
    "        output = F.dropout(input, self.p, self.training, self.inplace)\n",
    "        return scaled(output, alpha=scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're now ready to unit-scale the full MLP 🥳\n",
    "\n",
    "All this requires is swapping out the old layers for our new ones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-20T16:50:16.804806Z",
     "iopub.status.busy": "2023-03-20T16:50:16.804504Z",
     "iopub.status.idle": "2023-03-20T16:50:16.809173Z",
     "shell.execute_reply": "2023-03-20T16:50:16.808547Z",
     "shell.execute_reply.started": "2023-03-20T16:50:16.804787Z"
    }
   },
   "outputs": [],
   "source": [
    "class UnitScaledMLP(MLP):\n",
    "    def __init__(self, config: NanoGPTConfig) -> None:\n",
    "        super().__init__(config)\n",
    "        self.ln = UnitScaledLayerNorm(config.hidden_size)\n",
    "        self.linear_1 = UnitScaledLinear(config.hidden_size, config.hidden_size * 4)\n",
    "        self.act = UnitScaledGELU()\n",
    "        self.linear_2 = UnitScaledLinear(config.hidden_size * 4, config.hidden_size)\n",
    "        self.dropout = UnitScaledDropout(config.dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also change our initialisation to give our weights unit scale. Let's analyse our new unit-scaled MLP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-20T16:50:16.810214Z",
     "iopub.status.busy": "2023-03-20T16:50:16.810035Z",
     "iopub.status.idle": "2023-03-20T16:50:17.038850Z",
     "shell.execute_reply": "2023-03-20T16:50:17.038182Z",
     "shell.execute_reply.started": "2023-03-20T16:50:16.810196Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-825ff9ade64f4ed6aed0390e6d819aed\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-825ff9ade64f4ed6aed0390e6d819aed\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-825ff9ade64f4ed6aed0390e6d819aed\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.17.0?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"4.17.0\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}, \"axis\": {\"labelFontSize\": 12, \"titleFontSize\": 16}}, \"layer\": [{\"data\": {\"name\": \"data-5a3ba922d34b64ba81cbb6729e135da2\"}, \"mark\": \"line\", \"encoding\": {\"color\": {\"field\": \"type\", \"legend\": {\"labelFontSize\": 12, \"symbolSize\": 100, \"title\": \"\"}, \"scale\": {\"range\": [\"#6C8EBF\", \"#FF8000\"]}, \"sort\": \"descending\", \"type\": \"nominal\"}, \"x\": {\"axis\": {\"orient\": \"top\", \"values\": [-14.0, -12.0, -10.0, -8.0, -6.0, -4.0, -2.0, 0.0, 2.0, 4.0, 6.0, 8.0, 10.0, 12.0, 14.0, 16.0]}, \"field\": \"scale (log\\u2082)\", \"scale\": {\"domain\": [-14.0, 16.0]}, \"type\": \"quantitative\"}, \"y\": {\"field\": \"op\", \"sort\": [\"ln\", \"linear_1\", \"act\", \"linear_2\", \"dropout\"], \"title\": \"\", \"type\": \"ordinal\"}}}, {\"data\": {\"name\": \"data-167c38a73ce9e99bed4748ed934a7fef\"}, \"mark\": {\"type\": \"point\", \"size\": 100}, \"encoding\": {\"color\": {\"field\": \"type\", \"legend\": {\"labelFontSize\": 12, \"symbolSize\": 100, \"title\": \"\"}, \"scale\": {\"range\": [\"#5D8944\", \"#ED3434\"]}, \"sort\": \"descending\", \"type\": \"nominal\"}, \"shape\": {\"field\": \"type\", \"scale\": {\"range\": [\"square\", \"triangle-down\"]}, \"sort\": \"descending\", \"type\": \"nominal\"}, \"x\": {\"axis\": {\"orient\": \"top\", \"values\": [-14.0, -12.0, -10.0, -8.0, -6.0, -4.0, -2.0, 0.0, 2.0, 4.0, 6.0, 8.0, 10.0, 12.0, 14.0, 16.0]}, \"field\": \"scale (log\\u2082)\", \"scale\": {\"domain\": [-14.0, 16.0]}, \"type\": \"quantitative\"}, \"y\": {\"field\": \"op\", \"sort\": [\"ln\", \"linear_1\", \"act\", \"linear_2\", \"dropout\"], \"title\": \"\", \"type\": \"ordinal\"}}}], \"resolve\": {\"scale\": {\"color\": \"independent\", \"shape\": \"independent\"}}, \"width\": 500, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.17.0.json\", \"datasets\": {\"data-5a3ba922d34b64ba81cbb6729e135da2\": [{\"type\": \"x\", \"op\": \"ln\", \"scale (log\\u2082)\": -5.41746367372377e-06}, {\"type\": \"x\", \"op\": \"linear_1\", \"scale (log\\u2082)\": -0.49901835718580956}, {\"type\": \"x\", \"op\": \"act\", \"scale (log\\u2082)\": -0.6401562224385232}, {\"type\": \"x\", \"op\": \"linear_2\", \"scale (log\\u2082)\": -0.03385218156472721}, {\"type\": \"x\", \"op\": \"dropout\", \"scale (log\\u2082)\": -0.034822718884220796}, {\"type\": \"grad_x\", \"op\": \"ln\", \"scale (log\\u2082)\": 0.020356044962510422}, {\"type\": \"grad_x\", \"op\": \"linear_1\", \"scale (log\\u2082)\": 0.02352341217455777}, {\"type\": \"grad_x\", \"op\": \"act\", \"scale (log\\u2082)\": -0.4799838645374737}, {\"type\": \"grad_x\", \"op\": \"linear_2\", \"scale (log\\u2082)\": -0.4981792116609853}, {\"type\": \"grad_x\", \"op\": \"dropout\", \"scale (log\\u2082)\": 0.002163123162508484}], \"data-167c38a73ce9e99bed4748ed934a7fef\": [{\"type\": \"w\", \"op\": \"ln\", \"scale (log\\u2082)\": null}, {\"type\": \"w\", \"op\": \"linear_1\", \"scale (log\\u2082)\": 0.00024075562155208443}, {\"type\": \"w\", \"op\": \"linear_2\", \"scale (log\\u2082)\": -0.001430056341375263}, {\"type\": \"grad_w\", \"op\": \"ln\", \"scale (log\\u2082)\": 0.07443500440070501}, {\"type\": \"grad_w\", \"op\": \"linear_1\", \"scale (log\\u2082)\": -0.4796446096858125}, {\"type\": \"grad_w\", \"op\": \"linear_2\", \"scale (log\\u2082)\": -0.523225618698185}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.LayerChart(...)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def unit_init(*args):\n",
    "    return 1\n",
    "\n",
    "mlp = UnitScaledMLP(config).apply(init_weights(unit_init))\n",
    "analyse_mlp(mlp, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is much better! The final scales are almost exactly 1 in both directions, and weights and grad_ws look much better.\n",
    "\n",
    "The compromise fwd & grad_x scaling factors in our linear layers do give temporary non-unit scaling, but this is minor and the second linear layer cancels out the first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The self-attention layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regular scaling\n",
    "\n",
    "We start with a basic implementation of a (self-)attention module.\n",
    "\n",
    "(Note that we use [ALiBi](https://arxiv.org/abs/2108.12409) biases over positional embeddings here. This is primarily for simplicity, though these have also been [shown to perform](https://arxiv.org/abs/2210.15424) remarkably well!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-20T16:50:17.039948Z",
     "iopub.status.busy": "2023-03-20T16:50:17.039765Z",
     "iopub.status.idle": "2023-03-20T16:50:17.050083Z",
     "shell.execute_reply": "2023-03-20T16:50:17.049459Z",
     "shell.execute_reply.started": "2023-03-20T16:50:17.039929Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: hide\n",
    "class Matmul(nn.Module):\n",
    "    def forward(self, a: Tensor, b: Tensor, scale: float=1.0) -> Tensor:\n",
    "        return (a @ b) * scale\n",
    "\n",
    "def split_heads(tensor: Tensor, num_heads: int, head_size: int) -> Tensor:\n",
    "    batch_size, seq_len, hidden_size = tensor.shape\n",
    "    tensor = tensor.view(batch_size, seq_len, num_heads, head_size)\n",
    "    return tensor.permute(0, 2, 1, 3)\n",
    "\n",
    "\n",
    "def merge_heads(tensor: Tensor, num_heads: int) -> Tensor:\n",
    "    tensor = tensor.permute(0, 2, 1, 3).contiguous()\n",
    "    batch_size, seq_len, num_heads, head_size = tensor.shape\n",
    "    return tensor.view(batch_size, seq_len, num_heads * head_size)\n",
    "\n",
    "\n",
    "def causal_mask(seq_len: int, num_heads: int) -> Tensor:\n",
    "    causal_mask = torch.tril(torch.ones((seq_len, seq_len), dtype=torch.float16))\n",
    "    causal_mask = causal_mask.view(1, 1, seq_len, seq_len)\n",
    "    alibi_mask = gen_alibi_mask(causal_mask, num_heads)\n",
    "    causal_mask = (1.0 - causal_mask) * -10_000\n",
    "    return alibi_mask + causal_mask\n",
    "\n",
    "\n",
    "# Based on https://github.com/labmlai/annotated_deep_learning_paper_implementations/blob/master/labml_nn/transformers/alibi/__init__.py\n",
    "def gen_alibi_mask(causal_mask: Tensor, num_heads: int) -> Tensor:\n",
    "    distances = causal_mask.to(torch.float32).cumsum(dim=-1)\n",
    "    slopes = gen_slopes(num_heads)\n",
    "    return distances.to(torch.float16) * slopes.view(1, num_heads, 1, 1)\n",
    "\n",
    "\n",
    "def gen_slopes(num_heads: int) -> Tensor:\n",
    "    n = 2 ** math.floor(math.log2(num_heads))\n",
    "    m_0 = 2.0 ** (-8.0 / n)\n",
    "    m = torch.pow(m_0, torch.arange(1, 1 + n))\n",
    "    if n < num_heads:\n",
    "        m_hat_0 = 2.0 ** (-4.0 / n)\n",
    "        m_hat = torch.pow(m_hat_0, torch.arange(1, 1 + 2 * (num_heads - n), 2))\n",
    "        m = torch.cat([m, m_hat])\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-20T16:50:17.051038Z",
     "iopub.status.busy": "2023-03-20T16:50:17.050850Z",
     "iopub.status.idle": "2023-03-20T16:50:17.060039Z",
     "shell.execute_reply": "2023-03-20T16:50:17.059429Z",
     "shell.execute_reply.started": "2023-03-20T16:50:17.051018Z"
    }
   },
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, config: NanoGPTConfig) -> None:\n",
    "        super().__init__()\n",
    "        self.ln = nn.LayerNorm(config.hidden_size)\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.num_heads = config.num_attention_heads\n",
    "        \n",
    "        self.w_qkv = nn.Linear(config.hidden_size, 3 * config.hidden_size)\n",
    "        self.qk_matmul = Matmul()\n",
    "        self.softmax = nn.Softmax(-1)\n",
    "        self.attn_dropout = nn.Dropout(config.dropout)\n",
    "        self.qkv_matmul = Matmul()\n",
    "        self.w_o = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.residual_dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, hidden_states: Tensor, attention_mask: Tensor) -> Tensor:\n",
    "        seq_len = hidden_states.shape[1]\n",
    "        head_size = self.hidden_size // self.num_heads\n",
    "\n",
    "        hidden_states = self.ln(hidden_states)\n",
    "        q_k_v = self.w_qkv(hidden_states)\n",
    "        q, k, v = q_k_v.split(self.hidden_size, dim=-1)\n",
    "        q, k, v = (split_heads(t, self.num_heads, head_size) for t in (q, k, v))\n",
    "\n",
    "        qk = self.qk_matmul(q, k.transpose(-1, -2), scale=1 / head_size ** 0.5).clone()\n",
    "        qk += causal_mask(seq_len, self.num_heads) + attention_mask\n",
    "        qk = self.softmax(qk)\n",
    "        qk = self.attn_dropout(qk)\n",
    "\n",
    "        qkv = self.qkv_matmul(qk, v)\n",
    "        qkv = merge_heads(qkv, self.num_heads)\n",
    "        qkvo = self.w_o(qkv)\n",
    "        return self.residual_dropout(qkvo).clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-20T16:50:17.060965Z",
     "iopub.status.busy": "2023-03-20T16:50:17.060785Z",
     "iopub.status.idle": "2023-03-20T16:50:17.065387Z",
     "shell.execute_reply": "2023-03-20T16:50:17.064755Z",
     "shell.execute_reply.started": "2023-03-20T16:50:17.060946Z"
    }
   },
   "outputs": [],
   "source": [
    "def analyse_attn(attention, config, batch_size=64, seq_len=16):\n",
    "    stats = instrument(attention)\n",
    "    x = torch.normal(0.0, 1.0, size=(batch_size, seq_len, config.hidden_size))\n",
    "    attention_mask = torch.zeros(batch_size, 1, 1, seq_len)\n",
    "    y = attention(x, attention_mask)\n",
    "    y.backward(torch.normal(0.0, 1.0, size=y.shape))\n",
    "    visualise(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-20T16:50:17.066370Z",
     "iopub.status.busy": "2023-03-20T16:50:17.066186Z",
     "iopub.status.idle": "2023-03-20T16:50:17.273683Z",
     "shell.execute_reply": "2023-03-20T16:50:17.273050Z",
     "shell.execute_reply.started": "2023-03-20T16:50:17.066351Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-3cdaa088140746a29282894cf849f87a\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-3cdaa088140746a29282894cf849f87a\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-3cdaa088140746a29282894cf849f87a\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.17.0?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"4.17.0\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}, \"axis\": {\"labelFontSize\": 12, \"titleFontSize\": 16}}, \"layer\": [{\"data\": {\"name\": \"data-059dac9ebdd4bac2bacd137af2b868ef\"}, \"mark\": \"line\", \"encoding\": {\"color\": {\"field\": \"type\", \"legend\": {\"labelFontSize\": 12, \"symbolSize\": 100, \"title\": \"\"}, \"scale\": {\"range\": [\"#6C8EBF\", \"#FF8000\"]}, \"sort\": \"descending\", \"type\": \"nominal\"}, \"x\": {\"axis\": {\"orient\": \"top\", \"values\": [-14.0, -12.0, -10.0, -8.0, -6.0, -4.0, -2.0, 0.0, 2.0, 4.0, 6.0, 8.0, 10.0, 12.0, 14.0, 16.0]}, \"field\": \"scale (log\\u2082)\", \"scale\": {\"domain\": [-14.0, 16.0]}, \"type\": \"quantitative\"}, \"y\": {\"field\": \"op\", \"sort\": [\"ln\", \"w_qkv\", \"qk_matmul\", \"attn_dropout\", \"qkv_matmul\", \"w_o\", \"residual_dropout\"], \"title\": \"\", \"type\": \"ordinal\"}}}, {\"data\": {\"name\": \"data-cd3eefd10fb144a33d7d0056c533eb25\"}, \"mark\": {\"type\": \"point\", \"size\": 100}, \"encoding\": {\"color\": {\"field\": \"type\", \"legend\": {\"labelFontSize\": 12, \"symbolSize\": 100, \"title\": \"\"}, \"scale\": {\"range\": [\"#5D8944\", \"#ED3434\"]}, \"sort\": \"descending\", \"type\": \"nominal\"}, \"shape\": {\"field\": \"type\", \"scale\": {\"range\": [\"square\", \"triangle-down\"]}, \"sort\": \"descending\", \"type\": \"nominal\"}, \"x\": {\"axis\": {\"orient\": \"top\", \"values\": [-14.0, -12.0, -10.0, -8.0, -6.0, -4.0, -2.0, 0.0, 2.0, 4.0, 6.0, 8.0, 10.0, 12.0, 14.0, 16.0]}, \"field\": \"scale (log\\u2082)\", \"scale\": {\"domain\": [-14.0, 16.0]}, \"type\": \"quantitative\"}, \"y\": {\"field\": \"op\", \"sort\": [\"ln\", \"w_qkv\", \"qk_matmul\", \"attn_dropout\", \"qkv_matmul\", \"w_o\", \"residual_dropout\"], \"title\": \"\", \"type\": \"ordinal\"}}}], \"resolve\": {\"scale\": {\"color\": \"independent\", \"shape\": \"independent\"}}, \"width\": 500, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.17.0.json\", \"datasets\": {\"data-059dac9ebdd4bac2bacd137af2b868ef\": [{\"type\": \"x\", \"op\": \"ln\", \"scale (log\\u2082)\": -5.41746367372377e-06}, {\"type\": \"x\", \"op\": \"w_qkv\", \"scale (log\\u2082)\": -0.5013563714774321}, {\"type\": \"x\", \"op\": \"qk_matmul\", \"scale (log\\u2082)\": -1.0099524029086426}, {\"type\": \"x\", \"op\": \"attn_dropout\", \"scale (log\\u2082)\": -3.0483967207828053}, {\"type\": \"x\", \"op\": \"qkv_matmul\", \"scale (log\\u2082)\": -1.3813607734372517}, {\"type\": \"x\", \"op\": \"w_o\", \"scale (log\\u2082)\": -1.3758932002278421}, {\"type\": \"x\", \"op\": \"residual_dropout\", \"scale (log\\u2082)\": -1.2992003802951975}, {\"type\": \"grad_x\", \"op\": \"ln\", \"scale (log\\u2082)\": -1.144099573000794}, {\"type\": \"grad_x\", \"op\": \"w_qkv\", \"scale (log\\u2082)\": -1.145576239969371}, {\"type\": \"grad_x\", \"op\": \"attn_dropout\", \"scale (log\\u2082)\": 2.6517647175907166}, {\"type\": \"grad_x\", \"op\": \"w_o\", \"scale (log\\u2082)\": 0.08001497026072119}, {\"type\": \"grad_x\", \"op\": \"residual_dropout\", \"scale (log\\u2082)\": 0.0750999485223603}], \"data-cd3eefd10fb144a33d7d0056c533eb25\": [{\"type\": \"w\", \"op\": \"ln\", \"scale (log\\u2082)\": null}, {\"type\": \"w\", \"op\": \"w_qkv\", \"scale (log\\u2082)\": -4.792115224865242}, {\"type\": \"w\", \"op\": \"w_o\", \"scale (log\\u2082)\": -4.287363958071028}, {\"type\": \"grad_w\", \"op\": \"ln\", \"scale (log\\u2082)\": 3.848107983962023}, {\"type\": \"grad_w\", \"op\": \"w_qkv\", \"scale (log\\u2082)\": 3.563807930634326}, {\"type\": \"grad_w\", \"op\": \"w_o\", \"scale (log\\u2082)\": 3.6961164294059254}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.LayerChart(...)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "attention = Attention(config).apply(init_weights(glorot_init))\n",
    "analyse_attn(attention, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again here, the activation scales and grad_x scales fall by half as they go through the layer, and fluctuate in between (Note that we omit the softmax operation here as a) its output isn't normally distributed, and b) we usually do it in higher-precision anyway). We see the same problems as before for weights and grad_ws.\n",
    "\n",
    "Let's fix this:\n",
    "\n",
    "#### Unit scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-20T16:50:17.274901Z",
     "iopub.status.busy": "2023-03-20T16:50:17.274690Z",
     "iopub.status.idle": "2023-03-20T16:50:17.285581Z",
     "shell.execute_reply": "2023-03-20T16:50:17.285014Z",
     "shell.execute_reply.started": "2023-03-20T16:50:17.274880Z"
    }
   },
   "outputs": [],
   "source": [
    "class UnitScaledAttention(Attention):\n",
    "    def __init__(self, config: NanoGPTConfig) -> None:\n",
    "        super().__init__(config)\n",
    "        self.ln = UnitScaledLayerNorm(config.hidden_size)\n",
    "        self.w_qkv = UnitScaledLinear(\n",
    "            config.hidden_size, 3 * config.hidden_size, scale_for=\"fwd\"\n",
    "        )\n",
    "        self.qk_matmul = UnitScaledMatmul(scale_for=\"fwd\")\n",
    "        self.softmax = UnitScaledSoftmax(-1)\n",
    "        self.attn_dropout = UnitScaledDropout(config.dropout)\n",
    "        self.qkv_matmul = UnitScaledMatmul(scale_for=\"fwd\")\n",
    "        self.w_o = UnitScaledLinear(config.hidden_size, config.hidden_size)\n",
    "        self.residual_dropout = UnitScaledDropout(config.dropout)\n",
    "\n",
    "class UnitScaledMatmul(Matmul):\n",
    "    def __init__(self, scale_for=\"fwd, grad_a, grad_b\") -> None:\n",
    "        super().__init__()\n",
    "        self.scale_for = scale_for\n",
    "    \n",
    "    def get_scales(self, a: Tensor, b: Tensor):\n",
    "        fwd_scale = a.shape[-1] ** -0.5\n",
    "        grad_a_scale = b.shape[-1] ** -0.5\n",
    "        grad_b_scale = a.shape[-2] ** -0.5\n",
    "        if self.scale_for == \"fwd\":\n",
    "            grad_a_scale = grad_b_scale = fwd_scale\n",
    "        elif self.scale_for == \"fwd, grad_a, grad_b\":\n",
    "            fwd_scale = grad_a_scale = grad_b_scale = geometric_mean([\n",
    "                fwd_scale, grad_a_scale, grad_b_scale\n",
    "            ])\n",
    "        else:\n",
    "            assert False, f\"demo implementation has no {self.scale_for} scaling\"\n",
    "        return fwd_scale, grad_a_scale, grad_b_scale\n",
    "    \n",
    "    def forward(self, a: Tensor, b: Tensor, scale: float=1.0) -> Tensor:\n",
    "        # ignores provided scale\n",
    "        fwd_scale, grad_a_scale, grad_b_scale = self.get_scales(a, b)\n",
    "        a = scaled(a, beta=grad_a_scale)\n",
    "        b = scaled(b, beta=grad_b_scale)\n",
    "        output = a @ b\n",
    "        return scaled(output, alpha=fwd_scale)\n",
    "\n",
    "class UnitScaledSoftmax(nn.Softmax):\n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        fwd_scale = bwd_scale = input.shape[-1] ** 0.5\n",
    "        input = scaled(input, fwd_scale)\n",
    "        output = F.softmax(input, self.dim, _stacklevel=5)\n",
    "        return scaled(output, bwd_scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To unit scale the attention layer we again swap out regular layers for unit-scaled ones. This requires altering two more operations: matmul and softmax.\n",
    "\n",
    "Like the linear layer, we provide scaling the matmul based on varying criteria. We find that scaling each matmul and linear layer here for the forward pass ensures good scaling in both directions. We see this empirically in our analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-20T16:50:17.286549Z",
     "iopub.status.busy": "2023-03-20T16:50:17.286366Z",
     "iopub.status.idle": "2023-03-20T16:50:17.615443Z",
     "shell.execute_reply": "2023-03-20T16:50:17.614655Z",
     "shell.execute_reply.started": "2023-03-20T16:50:17.286530Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-dfd32495b30448cfb596d87d1ee44575\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-dfd32495b30448cfb596d87d1ee44575\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-dfd32495b30448cfb596d87d1ee44575\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.17.0?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"4.17.0\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}, \"axis\": {\"labelFontSize\": 12, \"titleFontSize\": 16}}, \"layer\": [{\"data\": {\"name\": \"data-06b40ffe9788aa22c356f8cc967be539\"}, \"mark\": \"line\", \"encoding\": {\"color\": {\"field\": \"type\", \"legend\": {\"labelFontSize\": 12, \"symbolSize\": 100, \"title\": \"\"}, \"scale\": {\"range\": [\"#6C8EBF\", \"#FF8000\"]}, \"sort\": \"descending\", \"type\": \"nominal\"}, \"x\": {\"axis\": {\"orient\": \"top\", \"values\": [-14.0, -12.0, -10.0, -8.0, -6.0, -4.0, -2.0, 0.0, 2.0, 4.0, 6.0, 8.0, 10.0, 12.0, 14.0, 16.0]}, \"field\": \"scale (log\\u2082)\", \"scale\": {\"domain\": [-14.0, 16.0]}, \"type\": \"quantitative\"}, \"y\": {\"field\": \"op\", \"sort\": [\"ln\", \"w_qkv\", \"qk_matmul\", \"attn_dropout\", \"qkv_matmul\", \"w_o\", \"residual_dropout\"], \"title\": \"\", \"type\": \"ordinal\"}}}, {\"data\": {\"name\": \"data-aede1f489f2a178a8d83c77516a8e9e3\"}, \"mark\": {\"type\": \"point\", \"size\": 100}, \"encoding\": {\"color\": {\"field\": \"type\", \"legend\": {\"labelFontSize\": 12, \"symbolSize\": 100, \"title\": \"\"}, \"scale\": {\"range\": [\"#5D8944\", \"#ED3434\"]}, \"sort\": \"descending\", \"type\": \"nominal\"}, \"shape\": {\"field\": \"type\", \"scale\": {\"range\": [\"square\", \"triangle-down\"]}, \"sort\": \"descending\", \"type\": \"nominal\"}, \"x\": {\"axis\": {\"orient\": \"top\", \"values\": [-14.0, -12.0, -10.0, -8.0, -6.0, -4.0, -2.0, 0.0, 2.0, 4.0, 6.0, 8.0, 10.0, 12.0, 14.0, 16.0]}, \"field\": \"scale (log\\u2082)\", \"scale\": {\"domain\": [-14.0, 16.0]}, \"type\": \"quantitative\"}, \"y\": {\"field\": \"op\", \"sort\": [\"ln\", \"w_qkv\", \"qk_matmul\", \"attn_dropout\", \"qkv_matmul\", \"w_o\", \"residual_dropout\"], \"title\": \"\", \"type\": \"ordinal\"}}}], \"resolve\": {\"scale\": {\"color\": \"independent\", \"shape\": \"independent\"}}, \"width\": 500, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.17.0.json\", \"datasets\": {\"data-06b40ffe9788aa22c356f8cc967be539\": [{\"type\": \"x\", \"op\": \"ln\", \"scale (log\\u2082)\": -6.793330703198998e-06}, {\"type\": \"x\", \"op\": \"w_qkv\", \"scale (log\\u2082)\": 0.0017737700991355188}, {\"type\": \"x\", \"op\": \"qk_matmul\", \"scale (log\\u2082)\": 0.005369440901645975}, {\"type\": \"x\", \"op\": \"attn_dropout\", \"scale (log\\u2082)\": -0.16431944675309343}, {\"type\": \"x\", \"op\": \"qkv_matmul\", \"scale (log\\u2082)\": -0.1508937924207966}, {\"type\": \"x\", \"op\": \"w_o\", \"scale (log\\u2082)\": -0.14552050000533104}, {\"type\": \"x\", \"op\": \"residual_dropout\", \"scale (log\\u2082)\": -0.14612757283163497}, {\"type\": \"grad_x\", \"op\": \"ln\", \"scale (log\\u2082)\": -0.14706527490284663}, {\"type\": \"grad_x\", \"op\": \"w_qkv\", \"scale (log\\u2082)\": -0.14874039552110804}, {\"type\": \"grad_x\", \"op\": \"attn_dropout\", \"scale (log\\u2082)\": -0.0012721144157514712}, {\"type\": \"grad_x\", \"op\": \"w_o\", \"scale (log\\u2082)\": 0.00017988263799014346}, {\"type\": \"grad_x\", \"op\": \"residual_dropout\", \"scale (log\\u2082)\": -0.001245433835795044}], \"data-aede1f489f2a178a8d83c77516a8e9e3\": [{\"type\": \"w\", \"op\": \"ln\", \"scale (log\\u2082)\": null}, {\"type\": \"w\", \"op\": \"w_qkv\", \"scale (log\\u2082)\": 0.001574158004018782}, {\"type\": \"w\", \"op\": \"w_o\", \"scale (log\\u2082)\": 0.0032324017059841103}, {\"type\": \"grad_w\", \"op\": \"ln\", \"scale (log\\u2082)\": -0.17537623788853096}, {\"type\": \"grad_w\", \"op\": \"w_qkv\", \"scale (log\\u2082)\": -0.941982174834324}, {\"type\": \"grad_w\", \"op\": \"w_o\", \"scale (log\\u2082)\": -0.14569229253768928}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.LayerChart(...)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "attention = UnitScaledAttention(config).apply(init_weights(unit_init))\n",
    "analyse_attn(attention, config, batch_size=64, seq_len=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, a great improvement for all four types of tensors, with each having approximate unit scale. Note that these scaling rules are robust to changes in hyperparameters too. You can experiment with the initial config to verify this.\n",
    "\n",
    "We now have our key building-blocks 🧱 Let's put it all together!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The transformer block\n",
    "\n",
    "#### Regular scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-20T16:50:17.619131Z",
     "iopub.status.busy": "2023-03-20T16:50:17.618953Z",
     "iopub.status.idle": "2023-03-20T16:50:17.625020Z",
     "shell.execute_reply": "2023-03-20T16:50:17.624375Z",
     "shell.execute_reply.started": "2023-03-20T16:50:17.619112Z"
    }
   },
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, config: NanoGPTConfig) -> None:\n",
    "        super().__init__()\n",
    "        self.attention_layer = Residual(Attention(config))\n",
    "        self.mlp_layer = Residual(MLP(config))\n",
    "        \n",
    "    def forward(self, hidden_states: Tensor, attention_mask: Tensor) -> Tensor:\n",
    "        hidden_states = self.attention_layer(hidden_states, attention_mask)\n",
    "        return self.mlp_layer(hidden_states)\n",
    "\n",
    "\n",
    "class Residual(nn.Module):\n",
    "    def __init__(self, f: nn.Module):\n",
    "        super().__init__()\n",
    "        self.f = f\n",
    "    \n",
    "    def forward(self, x: Tensor, *args):\n",
    "        return x + self.f(x, *args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unit scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-20T16:50:17.626354Z",
     "iopub.status.busy": "2023-03-20T16:50:17.626184Z",
     "iopub.status.idle": "2023-03-20T16:50:17.633035Z",
     "shell.execute_reply": "2023-03-20T16:50:17.632286Z",
     "shell.execute_reply.started": "2023-03-20T16:50:17.626336Z"
    }
   },
   "outputs": [],
   "source": [
    "class UnitScaledTransformerBlock(TransformerBlock):\n",
    "    def __init__(self, config: NanoGPTConfig) -> None:\n",
    "        super().__init__(config)\n",
    "        self.attention_layer = UnitScaledResidual(UnitScaledAttention(config))\n",
    "        self.mlp_layer = UnitScaledResidual(UnitScaledMLP(config))\n",
    "\n",
    "class UnitScaledResidual(Residual):\n",
    "    def __init__(self, f: nn.Module, tau: float=0.2):\n",
    "        super().__init__(f)\n",
    "        self.tau = tau\n",
    "    \n",
    "    def forward(self, x: Tensor, *args):\n",
    "        y = x * (1 - self.tau) ** 0.5\n",
    "        z = scaled(x, beta=self.tau ** 0.5)\n",
    "        z = self.f(z, *args)\n",
    "        z = scaled(z, alpha=self.tau ** 0.5)\n",
    "        return y + z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the scaling in our residual layer includes an important trick.\n",
    "\n",
    "It's necessary for unit-scaled models to use a weighted sum when doing their residual-add, to down-weight the residual/trunk branch. If this is not included, training can fail as too much signal comes from each residual (regular models avoid this as their residual implicitly reduces scale).\n",
    "\n",
    "However, the naïve implementation of this breaks unit scale. We get around this by delaying the weighting until the end of the residual branch in the backward pass (see paper for more details)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The full transformer\n",
    "\n",
    "#### Regular scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that our implementation contains a little extra boilerplate to make it 🤗-compliant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-20T16:50:17.634020Z",
     "iopub.status.busy": "2023-03-20T16:50:17.633844Z",
     "iopub.status.idle": "2023-03-20T16:50:17.645387Z",
     "shell.execute_reply": "2023-03-20T16:50:17.644716Z",
     "shell.execute_reply.started": "2023-03-20T16:50:17.634002Z"
    }
   },
   "outputs": [],
   "source": [
    "class NanoGPTModel(PreTrainedModel):\n",
    "    config_class = NanoGPTConfig\n",
    "\n",
    "    def __init__(self, config: NanoGPTConfig) -> None:\n",
    "        super().__init__(config)\n",
    "        self.input_embeddings = nn.Embedding(config.vocab_size, config.hidden_size)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        self.ln = nn.LayerNorm(config.hidden_size)\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [TransformerBlock(config) for _ in range(config.num_hidden_layers)]\n",
    "        )\n",
    "        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        self.apply(init_weights(glorot_init))\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Tensor,\n",
    "        attention_mask: Tensor,\n",
    "        position_ids: Optional[Tensor] = None,\n",
    "        labels: Optional[Tensor] = None,\n",
    "        **kwargs,\n",
    "    ) -> CausalLMOutputWithCrossAttentions:\n",
    "        attention_mask = attention_mask[:, None, None, :].to(dtype=self.dtype)\n",
    "        attention_mask = (1.0 - attention_mask) * -10_000\n",
    "\n",
    "        hidden_states = self.input_embeddings(input_ids)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        for block in self.blocks:\n",
    "            hidden_states = block(hidden_states, attention_mask=attention_mask)\n",
    "        hidden_states = self.ln(hidden_states)\n",
    "        logits = self.lm_head(hidden_states)\n",
    "\n",
    "        if labels is None:\n",
    "            return CausalLMOutputWithCrossAttentions(logits=logits)\n",
    "        else:\n",
    "            labels = torch.roll(labels, -1, 1)\n",
    "            labels[:, -1] = -100  # By default, ignore_index of CrossEntropyLoss is -100\n",
    "            loss = self.loss_fn(logits.view(-1, logits.shape[-1]), labels.view(-1))\n",
    "            return CausalLMOutputWithCrossAttentions(loss=loss)\n",
    "\n",
    "    def get_input_embeddings(self) -> nn.Embedding:\n",
    "        return self.input_embeddings\n",
    "\n",
    "    def prepare_inputs_for_generation(\n",
    "        self, input_ids: Tensor, **kwargs\n",
    "    ) -> Dict[str, Tensor]:\n",
    "        attention_mask = kwargs[\"attention_mask\"]\n",
    "        position_ids = attention_mask.long().cumsum(-1) - 1\n",
    "        position_ids.masked_fill_(attention_mask == 0, 1)\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"position_ids\": position_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-20T16:50:17.646397Z",
     "iopub.status.busy": "2023-03-20T16:50:17.646207Z",
     "iopub.status.idle": "2023-03-20T16:50:17.651342Z",
     "shell.execute_reply": "2023-03-20T16:50:17.650702Z",
     "shell.execute_reply.started": "2023-03-20T16:50:17.646376Z"
    }
   },
   "outputs": [],
   "source": [
    "def analyse_full_model(model, config, batch_size=64, seq_len=16):\n",
    "    stats = instrument(model)\n",
    "    input_ids = labels = torch.randint(0, config.vocab_size, size=(batch_size, seq_len))\n",
    "    attention_mask = torch.zeros(batch_size, seq_len)\n",
    "    y = model(input_ids, attention_mask, labels=labels).loss\n",
    "    y.backward()\n",
    "    visualise(stats, subnormal=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the scaling for the entire (non-unit-scaled) transformer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-20T16:50:17.652356Z",
     "iopub.status.busy": "2023-03-20T16:50:17.652167Z",
     "iopub.status.idle": "2023-03-20T16:50:18.425149Z",
     "shell.execute_reply": "2023-03-20T16:50:18.424312Z",
     "shell.execute_reply.started": "2023-03-20T16:50:17.652335Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-6603b41bde2f4f8ca8965806e5ce9bd1\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-6603b41bde2f4f8ca8965806e5ce9bd1\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-6603b41bde2f4f8ca8965806e5ce9bd1\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.17.0?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"4.17.0\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}, \"axis\": {\"labelFontSize\": 12, \"titleFontSize\": 16}}, \"layer\": [{\"data\": {\"name\": \"data-8bfd57eb55ef9212c3ca179959246eff\"}, \"mark\": \"line\", \"encoding\": {\"color\": {\"field\": \"type\", \"legend\": {\"labelFontSize\": 12, \"symbolSize\": 100, \"title\": \"\"}, \"scale\": {\"range\": [\"#6C8EBF\", \"#FF8000\"]}, \"sort\": \"descending\", \"type\": \"nominal\"}, \"x\": {\"axis\": {\"orient\": \"top\", \"values\": [-18.0, -16.0, -14.0, -12.0, -10.0, -8.0, -6.0, -4.0, -2.0, 0.0, 2.0, 4.0, 6.0, 8.0, 10.0, 12.0, 14.0, 16.0, 18.0]}, \"field\": \"scale (log\\u2082)\", \"scale\": {\"domain\": [-18.0, 18.0]}, \"type\": \"quantitative\"}, \"y\": {\"field\": \"op\", \"sort\": [\"input_embeddings\", \"dropout\", \"ln\", \"0.attention_layer.f.ln\", \"0.attention_layer.f.w_qkv\", \"0.attention_layer.f.qk_matmul\", \"0.attention_layer.f.attn_dropout\", \"0.attention_layer.f.qkv_matmul\", \"0.attention_layer.f.w_o\", \"0.attention_layer.f.residual_dropout\", \"0.mlp_layer.f.ln\", \"0.mlp_layer.f.linear_1\", \"0.mlp_layer.f.act\", \"0.mlp_layer.f.linear_2\", \"0.mlp_layer.f.dropout\", \"1.attention_layer.f.ln\", \"1.attention_layer.f.w_qkv\", \"1.attention_layer.f.qk_matmul\", \"1.attention_layer.f.attn_dropout\", \"1.attention_layer.f.qkv_matmul\", \"1.attention_layer.f.w_o\", \"1.attention_layer.f.residual_dropout\", \"1.mlp_layer.f.ln\", \"1.mlp_layer.f.linear_1\", \"1.mlp_layer.f.act\", \"1.mlp_layer.f.linear_2\", \"1.mlp_layer.f.dropout\", \"2.attention_layer.f.ln\", \"2.attention_layer.f.w_qkv\", \"2.attention_layer.f.qk_matmul\", \"2.attention_layer.f.attn_dropout\", \"2.attention_layer.f.qkv_matmul\", \"2.attention_layer.f.w_o\", \"2.attention_layer.f.residual_dropout\", \"2.mlp_layer.f.ln\", \"2.mlp_layer.f.linear_1\", \"2.mlp_layer.f.act\", \"2.mlp_layer.f.linear_2\", \"2.mlp_layer.f.dropout\", \"3.attention_layer.f.ln\", \"3.attention_layer.f.w_qkv\", \"3.attention_layer.f.qk_matmul\", \"3.attention_layer.f.attn_dropout\", \"3.attention_layer.f.qkv_matmul\", \"3.attention_layer.f.w_o\", \"3.attention_layer.f.residual_dropout\", \"3.mlp_layer.f.ln\", \"3.mlp_layer.f.linear_1\", \"3.mlp_layer.f.act\", \"3.mlp_layer.f.linear_2\", \"3.mlp_layer.f.dropout\", \"4.attention_layer.f.ln\", \"4.attention_layer.f.w_qkv\", \"4.attention_layer.f.qk_matmul\", \"4.attention_layer.f.attn_dropout\", \"4.attention_layer.f.qkv_matmul\", \"4.attention_layer.f.w_o\", \"4.attention_layer.f.residual_dropout\", \"4.mlp_layer.f.ln\", \"4.mlp_layer.f.linear_1\", \"4.mlp_layer.f.act\", \"4.mlp_layer.f.linear_2\", \"4.mlp_layer.f.dropout\", \"5.attention_layer.f.ln\", \"5.attention_layer.f.w_qkv\", \"5.attention_layer.f.qk_matmul\", \"5.attention_layer.f.attn_dropout\", \"5.attention_layer.f.qkv_matmul\", \"5.attention_layer.f.w_o\", \"5.attention_layer.f.residual_dropout\", \"5.mlp_layer.f.ln\", \"5.mlp_layer.f.linear_1\", \"5.mlp_layer.f.act\", \"5.mlp_layer.f.linear_2\", \"5.mlp_layer.f.dropout\", \"lm_head\"], \"title\": \"\", \"type\": \"ordinal\"}}}, {\"data\": {\"name\": \"data-21f6b40b7003825d25b09fa73cc2691b\"}, \"mark\": {\"type\": \"point\", \"size\": 100}, \"encoding\": {\"color\": {\"field\": \"type\", \"legend\": {\"labelFontSize\": 12, \"symbolSize\": 100, \"title\": \"\"}, \"scale\": {\"range\": [\"#5D8944\", \"#ED3434\"]}, \"sort\": \"descending\", \"type\": \"nominal\"}, \"shape\": {\"field\": \"type\", \"scale\": {\"range\": [\"square\", \"triangle-down\"]}, \"sort\": \"descending\", \"type\": \"nominal\"}, \"x\": {\"axis\": {\"orient\": \"top\", \"values\": [-18.0, -16.0, -14.0, -12.0, -10.0, -8.0, -6.0, -4.0, -2.0, 0.0, 2.0, 4.0, 6.0, 8.0, 10.0, 12.0, 14.0, 16.0, 18.0]}, \"field\": \"scale (log\\u2082)\", \"scale\": {\"domain\": [-18.0, 18.0]}, \"type\": \"quantitative\"}, \"y\": {\"field\": \"op\", \"sort\": [\"input_embeddings\", \"dropout\", \"ln\", \"0.attention_layer.f.ln\", \"0.attention_layer.f.w_qkv\", \"0.attention_layer.f.qk_matmul\", \"0.attention_layer.f.attn_dropout\", \"0.attention_layer.f.qkv_matmul\", \"0.attention_layer.f.w_o\", \"0.attention_layer.f.residual_dropout\", \"0.mlp_layer.f.ln\", \"0.mlp_layer.f.linear_1\", \"0.mlp_layer.f.act\", \"0.mlp_layer.f.linear_2\", \"0.mlp_layer.f.dropout\", \"1.attention_layer.f.ln\", \"1.attention_layer.f.w_qkv\", \"1.attention_layer.f.qk_matmul\", \"1.attention_layer.f.attn_dropout\", \"1.attention_layer.f.qkv_matmul\", \"1.attention_layer.f.w_o\", \"1.attention_layer.f.residual_dropout\", \"1.mlp_layer.f.ln\", \"1.mlp_layer.f.linear_1\", \"1.mlp_layer.f.act\", \"1.mlp_layer.f.linear_2\", \"1.mlp_layer.f.dropout\", \"2.attention_layer.f.ln\", \"2.attention_layer.f.w_qkv\", \"2.attention_layer.f.qk_matmul\", \"2.attention_layer.f.attn_dropout\", \"2.attention_layer.f.qkv_matmul\", \"2.attention_layer.f.w_o\", \"2.attention_layer.f.residual_dropout\", \"2.mlp_layer.f.ln\", \"2.mlp_layer.f.linear_1\", \"2.mlp_layer.f.act\", \"2.mlp_layer.f.linear_2\", \"2.mlp_layer.f.dropout\", \"3.attention_layer.f.ln\", \"3.attention_layer.f.w_qkv\", \"3.attention_layer.f.qk_matmul\", \"3.attention_layer.f.attn_dropout\", \"3.attention_layer.f.qkv_matmul\", \"3.attention_layer.f.w_o\", \"3.attention_layer.f.residual_dropout\", \"3.mlp_layer.f.ln\", \"3.mlp_layer.f.linear_1\", \"3.mlp_layer.f.act\", \"3.mlp_layer.f.linear_2\", \"3.mlp_layer.f.dropout\", \"4.attention_layer.f.ln\", \"4.attention_layer.f.w_qkv\", \"4.attention_layer.f.qk_matmul\", \"4.attention_layer.f.attn_dropout\", \"4.attention_layer.f.qkv_matmul\", \"4.attention_layer.f.w_o\", \"4.attention_layer.f.residual_dropout\", \"4.mlp_layer.f.ln\", \"4.mlp_layer.f.linear_1\", \"4.mlp_layer.f.act\", \"4.mlp_layer.f.linear_2\", \"4.mlp_layer.f.dropout\", \"5.attention_layer.f.ln\", \"5.attention_layer.f.w_qkv\", \"5.attention_layer.f.qk_matmul\", \"5.attention_layer.f.attn_dropout\", \"5.attention_layer.f.qkv_matmul\", \"5.attention_layer.f.w_o\", \"5.attention_layer.f.residual_dropout\", \"5.mlp_layer.f.ln\", \"5.mlp_layer.f.linear_1\", \"5.mlp_layer.f.act\", \"5.mlp_layer.f.linear_2\", \"5.mlp_layer.f.dropout\", \"lm_head\"], \"title\": \"\", \"type\": \"ordinal\"}}}, {\"data\": {\"name\": \"empty\"}, \"mark\": {\"type\": \"rule\", \"strokeDash\": [4, 4]}, \"encoding\": {\"x\": {\"datum\": -14}}}, {\"data\": {\"name\": \"empty\"}, \"mark\": {\"type\": \"rule\", \"strokeDash\": [4, 4]}, \"encoding\": {\"x\": {\"datum\": 16}}}, {\"data\": {\"name\": \"empty\"}, \"mark\": {\"type\": \"text\", \"dy\": -740}, \"encoding\": {\"text\": {\"value\": \"Min FP16 (normal)\"}, \"x\": {\"datum\": -10}}}, {\"data\": {\"name\": \"empty\"}, \"mark\": {\"type\": \"text\", \"dy\": -740}, \"encoding\": {\"text\": {\"value\": \"Max FP16\"}, \"x\": {\"datum\": 13}}}], \"resolve\": {\"scale\": {\"color\": \"independent\", \"shape\": \"independent\"}}, \"width\": 500, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.17.0.json\", \"datasets\": {\"data-8bfd57eb55ef9212c3ca179959246eff\": [{\"type\": \"x\", \"op\": \"input_embeddings\", \"scale (log\\u2082)\": -0.006495940706469861}, {\"type\": \"x\", \"op\": \"dropout\", \"scale (log\\u2082)\": 0.06877942581702431}, {\"type\": \"x\", \"op\": \"ln\", \"scale (log\\u2082)\": 0.0}, {\"type\": \"x\", \"op\": \"0.attention_layer.f.ln\", \"scale (log\\u2082)\": -4.815522260932938e-06}, {\"type\": \"x\", \"op\": \"0.attention_layer.f.w_qkv\", \"scale (log\\u2082)\": -0.49762074033785064}, {\"type\": \"x\", \"op\": \"0.attention_layer.f.qk_matmul\", \"scale (log\\u2082)\": -0.9923726454944578}, {\"type\": \"x\", \"op\": \"0.attention_layer.f.attn_dropout\", \"scale (log\\u2082)\": -3.03691122153932}, {\"type\": \"x\", \"op\": \"0.attention_layer.f.qkv_matmul\", \"scale (log\\u2082)\": -1.3680305548471758}, {\"type\": \"x\", \"op\": \"0.attention_layer.f.w_o\", \"scale (log\\u2082)\": -1.372514652413348}, {\"type\": \"x\", \"op\": \"0.attention_layer.f.residual_dropout\", \"scale (log\\u2082)\": -1.2955268202961976}, {\"type\": \"x\", \"op\": \"0.mlp_layer.f.ln\", \"scale (log\\u2082)\": -3.955606392614896e-06}, {\"type\": \"x\", \"op\": \"0.mlp_layer.f.linear_1\", \"scale (log\\u2082)\": -0.6597635847427934}, {\"type\": \"x\", \"op\": \"0.mlp_layer.f.act\", \"scale (log\\u2082)\": -1.488124436174622}, {\"type\": \"x\", \"op\": \"0.mlp_layer.f.linear_2\", \"scale (log\\u2082)\": -1.0548244619680875}, {\"type\": \"x\", \"op\": \"0.mlp_layer.f.dropout\", \"scale (log\\u2082)\": -0.9786573772346313}, {\"type\": \"x\", \"op\": \"1.attention_layer.f.ln\", \"scale (log\\u2082)\": -3.009699529461491e-06}, {\"type\": \"x\", \"op\": \"1.attention_layer.f.w_qkv\", \"scale (log\\u2082)\": -0.4975241014475378}, {\"type\": \"x\", \"op\": \"1.attention_layer.f.qk_matmul\", \"scale (log\\u2082)\": -1.0026837040956766}, {\"type\": \"x\", \"op\": \"1.attention_layer.f.attn_dropout\", \"scale (log\\u2082)\": -3.054006504346336}, {\"type\": \"x\", \"op\": \"1.attention_layer.f.qkv_matmul\", \"scale (log\\u2082)\": -1.2580266826684026}, {\"type\": \"x\", \"op\": \"1.attention_layer.f.w_o\", \"scale (log\\u2082)\": -1.2461182461620475}, {\"type\": \"x\", \"op\": \"1.attention_layer.f.residual_dropout\", \"scale (log\\u2082)\": -1.1693183634614535}, {\"type\": \"x\", \"op\": \"1.mlp_layer.f.ln\", \"scale (log\\u2082)\": -2.4937505927785606e-06}, {\"type\": \"x\", \"op\": \"1.mlp_layer.f.linear_1\", \"scale (log\\u2082)\": -0.6611258636418815}, {\"type\": \"x\", \"op\": \"1.mlp_layer.f.act\", \"scale (log\\u2082)\": -1.488747537748939}, {\"type\": \"x\", \"op\": \"1.mlp_layer.f.linear_2\", \"scale (log\\u2082)\": -1.0513846668282152}, {\"type\": \"x\", \"op\": \"1.mlp_layer.f.dropout\", \"scale (log\\u2082)\": -0.9752262254976357}, {\"type\": \"x\", \"op\": \"2.attention_layer.f.ln\", \"scale (log\\u2082)\": -1.8918103998587215e-06}, {\"type\": \"x\", \"op\": \"2.attention_layer.f.w_qkv\", \"scale (log\\u2082)\": -0.5015567438953652}, {\"type\": \"x\", \"op\": \"2.attention_layer.f.qk_matmul\", \"scale (log\\u2082)\": -1.0089492132491842}, {\"type\": \"x\", \"op\": \"2.attention_layer.f.attn_dropout\", \"scale (log\\u2082)\": -3.062153337112193}, {\"type\": \"x\", \"op\": \"2.attention_layer.f.qkv_matmul\", \"scale (log\\u2082)\": -1.1780313369227635}, {\"type\": \"x\", \"op\": \"2.attention_layer.f.w_o\", \"scale (log\\u2082)\": -1.1766626951165513}, {\"type\": \"x\", \"op\": \"2.attention_layer.f.residual_dropout\", \"scale (log\\u2082)\": -1.1008784016705861}, {\"type\": \"x\", \"op\": \"2.mlp_layer.f.ln\", \"scale (log\\u2082)\": -1.5478446880940214e-06}, {\"type\": \"x\", \"op\": \"2.mlp_layer.f.linear_1\", \"scale (log\\u2082)\": -0.6612153409108582}, {\"type\": \"x\", \"op\": \"2.mlp_layer.f.act\", \"scale (log\\u2082)\": -1.4849757699974013}, {\"type\": \"x\", \"op\": \"2.mlp_layer.f.linear_2\", \"scale (log\\u2082)\": -1.0637782263710398}, {\"type\": \"x\", \"op\": \"2.mlp_layer.f.dropout\", \"scale (log\\u2082)\": -0.9886123430090537}, {\"type\": \"x\", \"op\": \"3.attention_layer.f.ln\", \"scale (log\\u2082)\": -1.2038790583372158e-06}, {\"type\": \"x\", \"op\": \"3.attention_layer.f.w_qkv\", \"scale (log\\u2082)\": -0.49825560995721757}, {\"type\": \"x\", \"op\": \"3.attention_layer.f.qk_matmul\", \"scale (log\\u2082)\": -1.006448260072955}, {\"type\": \"x\", \"op\": \"3.attention_layer.f.attn_dropout\", \"scale (log\\u2082)\": -3.056578740377313}, {\"type\": \"x\", \"op\": \"3.attention_layer.f.qkv_matmul\", \"scale (log\\u2082)\": -1.1101893384503427}, {\"type\": \"x\", \"op\": \"3.attention_layer.f.w_o\", \"scale (log\\u2082)\": -1.1314078195712616}, {\"type\": \"x\", \"op\": \"3.attention_layer.f.residual_dropout\", \"scale (log\\u2082)\": -1.0540895291133903}, {\"type\": \"x\", \"op\": \"3.mlp_layer.f.ln\", \"scale (log\\u2082)\": -9.459048898372686e-07}, {\"type\": \"x\", \"op\": \"3.mlp_layer.f.linear_1\", \"scale (log\\u2082)\": -0.6637070220061585}, {\"type\": \"x\", \"op\": \"3.mlp_layer.f.act\", \"scale (log\\u2082)\": -1.4771212310379587}, {\"type\": \"x\", \"op\": \"3.mlp_layer.f.linear_2\", \"scale (log\\u2082)\": -1.0581858997041633}, {\"type\": \"x\", \"op\": \"3.mlp_layer.f.dropout\", \"scale (log\\u2082)\": -0.9818179927526028}, {\"type\": \"x\", \"op\": \"4.attention_layer.f.ln\", \"scale (log\\u2082)\": -6.879307674667236e-07}, {\"type\": \"x\", \"op\": \"4.attention_layer.f.w_qkv\", \"scale (log\\u2082)\": -0.501298066626815}, {\"type\": \"x\", \"op\": \"4.attention_layer.f.qk_matmul\", \"scale (log\\u2082)\": -1.018675477997743}, {\"type\": \"x\", \"op\": \"4.attention_layer.f.attn_dropout\", \"scale (log\\u2082)\": -3.0731151606541514}, {\"type\": \"x\", \"op\": \"4.attention_layer.f.qkv_matmul\", \"scale (log\\u2082)\": -1.077434197405243}, {\"type\": \"x\", \"op\": \"4.attention_layer.f.w_o\", \"scale (log\\u2082)\": -1.0713936174888705}, {\"type\": \"x\", \"op\": \"4.attention_layer.f.residual_dropout\", \"scale (log\\u2082)\": -0.9958217184442931}, {\"type\": \"x\", \"op\": \"4.mlp_layer.f.ln\", \"scale (log\\u2082)\": -5.159480448471312e-07}, {\"type\": \"x\", \"op\": \"4.mlp_layer.f.linear_1\", \"scale (log\\u2082)\": -0.6594989709999876}, {\"type\": \"x\", \"op\": \"4.mlp_layer.f.act\", \"scale (log\\u2082)\": -1.48330603483625}, {\"type\": \"x\", \"op\": \"4.mlp_layer.f.linear_2\", \"scale (log\\u2082)\": -1.0437401315978188}, {\"type\": \"x\", \"op\": \"4.mlp_layer.f.dropout\", \"scale (log\\u2082)\": -0.9685702016339428}, {\"type\": \"x\", \"op\": \"5.attention_layer.f.ln\", \"scale (log\\u2082)\": -3.4396534272948306e-07}, {\"type\": \"x\", \"op\": \"5.attention_layer.f.w_qkv\", \"scale (log\\u2082)\": -0.5054350327670077}, {\"type\": \"x\", \"op\": \"5.attention_layer.f.qk_matmul\", \"scale (log\\u2082)\": -1.0121155713449848}, {\"type\": \"x\", \"op\": \"5.attention_layer.f.attn_dropout\", \"scale (log\\u2082)\": -3.0685353986333506}, {\"type\": \"x\", \"op\": \"5.attention_layer.f.qkv_matmul\", \"scale (log\\u2082)\": -1.0550821789624694}, {\"type\": \"x\", \"op\": \"5.attention_layer.f.w_o\", \"scale (log\\u2082)\": -1.0518091568061014}, {\"type\": \"x\", \"op\": \"5.attention_layer.f.residual_dropout\", \"scale (log\\u2082)\": -0.9752020508925469}, {\"type\": \"x\", \"op\": \"5.mlp_layer.f.ln\", \"scale (log\\u2082)\": -1.7198266111377426e-07}, {\"type\": \"x\", \"op\": \"5.mlp_layer.f.linear_1\", \"scale (log\\u2082)\": -0.6556696832640883}, {\"type\": \"x\", \"op\": \"5.mlp_layer.f.act\", \"scale (log\\u2082)\": -1.4839119053324228}, {\"type\": \"x\", \"op\": \"5.mlp_layer.f.linear_2\", \"scale (log\\u2082)\": -1.044648074092714}, {\"type\": \"x\", \"op\": \"5.mlp_layer.f.dropout\", \"scale (log\\u2082)\": -0.968273056036392}, {\"type\": \"x\", \"op\": \"lm_head\", \"scale (log\\u2082)\": 0.005283423773264084}, {\"type\": \"grad_x\", \"op\": \"dropout\", \"scale (log\\u2082)\": -14.157470488018234}, {\"type\": \"grad_x\", \"op\": \"ln\", \"scale (log\\u2082)\": -15.22971378040465}, {\"type\": \"grad_x\", \"op\": \"0.attention_layer.f.ln\", \"scale (log\\u2082)\": -15.360023116096404}, {\"type\": \"grad_x\", \"op\": \"0.attention_layer.f.w_qkv\", \"scale (log\\u2082)\": -15.298254651368373}, {\"type\": \"grad_x\", \"op\": \"0.attention_layer.f.attn_dropout\", \"scale (log\\u2082)\": -11.74069736646994}, {\"type\": \"grad_x\", \"op\": \"0.attention_layer.f.w_o\", \"scale (log\\u2082)\": -14.318736491123275}, {\"type\": \"grad_x\", \"op\": \"0.attention_layer.f.residual_dropout\", \"scale (log\\u2082)\": -14.323872033998837}, {\"type\": \"grad_x\", \"op\": \"0.mlp_layer.f.ln\", \"scale (log\\u2082)\": -15.639779950929093}, {\"type\": \"grad_x\", \"op\": \"0.mlp_layer.f.linear_1\", \"scale (log\\u2082)\": -15.46947292706214}, {\"type\": \"grad_x\", \"op\": \"0.mlp_layer.f.act\", \"scale (log\\u2082)\": -15.810295918819282}, {\"type\": \"grad_x\", \"op\": \"0.mlp_layer.f.linear_2\", \"scale (log\\u2082)\": -15.130485507379476}, {\"type\": \"grad_x\", \"op\": \"0.mlp_layer.f.dropout\", \"scale (log\\u2082)\": -14.466961080905417}, {\"type\": \"grad_x\", \"op\": \"1.attention_layer.f.ln\", \"scale (log\\u2082)\": -16.006651172815392}, {\"type\": \"grad_x\", \"op\": \"1.attention_layer.f.w_qkv\", \"scale (log\\u2082)\": -15.643104128340234}, {\"type\": \"grad_x\", \"op\": \"1.attention_layer.f.attn_dropout\", \"scale (log\\u2082)\": -11.985709258153452}, {\"type\": \"grad_x\", \"op\": \"1.attention_layer.f.w_o\", \"scale (log\\u2082)\": -14.566285569917875}, {\"type\": \"grad_x\", \"op\": \"1.attention_layer.f.residual_dropout\", \"scale (log\\u2082)\": -14.56903069563885}, {\"type\": \"grad_x\", \"op\": \"1.mlp_layer.f.ln\", \"scale (log\\u2082)\": -16.064119803520807}, {\"type\": \"grad_x\", \"op\": \"1.mlp_layer.f.linear_1\", \"scale (log\\u2082)\": -15.682760194400608}, {\"type\": \"grad_x\", \"op\": \"1.mlp_layer.f.act\", \"scale (log\\u2082)\": -16.02410079129574}, {\"type\": \"grad_x\", \"op\": \"1.mlp_layer.f.linear_2\", \"scale (log\\u2082)\": -15.341797453562533}, {\"type\": \"grad_x\", \"op\": \"1.mlp_layer.f.dropout\", \"scale (log\\u2082)\": -14.67881079417662}, {\"type\": \"grad_x\", \"op\": \"2.attention_layer.f.ln\", \"scale (log\\u2082)\": -16.487867794172583}, {\"type\": \"grad_x\", \"op\": \"2.attention_layer.f.w_qkv\", \"scale (log\\u2082)\": -15.920712987523927}, {\"type\": \"grad_x\", \"op\": \"2.attention_layer.f.attn_dropout\", \"scale (log\\u2082)\": -12.17748207116142}, {\"type\": \"grad_x\", \"op\": \"2.attention_layer.f.w_o\", \"scale (log\\u2082)\": -14.754627331996549}, {\"type\": \"grad_x\", \"op\": \"2.attention_layer.f.residual_dropout\", \"scale (log\\u2082)\": -14.746684116803253}, {\"type\": \"grad_x\", \"op\": \"2.mlp_layer.f.ln\", \"scale (log\\u2082)\": -16.382050357984546}, {\"type\": \"grad_x\", \"op\": \"2.mlp_layer.f.linear_1\", \"scale (log\\u2082)\": -15.83710117578074}, {\"type\": \"grad_x\", \"op\": \"2.mlp_layer.f.act\", \"scale (log\\u2082)\": -16.177514844214684}, {\"type\": \"grad_x\", \"op\": \"2.mlp_layer.f.linear_2\", \"scale (log\\u2082)\": -15.499735060614155}, {\"type\": \"grad_x\", \"op\": \"2.mlp_layer.f.dropout\", \"scale (log\\u2082)\": -14.838737232289601}, {\"type\": \"grad_x\", \"op\": \"3.attention_layer.f.ln\", \"scale (log\\u2082)\": -16.836738985751076}, {\"type\": \"grad_x\", \"op\": \"3.attention_layer.f.w_qkv\", \"scale (log\\u2082)\": -16.113721984429354}, {\"type\": \"grad_x\", \"op\": \"3.attention_layer.f.attn_dropout\", \"scale (log\\u2082)\": -12.309692696188163}, {\"type\": \"grad_x\", \"op\": \"3.attention_layer.f.w_o\", \"scale (log\\u2082)\": -14.888497665811894}, {\"type\": \"grad_x\", \"op\": \"3.attention_layer.f.residual_dropout\", \"scale (log\\u2082)\": -14.890095397255529}, {\"type\": \"grad_x\", \"op\": \"3.mlp_layer.f.ln\", \"scale (log\\u2082)\": -16.645293395871878}, {\"type\": \"grad_x\", \"op\": \"3.mlp_layer.f.linear_1\", \"scale (log\\u2082)\": -15.959896268721998}, {\"type\": \"grad_x\", \"op\": \"3.mlp_layer.f.act\", \"scale (log\\u2082)\": -16.296640591541337}, {\"type\": \"grad_x\", \"op\": \"3.mlp_layer.f.linear_2\", \"scale (log\\u2082)\": -15.627459571742769}, {\"type\": \"grad_x\", \"op\": \"3.mlp_layer.f.dropout\", \"scale (log\\u2082)\": -14.9643256205361}, {\"type\": \"grad_x\", \"op\": \"4.attention_layer.f.ln\", \"scale (log\\u2082)\": -17.151067467313876}, {\"type\": \"grad_x\", \"op\": \"4.attention_layer.f.w_qkv\", \"scale (log\\u2082)\": -16.294922835796005}, {\"type\": \"grad_x\", \"op\": \"4.attention_layer.f.attn_dropout\", \"scale (log\\u2082)\": -12.418016675619251}, {\"type\": \"grad_x\", \"op\": \"4.attention_layer.f.w_o\", \"scale (log\\u2082)\": -15.002384272781393}, {\"type\": \"grad_x\", \"op\": \"4.attention_layer.f.residual_dropout\", \"scale (log\\u2082)\": -15.003281460421988}, {\"type\": \"grad_x\", \"op\": \"4.mlp_layer.f.ln\", \"scale (log\\u2082)\": -16.876977744201355}, {\"type\": \"grad_x\", \"op\": \"4.mlp_layer.f.linear_1\", \"scale (log\\u2082)\": -16.071360638740877}, {\"type\": \"grad_x\", \"op\": \"4.mlp_layer.f.act\", \"scale (log\\u2082)\": -16.406985996173265}, {\"type\": \"grad_x\", \"op\": \"4.mlp_layer.f.linear_2\", \"scale (log\\u2082)\": -15.729361049947478}, {\"type\": \"grad_x\", \"op\": \"4.mlp_layer.f.dropout\", \"scale (log\\u2082)\": -15.06726737501492}, {\"type\": \"grad_x\", \"op\": \"5.attention_layer.f.ln\", \"scale (log\\u2082)\": -17.407383742316636}, {\"type\": \"grad_x\", \"op\": \"5.attention_layer.f.w_qkv\", \"scale (log\\u2082)\": -16.44457545227096}, {\"type\": \"grad_x\", \"op\": \"5.attention_layer.f.attn_dropout\", \"scale (log\\u2082)\": -12.54822104442757}, {\"type\": \"grad_x\", \"op\": \"5.attention_layer.f.w_o\", \"scale (log\\u2082)\": -15.097840208764973}, {\"type\": \"grad_x\", \"op\": \"5.attention_layer.f.residual_dropout\", \"scale (log\\u2082)\": -15.09831283634829}, {\"type\": \"grad_x\", \"op\": \"5.mlp_layer.f.ln\", \"scale (log\\u2082)\": -17.058402400365278}, {\"type\": \"grad_x\", \"op\": \"5.mlp_layer.f.linear_1\", \"scale (log\\u2082)\": -16.148776335409433}, {\"type\": \"grad_x\", \"op\": \"5.mlp_layer.f.act\", \"scale (log\\u2082)\": -16.49393229746909}, {\"type\": \"grad_x\", \"op\": \"5.mlp_layer.f.linear_2\", \"scale (log\\u2082)\": -15.811774889657611}, {\"type\": \"grad_x\", \"op\": \"5.mlp_layer.f.dropout\", \"scale (log\\u2082)\": -15.153030807327246}, {\"type\": \"grad_x\", \"op\": \"lm_head\", \"scale (log\\u2082)\": -14.244797205286218}], \"data-21f6b40b7003825d25b09fa73cc2691b\": [{\"type\": \"w\", \"op\": \"input_embeddings\", \"scale (log\\u2082)\": -0.0033524803719955512}, {\"type\": \"w\", \"op\": \"ln\", \"scale (log\\u2082)\": null}, {\"type\": \"w\", \"op\": \"0.attention_layer.f.ln\", \"scale (log\\u2082)\": null}, {\"type\": \"w\", \"op\": \"0.attention_layer.f.w_qkv\", \"scale (log\\u2082)\": -4.7923357678962}, {\"type\": \"w\", \"op\": \"0.attention_layer.f.w_o\", \"scale (log\\u2082)\": -4.293180042929255}, {\"type\": \"w\", \"op\": \"0.mlp_layer.f.ln\", \"scale (log\\u2082)\": null}, {\"type\": \"w\", \"op\": \"0.mlp_layer.f.linear_1\", \"scale (log\\u2082)\": -4.953137447161682}, {\"type\": \"w\", \"op\": \"0.mlp_layer.f.linear_2\", \"scale (log\\u2082)\": -4.955532670005829}, {\"type\": \"w\", \"op\": \"1.attention_layer.f.ln\", \"scale (log\\u2082)\": null}, {\"type\": \"w\", \"op\": \"1.attention_layer.f.w_qkv\", \"scale (log\\u2082)\": -4.791022091348344}, {\"type\": \"w\", \"op\": \"1.attention_layer.f.w_o\", \"scale (log\\u2082)\": -4.290777641426833}, {\"type\": \"w\", \"op\": \"1.mlp_layer.f.ln\", \"scale (log\\u2082)\": null}, {\"type\": \"w\", \"op\": \"1.mlp_layer.f.linear_1\", \"scale (log\\u2082)\": -4.953038557892993}, {\"type\": \"w\", \"op\": \"1.mlp_layer.f.linear_2\", \"scale (log\\u2082)\": -4.952395610015114}, {\"type\": \"w\", \"op\": \"2.attention_layer.f.ln\", \"scale (log\\u2082)\": null}, {\"type\": \"w\", \"op\": \"2.attention_layer.f.w_qkv\", \"scale (log\\u2082)\": -4.794375265579663}, {\"type\": \"w\", \"op\": \"2.attention_layer.f.w_o\", \"scale (log\\u2082)\": -4.295320082254144}, {\"type\": \"w\", \"op\": \"2.mlp_layer.f.ln\", \"scale (log\\u2082)\": null}, {\"type\": \"w\", \"op\": \"2.mlp_layer.f.linear_1\", \"scale (log\\u2082)\": -4.953605015486268}, {\"type\": \"w\", \"op\": \"2.mlp_layer.f.linear_2\", \"scale (log\\u2082)\": -4.953448976018221}, {\"type\": \"w\", \"op\": \"3.attention_layer.f.ln\", \"scale (log\\u2082)\": null}, {\"type\": \"w\", \"op\": \"3.attention_layer.f.w_qkv\", \"scale (log\\u2082)\": -4.789052256502348}, {\"type\": \"w\", \"op\": \"3.attention_layer.f.w_o\", \"scale (log\\u2082)\": -4.293824304153072}, {\"type\": \"w\", \"op\": \"3.mlp_layer.f.ln\", \"scale (log\\u2082)\": null}, {\"type\": \"w\", \"op\": \"3.mlp_layer.f.linear_1\", \"scale (log\\u2082)\": -4.953318928267919}, {\"type\": \"w\", \"op\": \"3.mlp_layer.f.linear_2\", \"scale (log\\u2082)\": -4.954410627436726}, {\"type\": \"w\", \"op\": \"4.attention_layer.f.ln\", \"scale (log\\u2082)\": null}, {\"type\": \"w\", \"op\": \"4.attention_layer.f.w_qkv\", \"scale (log\\u2082)\": -4.793624705154721}, {\"type\": \"w\", \"op\": \"4.attention_layer.f.w_o\", \"scale (log\\u2082)\": -4.292920438403956}, {\"type\": \"w\", \"op\": \"4.mlp_layer.f.ln\", \"scale (log\\u2082)\": null}, {\"type\": \"w\", \"op\": \"4.mlp_layer.f.linear_1\", \"scale (log\\u2082)\": -4.954618433684725}, {\"type\": \"w\", \"op\": \"4.mlp_layer.f.linear_2\", \"scale (log\\u2082)\": -4.952533728934084}, {\"type\": \"w\", \"op\": \"5.attention_layer.f.ln\", \"scale (log\\u2082)\": null}, {\"type\": \"w\", \"op\": \"5.attention_layer.f.w_qkv\", \"scale (log\\u2082)\": -4.795132483715259}, {\"type\": \"w\", \"op\": \"5.attention_layer.f.w_o\", \"scale (log\\u2082)\": -4.2908570644366515}, {\"type\": \"w\", \"op\": \"5.mlp_layer.f.ln\", \"scale (log\\u2082)\": null}, {\"type\": \"w\", \"op\": \"5.mlp_layer.f.linear_1\", \"scale (log\\u2082)\": -4.9543959638120825}, {\"type\": \"w\", \"op\": \"5.mlp_layer.f.linear_2\", \"scale (log\\u2082)\": -4.954782600119157}, {\"type\": \"w\", \"op\": \"lm_head\", \"scale (log\\u2082)\": -4.292006504982674}, {\"type\": \"grad_w\", \"op\": \"ln\", \"scale (log\\u2082)\": -9.150107027341907}, {\"type\": \"grad_w\", \"op\": \"0.attention_layer.f.ln\", \"scale (log\\u2082)\": -10.318825083686933}, {\"type\": \"grad_w\", \"op\": \"0.attention_layer.f.w_qkv\", \"scale (log\\u2082)\": -10.57913872773219}, {\"type\": \"grad_w\", \"op\": \"0.attention_layer.f.w_o\", \"scale (log\\u2082)\": -10.39762428531793}, {\"type\": \"grad_w\", \"op\": \"0.mlp_layer.f.ln\", \"scale (log\\u2082)\": -10.500366117539182}, {\"type\": \"grad_w\", \"op\": \"0.mlp_layer.f.linear_1\", \"scale (log\\u2082)\": -10.789309782076423}, {\"type\": \"grad_w\", \"op\": \"0.mlp_layer.f.linear_2\", \"scale (log\\u2082)\": -10.772819183511055}, {\"type\": \"grad_w\", \"op\": \"1.attention_layer.f.ln\", \"scale (log\\u2082)\": -10.437725312558367}, {\"type\": \"grad_w\", \"op\": \"1.attention_layer.f.w_qkv\", \"scale (log\\u2082)\": -10.80333289349026}, {\"type\": \"grad_w\", \"op\": \"1.attention_layer.f.w_o\", \"scale (log\\u2082)\": -10.604053692115235}, {\"type\": \"grad_w\", \"op\": \"1.mlp_layer.f.ln\", \"scale (log\\u2082)\": -10.661515905495511}, {\"type\": \"grad_w\", \"op\": \"1.mlp_layer.f.linear_1\", \"scale (log\\u2082)\": -10.99181213899848}, {\"type\": \"grad_w\", \"op\": \"1.mlp_layer.f.linear_2\", \"scale (log\\u2082)\": -10.995620237308259}, {\"type\": \"grad_w\", \"op\": \"2.attention_layer.f.ln\", \"scale (log\\u2082)\": -10.746207180895828}, {\"type\": \"grad_w\", \"op\": \"2.attention_layer.f.w_qkv\", \"scale (log\\u2082)\": -10.989490162571487}, {\"type\": \"grad_w\", \"op\": \"2.attention_layer.f.w_o\", \"scale (log\\u2082)\": -10.773030918313562}, {\"type\": \"grad_w\", \"op\": \"2.mlp_layer.f.ln\", \"scale (log\\u2082)\": -10.650044593259802}, {\"type\": \"grad_w\", \"op\": \"2.mlp_layer.f.linear_1\", \"scale (log\\u2082)\": -11.144400640187325}, {\"type\": \"grad_w\", \"op\": \"2.mlp_layer.f.linear_2\", \"scale (log\\u2082)\": -11.175410031334641}, {\"type\": \"grad_w\", \"op\": \"3.attention_layer.f.ln\", \"scale (log\\u2082)\": -10.938706604506239}, {\"type\": \"grad_w\", \"op\": \"3.attention_layer.f.w_qkv\", \"scale (log\\u2082)\": -11.158258209327286}, {\"type\": \"grad_w\", \"op\": \"3.attention_layer.f.w_o\", \"scale (log\\u2082)\": -10.907982969910043}, {\"type\": \"grad_w\", \"op\": \"3.mlp_layer.f.ln\", \"scale (log\\u2082)\": -10.89873343111123}, {\"type\": \"grad_w\", \"op\": \"3.mlp_layer.f.linear_1\", \"scale (log\\u2082)\": -11.279624317476333}, {\"type\": \"grad_w\", \"op\": \"3.mlp_layer.f.linear_2\", \"scale (log\\u2082)\": -11.31087794180286}, {\"type\": \"grad_w\", \"op\": \"4.attention_layer.f.ln\", \"scale (log\\u2082)\": -11.00177070168307}, {\"type\": \"grad_w\", \"op\": \"4.attention_layer.f.w_qkv\", \"scale (log\\u2082)\": -11.288986367944844}, {\"type\": \"grad_w\", \"op\": \"4.attention_layer.f.w_o\", \"scale (log\\u2082)\": -11.048359372797727}, {\"type\": \"grad_w\", \"op\": \"4.mlp_layer.f.ln\", \"scale (log\\u2082)\": -11.123708923123422}, {\"type\": \"grad_w\", \"op\": \"4.mlp_layer.f.linear_1\", \"scale (log\\u2082)\": -11.3897299491101}, {\"type\": \"grad_w\", \"op\": \"4.mlp_layer.f.linear_2\", \"scale (log\\u2082)\": -11.430769023030633}, {\"type\": \"grad_w\", \"op\": \"5.attention_layer.f.ln\", \"scale (log\\u2082)\": -11.123246167148437}, {\"type\": \"grad_w\", \"op\": \"5.attention_layer.f.w_qkv\", \"scale (log\\u2082)\": -11.418192011631563}, {\"type\": \"grad_w\", \"op\": \"5.attention_layer.f.w_o\", \"scale (log\\u2082)\": -11.187034233890229}, {\"type\": \"grad_w\", \"op\": \"5.mlp_layer.f.ln\", \"scale (log\\u2082)\": -11.069606804112107}, {\"type\": \"grad_w\", \"op\": \"5.mlp_layer.f.linear_1\", \"scale (log\\u2082)\": -11.486248071648541}, {\"type\": \"grad_w\", \"op\": \"5.mlp_layer.f.linear_2\", \"scale (log\\u2082)\": -11.543141784509727}, {\"type\": \"grad_w\", \"op\": \"lm_head\", \"scale (log\\u2082)\": -9.215409482189294}], \"empty\": [{}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.LayerChart(...)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = NanoGPTModel(config)\n",
    "analyse_full_model(model, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results clearly demonstrates the inadiquacy of the standard approach to model design when it comes to scale. Grad_x and grad_w values are very far from having unit scale, with the former now relying on subnormal values (dangerous, as these begin to lose precision and disappear altogether at $2^{-24}$).\n",
    "\n",
    "A key issue introduced by this layer is the cross-entropy loss definition. This typically uses a `1 / batch_size` term to average over labels, which has the effect of dramatically under-scaling gradients in the backward pass.\n",
    "\n",
    "We can also see here that a single loss scaling factor for all gradients isn't ideal. This would shift all grad_xs and grad_ws to the right by the same amount, which would still leave us far short of our unit-scale ideal.\n",
    "\n",
    "What we really need is per-op scaling! Again, we'll fix this for the unit-scaled implementation:\n",
    "\n",
    "#### Unit scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-20T16:50:18.426418Z",
     "iopub.status.busy": "2023-03-20T16:50:18.426228Z",
     "iopub.status.idle": "2023-03-20T16:50:18.434829Z",
     "shell.execute_reply": "2023-03-20T16:50:18.434266Z",
     "shell.execute_reply.started": "2023-03-20T16:50:18.426398Z"
    }
   },
   "outputs": [],
   "source": [
    "class UnitScaledNanoGPTModel(NanoGPTModel):\n",
    "    config_class = NanoGPTConfig\n",
    "\n",
    "    def __init__(self, config: NanoGPTConfig) -> None:\n",
    "        super().__init__(config)\n",
    "        self.input_embeddings = UnitScaledEmbedding(config.vocab_size, config.hidden_size)\n",
    "        self.dropout = UnitScaledDropout(config.dropout)\n",
    "        self.ln = UnitScaledLayerNorm(config.hidden_size)\n",
    "        self.blocks = nn.ModuleList([\n",
    "            UnitScaledTransformerBlock(config)\n",
    "            for _ in range(config.num_hidden_layers)\n",
    "        ])\n",
    "        self.lm_head = UnitScaledLinear(\n",
    "            config.hidden_size, config.vocab_size, bias=False, scale_for=\"separate\"\n",
    "        )\n",
    "        self.loss_fn = UnitScaledCrossEntropyLoss()\n",
    "        self.apply(init_weights(unit_init))\n",
    "\n",
    "class UnitScaledEmbedding(nn.Embedding):\n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        batch_size = np.prod(input.shape)\n",
    "        weight = scaled(self.weight, beta=self.num_embeddings / batch_size)\n",
    "        return F.embedding(input, weight, self.padding_idx, self.max_norm,\n",
    "                           self.norm_type, self.scale_grad_by_freq, self.sparse)\n",
    "\n",
    "class UnitScaledCrossEntropyLoss(nn.CrossEntropyLoss):    \n",
    "    def forward(self, input: Tensor, target: Tensor) -> Tensor:\n",
    "        batch_size, seq_len = input.shape\n",
    "        input = scaled(input, beta=seq_len / (seq_len - 1) ** 0.5)\n",
    "        loss = F.cross_entropy(input, target, reduction='sum')\n",
    "        return scaled(loss, alpha=1 / batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-20T16:50:18.435875Z",
     "iopub.status.busy": "2023-03-20T16:50:18.435667Z",
     "iopub.status.idle": "2023-03-20T16:50:19.665350Z",
     "shell.execute_reply": "2023-03-20T16:50:19.664649Z",
     "shell.execute_reply.started": "2023-03-20T16:50:18.435855Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-5f8a874d02d0482286ecd49213a07650\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-5f8a874d02d0482286ecd49213a07650\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-5f8a874d02d0482286ecd49213a07650\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.17.0?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"4.17.0\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}, \"axis\": {\"labelFontSize\": 12, \"titleFontSize\": 16}}, \"layer\": [{\"data\": {\"name\": \"data-70698f89ba77d3458377a318e2f971bf\"}, \"mark\": \"line\", \"encoding\": {\"color\": {\"field\": \"type\", \"legend\": {\"labelFontSize\": 12, \"symbolSize\": 100, \"title\": \"\"}, \"scale\": {\"range\": [\"#6C8EBF\", \"#FF8000\"]}, \"sort\": \"descending\", \"type\": \"nominal\"}, \"x\": {\"axis\": {\"orient\": \"top\", \"values\": [-18.0, -16.0, -14.0, -12.0, -10.0, -8.0, -6.0, -4.0, -2.0, 0.0, 2.0, 4.0, 6.0, 8.0, 10.0, 12.0, 14.0, 16.0, 18.0]}, \"field\": \"scale (log\\u2082)\", \"scale\": {\"domain\": [-18.0, 18.0]}, \"type\": \"quantitative\"}, \"y\": {\"field\": \"op\", \"sort\": [\"input_embeddings\", \"dropout\", \"ln\", \"0.attention_layer.f.ln\", \"0.attention_layer.f.w_qkv\", \"0.attention_layer.f.qk_matmul\", \"0.attention_layer.f.attn_dropout\", \"0.attention_layer.f.qkv_matmul\", \"0.attention_layer.f.w_o\", \"0.attention_layer.f.residual_dropout\", \"0.mlp_layer.f.ln\", \"0.mlp_layer.f.linear_1\", \"0.mlp_layer.f.act\", \"0.mlp_layer.f.linear_2\", \"0.mlp_layer.f.dropout\", \"1.attention_layer.f.ln\", \"1.attention_layer.f.w_qkv\", \"1.attention_layer.f.qk_matmul\", \"1.attention_layer.f.attn_dropout\", \"1.attention_layer.f.qkv_matmul\", \"1.attention_layer.f.w_o\", \"1.attention_layer.f.residual_dropout\", \"1.mlp_layer.f.ln\", \"1.mlp_layer.f.linear_1\", \"1.mlp_layer.f.act\", \"1.mlp_layer.f.linear_2\", \"1.mlp_layer.f.dropout\", \"2.attention_layer.f.ln\", \"2.attention_layer.f.w_qkv\", \"2.attention_layer.f.qk_matmul\", \"2.attention_layer.f.attn_dropout\", \"2.attention_layer.f.qkv_matmul\", \"2.attention_layer.f.w_o\", \"2.attention_layer.f.residual_dropout\", \"2.mlp_layer.f.ln\", \"2.mlp_layer.f.linear_1\", \"2.mlp_layer.f.act\", \"2.mlp_layer.f.linear_2\", \"2.mlp_layer.f.dropout\", \"3.attention_layer.f.ln\", \"3.attention_layer.f.w_qkv\", \"3.attention_layer.f.qk_matmul\", \"3.attention_layer.f.attn_dropout\", \"3.attention_layer.f.qkv_matmul\", \"3.attention_layer.f.w_o\", \"3.attention_layer.f.residual_dropout\", \"3.mlp_layer.f.ln\", \"3.mlp_layer.f.linear_1\", \"3.mlp_layer.f.act\", \"3.mlp_layer.f.linear_2\", \"3.mlp_layer.f.dropout\", \"4.attention_layer.f.ln\", \"4.attention_layer.f.w_qkv\", \"4.attention_layer.f.qk_matmul\", \"4.attention_layer.f.attn_dropout\", \"4.attention_layer.f.qkv_matmul\", \"4.attention_layer.f.w_o\", \"4.attention_layer.f.residual_dropout\", \"4.mlp_layer.f.ln\", \"4.mlp_layer.f.linear_1\", \"4.mlp_layer.f.act\", \"4.mlp_layer.f.linear_2\", \"4.mlp_layer.f.dropout\", \"5.attention_layer.f.ln\", \"5.attention_layer.f.w_qkv\", \"5.attention_layer.f.qk_matmul\", \"5.attention_layer.f.attn_dropout\", \"5.attention_layer.f.qkv_matmul\", \"5.attention_layer.f.w_o\", \"5.attention_layer.f.residual_dropout\", \"5.mlp_layer.f.ln\", \"5.mlp_layer.f.linear_1\", \"5.mlp_layer.f.act\", \"5.mlp_layer.f.linear_2\", \"5.mlp_layer.f.dropout\", \"lm_head\"], \"title\": \"\", \"type\": \"ordinal\"}}}, {\"data\": {\"name\": \"data-aba164b708626377b1c2d166d9cbb395\"}, \"mark\": {\"type\": \"point\", \"size\": 100}, \"encoding\": {\"color\": {\"field\": \"type\", \"legend\": {\"labelFontSize\": 12, \"symbolSize\": 100, \"title\": \"\"}, \"scale\": {\"range\": [\"#5D8944\", \"#ED3434\"]}, \"sort\": \"descending\", \"type\": \"nominal\"}, \"shape\": {\"field\": \"type\", \"scale\": {\"range\": [\"square\", \"triangle-down\"]}, \"sort\": \"descending\", \"type\": \"nominal\"}, \"x\": {\"axis\": {\"orient\": \"top\", \"values\": [-18.0, -16.0, -14.0, -12.0, -10.0, -8.0, -6.0, -4.0, -2.0, 0.0, 2.0, 4.0, 6.0, 8.0, 10.0, 12.0, 14.0, 16.0, 18.0]}, \"field\": \"scale (log\\u2082)\", \"scale\": {\"domain\": [-18.0, 18.0]}, \"type\": \"quantitative\"}, \"y\": {\"field\": \"op\", \"sort\": [\"input_embeddings\", \"dropout\", \"ln\", \"0.attention_layer.f.ln\", \"0.attention_layer.f.w_qkv\", \"0.attention_layer.f.qk_matmul\", \"0.attention_layer.f.attn_dropout\", \"0.attention_layer.f.qkv_matmul\", \"0.attention_layer.f.w_o\", \"0.attention_layer.f.residual_dropout\", \"0.mlp_layer.f.ln\", \"0.mlp_layer.f.linear_1\", \"0.mlp_layer.f.act\", \"0.mlp_layer.f.linear_2\", \"0.mlp_layer.f.dropout\", \"1.attention_layer.f.ln\", \"1.attention_layer.f.w_qkv\", \"1.attention_layer.f.qk_matmul\", \"1.attention_layer.f.attn_dropout\", \"1.attention_layer.f.qkv_matmul\", \"1.attention_layer.f.w_o\", \"1.attention_layer.f.residual_dropout\", \"1.mlp_layer.f.ln\", \"1.mlp_layer.f.linear_1\", \"1.mlp_layer.f.act\", \"1.mlp_layer.f.linear_2\", \"1.mlp_layer.f.dropout\", \"2.attention_layer.f.ln\", \"2.attention_layer.f.w_qkv\", \"2.attention_layer.f.qk_matmul\", \"2.attention_layer.f.attn_dropout\", \"2.attention_layer.f.qkv_matmul\", \"2.attention_layer.f.w_o\", \"2.attention_layer.f.residual_dropout\", \"2.mlp_layer.f.ln\", \"2.mlp_layer.f.linear_1\", \"2.mlp_layer.f.act\", \"2.mlp_layer.f.linear_2\", \"2.mlp_layer.f.dropout\", \"3.attention_layer.f.ln\", \"3.attention_layer.f.w_qkv\", \"3.attention_layer.f.qk_matmul\", \"3.attention_layer.f.attn_dropout\", \"3.attention_layer.f.qkv_matmul\", \"3.attention_layer.f.w_o\", \"3.attention_layer.f.residual_dropout\", \"3.mlp_layer.f.ln\", \"3.mlp_layer.f.linear_1\", \"3.mlp_layer.f.act\", \"3.mlp_layer.f.linear_2\", \"3.mlp_layer.f.dropout\", \"4.attention_layer.f.ln\", \"4.attention_layer.f.w_qkv\", \"4.attention_layer.f.qk_matmul\", \"4.attention_layer.f.attn_dropout\", \"4.attention_layer.f.qkv_matmul\", \"4.attention_layer.f.w_o\", \"4.attention_layer.f.residual_dropout\", \"4.mlp_layer.f.ln\", \"4.mlp_layer.f.linear_1\", \"4.mlp_layer.f.act\", \"4.mlp_layer.f.linear_2\", \"4.mlp_layer.f.dropout\", \"5.attention_layer.f.ln\", \"5.attention_layer.f.w_qkv\", \"5.attention_layer.f.qk_matmul\", \"5.attention_layer.f.attn_dropout\", \"5.attention_layer.f.qkv_matmul\", \"5.attention_layer.f.w_o\", \"5.attention_layer.f.residual_dropout\", \"5.mlp_layer.f.ln\", \"5.mlp_layer.f.linear_1\", \"5.mlp_layer.f.act\", \"5.mlp_layer.f.linear_2\", \"5.mlp_layer.f.dropout\", \"lm_head\"], \"title\": \"\", \"type\": \"ordinal\"}}}, {\"data\": {\"name\": \"empty\"}, \"mark\": {\"type\": \"rule\", \"strokeDash\": [4, 4]}, \"encoding\": {\"x\": {\"datum\": -14}}}, {\"data\": {\"name\": \"empty\"}, \"mark\": {\"type\": \"rule\", \"strokeDash\": [4, 4]}, \"encoding\": {\"x\": {\"datum\": 16}}}, {\"data\": {\"name\": \"empty\"}, \"mark\": {\"type\": \"text\", \"dy\": -740}, \"encoding\": {\"text\": {\"value\": \"Min FP16 (normal)\"}, \"x\": {\"datum\": -10}}}, {\"data\": {\"name\": \"empty\"}, \"mark\": {\"type\": \"text\", \"dy\": -740}, \"encoding\": {\"text\": {\"value\": \"Max FP16\"}, \"x\": {\"datum\": 13}}}], \"resolve\": {\"scale\": {\"color\": \"independent\", \"shape\": \"independent\"}}, \"width\": 500, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.17.0.json\", \"datasets\": {\"data-70698f89ba77d3458377a318e2f971bf\": [{\"type\": \"x\", \"op\": \"input_embeddings\", \"scale (log\\u2082)\": 0.0015166054979348228}, {\"type\": \"x\", \"op\": \"dropout\", \"scale (log\\u2082)\": 0.0019273254967421686}, {\"type\": \"x\", \"op\": \"ln\", \"scale (log\\u2082)\": -6.793330703198998e-06}, {\"type\": \"x\", \"op\": \"0.attention_layer.f.ln\", \"scale (log\\u2082)\": -5.41746367372377e-06}, {\"type\": \"x\", \"op\": \"0.attention_layer.f.w_qkv\", \"scale (log\\u2082)\": -0.0019622800768521174}, {\"type\": \"x\", \"op\": \"0.attention_layer.f.qk_matmul\", \"scale (log\\u2082)\": -0.006159359501941302}, {\"type\": \"x\", \"op\": \"0.attention_layer.f.attn_dropout\", \"scale (log\\u2082)\": -0.28851292842813686}, {\"type\": \"x\", \"op\": \"0.attention_layer.f.qkv_matmul\", \"scale (log\\u2082)\": -0.22903099585746675}, {\"type\": \"x\", \"op\": \"0.attention_layer.f.w_o\", \"scale (log\\u2082)\": -0.2273425100268225}, {\"type\": \"x\", \"op\": \"0.attention_layer.f.residual_dropout\", \"scale (log\\u2082)\": -0.22714350306328285}, {\"type\": \"x\", \"op\": \"0.mlp_layer.f.ln\", \"scale (log\\u2082)\": -5.847421979482836e-06}, {\"type\": \"x\", \"op\": \"0.mlp_layer.f.linear_1\", \"scale (log\\u2082)\": -0.5020012902052475}, {\"type\": \"x\", \"op\": \"0.mlp_layer.f.act\", \"scale (log\\u2082)\": -0.6482465721513421}, {\"type\": \"x\", \"op\": \"0.mlp_layer.f.linear_2\", \"scale (log\\u2082)\": -0.03247732232167022}, {\"type\": \"x\", \"op\": \"0.mlp_layer.f.dropout\", \"scale (log\\u2082)\": -0.03278702426481984}, {\"type\": \"x\", \"op\": \"1.attention_layer.f.ln\", \"scale (log\\u2082)\": -5.847421979482836e-06}, {\"type\": \"x\", \"op\": \"1.attention_layer.f.w_qkv\", \"scale (log\\u2082)\": -0.001855079144097655}, {\"type\": \"x\", \"op\": \"1.attention_layer.f.qk_matmul\", \"scale (log\\u2082)\": -0.0035510789829669353}, {\"type\": \"x\", \"op\": \"1.attention_layer.f.attn_dropout\", \"scale (log\\u2082)\": -0.2941397418828685}, {\"type\": \"x\", \"op\": \"1.attention_layer.f.qkv_matmul\", \"scale (log\\u2082)\": -0.2270442614928061}, {\"type\": \"x\", \"op\": \"1.attention_layer.f.w_o\", \"scale (log\\u2082)\": -0.22948409969029732}, {\"type\": \"x\", \"op\": \"1.attention_layer.f.residual_dropout\", \"scale (log\\u2082)\": -0.22992070584247878}, {\"type\": \"x\", \"op\": \"1.mlp_layer.f.ln\", \"scale (log\\u2082)\": -6.191388716349517e-06}, {\"type\": \"x\", \"op\": \"1.mlp_layer.f.linear_1\", \"scale (log\\u2082)\": -0.49827030707279885}, {\"type\": \"x\", \"op\": \"1.mlp_layer.f.act\", \"scale (log\\u2082)\": -0.6371655119661086}, {\"type\": \"x\", \"op\": \"1.mlp_layer.f.linear_2\", \"scale (log\\u2082)\": -0.035332422136773886}, {\"type\": \"x\", \"op\": \"1.mlp_layer.f.dropout\", \"scale (log\\u2082)\": -0.034867646592304444}, {\"type\": \"x\", \"op\": \"2.attention_layer.f.ln\", \"scale (log\\u2082)\": -6.105397024444559e-06}, {\"type\": \"x\", \"op\": \"2.attention_layer.f.w_qkv\", \"scale (log\\u2082)\": 0.0010378858422119223}, {\"type\": \"x\", \"op\": \"2.attention_layer.f.qk_matmul\", \"scale (log\\u2082)\": -0.0037240130606113336}, {\"type\": \"x\", \"op\": \"2.attention_layer.f.attn_dropout\", \"scale (log\\u2082)\": -0.3043869302791807}, {\"type\": \"x\", \"op\": \"2.attention_layer.f.qkv_matmul\", \"scale (log\\u2082)\": -0.22239752349744554}, {\"type\": \"x\", \"op\": \"2.attention_layer.f.w_o\", \"scale (log\\u2082)\": -0.22468249579336733}, {\"type\": \"x\", \"op\": \"2.attention_layer.f.residual_dropout\", \"scale (log\\u2082)\": -0.22442829724058605}, {\"type\": \"x\", \"op\": \"2.mlp_layer.f.ln\", \"scale (log\\u2082)\": -6.53535553522462e-06}, {\"type\": \"x\", \"op\": \"2.mlp_layer.f.linear_1\", \"scale (log\\u2082)\": -0.4962188966935276}, {\"type\": \"x\", \"op\": \"2.mlp_layer.f.act\", \"scale (log\\u2082)\": -0.6366409462235146}, {\"type\": \"x\", \"op\": \"2.mlp_layer.f.linear_2\", \"scale (log\\u2082)\": -0.034029402740742794}, {\"type\": \"x\", \"op\": \"2.mlp_layer.f.dropout\", \"scale (log\\u2082)\": -0.03436532873960069}, {\"type\": \"x\", \"op\": \"3.attention_layer.f.ln\", \"scale (log\\u2082)\": -6.363372115536013e-06}, {\"type\": \"x\", \"op\": \"3.attention_layer.f.w_qkv\", \"scale (log\\u2082)\": -0.00309022383730504}, {\"type\": \"x\", \"op\": \"3.attention_layer.f.qk_matmul\", \"scale (log\\u2082)\": -0.004945055297794583}, {\"type\": \"x\", \"op\": \"3.attention_layer.f.attn_dropout\", \"scale (log\\u2082)\": -0.3182476137983484}, {\"type\": \"x\", \"op\": \"3.attention_layer.f.qkv_matmul\", \"scale (log\\u2082)\": -0.22617774792469414}, {\"type\": \"x\", \"op\": \"3.attention_layer.f.w_o\", \"scale (log\\u2082)\": -0.22835729855335224}, {\"type\": \"x\", \"op\": \"3.attention_layer.f.residual_dropout\", \"scale (log\\u2082)\": -0.22761000977304335}, {\"type\": \"x\", \"op\": \"3.mlp_layer.f.ln\", \"scale (log\\u2082)\": -6.7073389754153415e-06}, {\"type\": \"x\", \"op\": \"3.mlp_layer.f.linear_1\", \"scale (log\\u2082)\": -0.4945580342182381}, {\"type\": \"x\", \"op\": \"3.mlp_layer.f.act\", \"scale (log\\u2082)\": -0.6415486638641408}, {\"type\": \"x\", \"op\": \"3.mlp_layer.f.linear_2\", \"scale (log\\u2082)\": -0.029279873552998267}, {\"type\": \"x\", \"op\": \"3.mlp_layer.f.dropout\", \"scale (log\\u2082)\": -0.029691236936699113}, {\"type\": \"x\", \"op\": \"4.attention_layer.f.ln\", \"scale (log\\u2082)\": -6.449363822817553e-06}, {\"type\": \"x\", \"op\": \"4.attention_layer.f.w_qkv\", \"scale (log\\u2082)\": -0.0017935175505342138}, {\"type\": \"x\", \"op\": \"4.attention_layer.f.qk_matmul\", \"scale (log\\u2082)\": -0.0016112589754129296}, {\"type\": \"x\", \"op\": \"4.attention_layer.f.attn_dropout\", \"scale (log\\u2082)\": -0.31941727054371294}, {\"type\": \"x\", \"op\": \"4.attention_layer.f.qkv_matmul\", \"scale (log\\u2082)\": -0.21427276617480165}, {\"type\": \"x\", \"op\": \"4.attention_layer.f.w_o\", \"scale (log\\u2082)\": -0.21800342833907718}, {\"type\": \"x\", \"op\": \"4.attention_layer.f.residual_dropout\", \"scale (log\\u2082)\": -0.219267515325637}, {\"type\": \"x\", \"op\": \"4.mlp_layer.f.ln\", \"scale (log\\u2082)\": -6.6213472527572165e-06}, {\"type\": \"x\", \"op\": \"4.mlp_layer.f.linear_1\", \"scale (log\\u2082)\": -0.49970174595037675}, {\"type\": \"x\", \"op\": \"4.mlp_layer.f.act\", \"scale (log\\u2082)\": -0.6572355735390966}, {\"type\": \"x\", \"op\": \"4.mlp_layer.f.linear_2\", \"scale (log\\u2082)\": -0.0581671879796932}, {\"type\": \"x\", \"op\": \"4.mlp_layer.f.dropout\", \"scale (log\\u2082)\": -0.05757659520912702}, {\"type\": \"x\", \"op\": \"5.attention_layer.f.ln\", \"scale (log\\u2082)\": -6.6213472527572165e-06}, {\"type\": \"x\", \"op\": \"5.attention_layer.f.w_qkv\", \"scale (log\\u2082)\": 0.003662876231136465}, {\"type\": \"x\", \"op\": \"5.attention_layer.f.qk_matmul\", \"scale (log\\u2082)\": 0.034694031309955006}, {\"type\": \"x\", \"op\": \"5.attention_layer.f.attn_dropout\", \"scale (log\\u2082)\": -0.320384817382871}, {\"type\": \"x\", \"op\": \"5.attention_layer.f.qkv_matmul\", \"scale (log\\u2082)\": -0.2069612468710336}, {\"type\": \"x\", \"op\": \"5.attention_layer.f.w_o\", \"scale (log\\u2082)\": -0.2128447080633482}, {\"type\": \"x\", \"op\": \"5.attention_layer.f.residual_dropout\", \"scale (log\\u2082)\": -0.21318020757544923}, {\"type\": \"x\", \"op\": \"5.mlp_layer.f.ln\", \"scale (log\\u2082)\": -6.879322436108184e-06}, {\"type\": \"x\", \"op\": \"5.mlp_layer.f.linear_1\", \"scale (log\\u2082)\": -0.5018437168987807}, {\"type\": \"x\", \"op\": \"5.mlp_layer.f.act\", \"scale (log\\u2082)\": -0.6552201410577667}, {\"type\": \"x\", \"op\": \"5.mlp_layer.f.linear_2\", \"scale (log\\u2082)\": -0.05614059657570544}, {\"type\": \"x\", \"op\": \"5.mlp_layer.f.dropout\", \"scale (log\\u2082)\": -0.05627363515606328}, {\"type\": \"x\", \"op\": \"lm_head\", \"scale (log\\u2082)\": 0.004375476152019403}, {\"type\": \"grad_x\", \"op\": \"dropout\", \"scale (log\\u2082)\": 0.04404268994548789}, {\"type\": \"grad_x\", \"op\": \"ln\", \"scale (log\\u2082)\": 0.078198377892528}, {\"type\": \"grad_x\", \"op\": \"0.attention_layer.f.ln\", \"scale (log\\u2082)\": -0.0772664420780688}, {\"type\": \"grad_x\", \"op\": \"0.attention_layer.f.w_qkv\", \"scale (log\\u2082)\": -0.08303091774879261}, {\"type\": \"grad_x\", \"op\": \"0.attention_layer.f.attn_dropout\", \"scale (log\\u2082)\": 1.0702526525376692}, {\"type\": \"grad_x\", \"op\": \"0.attention_layer.f.w_o\", \"scale (log\\u2082)\": 0.0696996928308073}, {\"type\": \"grad_x\", \"op\": \"0.attention_layer.f.residual_dropout\", \"scale (log\\u2082)\": 0.06994859188753731}, {\"type\": \"grad_x\", \"op\": \"0.mlp_layer.f.ln\", \"scale (log\\u2082)\": 0.11095747898443203}, {\"type\": \"grad_x\", \"op\": \"0.mlp_layer.f.linear_1\", \"scale (log\\u2082)\": 0.07500917362720035}, {\"type\": \"grad_x\", \"op\": \"0.mlp_layer.f.act\", \"scale (log\\u2082)\": -0.4250690733613266}, {\"type\": \"grad_x\", \"op\": \"0.mlp_layer.f.linear_2\", \"scale (log\\u2082)\": -0.441580571355634}, {\"type\": \"grad_x\", \"op\": \"0.mlp_layer.f.dropout\", \"scale (log\\u2082)\": 0.06039430313714724}, {\"type\": \"grad_x\", \"op\": \"1.attention_layer.f.ln\", \"scale (log\\u2082)\": -0.06922296600416734}, {\"type\": \"grad_x\", \"op\": \"1.attention_layer.f.w_qkv\", \"scale (log\\u2082)\": -0.09681329534270039}, {\"type\": \"grad_x\", \"op\": \"1.attention_layer.f.attn_dropout\", \"scale (log\\u2082)\": 1.0767200377774275}, {\"type\": \"grad_x\", \"op\": \"1.attention_layer.f.w_o\", \"scale (log\\u2082)\": 0.08137875963902143}, {\"type\": \"grad_x\", \"op\": \"1.attention_layer.f.residual_dropout\", \"scale (log\\u2082)\": 0.08439318791635779}, {\"type\": \"grad_x\", \"op\": \"1.mlp_layer.f.ln\", \"scale (log\\u2082)\": 0.15825773297245868}, {\"type\": \"grad_x\", \"op\": \"1.mlp_layer.f.linear_1\", \"scale (log\\u2082)\": 0.08826110546206127}, {\"type\": \"grad_x\", \"op\": \"1.mlp_layer.f.act\", \"scale (log\\u2082)\": -0.41374511275157133}, {\"type\": \"grad_x\", \"op\": \"1.mlp_layer.f.linear_2\", \"scale (log\\u2082)\": -0.4362509430097452}, {\"type\": \"grad_x\", \"op\": \"1.mlp_layer.f.dropout\", \"scale (log\\u2082)\": 0.06404242917438739}, {\"type\": \"grad_x\", \"op\": \"2.attention_layer.f.ln\", \"scale (log\\u2082)\": -0.05732594832658063}, {\"type\": \"grad_x\", \"op\": \"2.attention_layer.f.w_qkv\", \"scale (log\\u2082)\": -0.11062415620638}, {\"type\": \"grad_x\", \"op\": \"2.attention_layer.f.attn_dropout\", \"scale (log\\u2082)\": 1.0813311316790364}, {\"type\": \"grad_x\", \"op\": \"2.attention_layer.f.w_o\", \"scale (log\\u2082)\": 0.08875605659794676}, {\"type\": \"grad_x\", \"op\": \"2.attention_layer.f.residual_dropout\", \"scale (log\\u2082)\": 0.09038334547458797}, {\"type\": \"grad_x\", \"op\": \"2.mlp_layer.f.ln\", \"scale (log\\u2082)\": 0.18576998988617177}, {\"type\": \"grad_x\", \"op\": \"2.mlp_layer.f.linear_1\", \"scale (log\\u2082)\": 0.09104663235164918}, {\"type\": \"grad_x\", \"op\": \"2.mlp_layer.f.act\", \"scale (log\\u2082)\": -0.410814975198909}, {\"type\": \"grad_x\", \"op\": \"2.mlp_layer.f.linear_2\", \"scale (log\\u2082)\": -0.43247846487366076}, {\"type\": \"grad_x\", \"op\": \"2.mlp_layer.f.dropout\", \"scale (log\\u2082)\": 0.06983946829175808}, {\"type\": \"grad_x\", \"op\": \"3.attention_layer.f.ln\", \"scale (log\\u2082)\": -0.057809742162381036}, {\"type\": \"grad_x\", \"op\": \"3.attention_layer.f.w_qkv\", \"scale (log\\u2082)\": -0.12394035678507492}, {\"type\": \"grad_x\", \"op\": \"3.attention_layer.f.attn_dropout\", \"scale (log\\u2082)\": 1.0936516067856545}, {\"type\": \"grad_x\", \"op\": \"3.attention_layer.f.w_o\", \"scale (log\\u2082)\": 0.0974264966044031}, {\"type\": \"grad_x\", \"op\": \"3.attention_layer.f.residual_dropout\", \"scale (log\\u2082)\": 0.09709836449271982}, {\"type\": \"grad_x\", \"op\": \"3.mlp_layer.f.ln\", \"scale (log\\u2082)\": 0.1919976114760179}, {\"type\": \"grad_x\", \"op\": \"3.mlp_layer.f.linear_1\", \"scale (log\\u2082)\": 0.07752084397444124}, {\"type\": \"grad_x\", \"op\": \"3.mlp_layer.f.act\", \"scale (log\\u2082)\": -0.4203160044516591}, {\"type\": \"grad_x\", \"op\": \"3.mlp_layer.f.linear_2\", \"scale (log\\u2082)\": -0.43384884889958203}, {\"type\": \"grad_x\", \"op\": \"3.mlp_layer.f.dropout\", \"scale (log\\u2082)\": 0.07079524811252318}, {\"type\": \"grad_x\", \"op\": \"4.attention_layer.f.ln\", \"scale (log\\u2082)\": -0.05612477223841756}, {\"type\": \"grad_x\", \"op\": \"4.attention_layer.f.w_qkv\", \"scale (log\\u2082)\": -0.1369636594428297}, {\"type\": \"grad_x\", \"op\": \"4.attention_layer.f.attn_dropout\", \"scale (log\\u2082)\": 1.0988240402371983}, {\"type\": \"grad_x\", \"op\": \"4.attention_layer.f.w_o\", \"scale (log\\u2082)\": 0.10069329429719312}, {\"type\": \"grad_x\", \"op\": \"4.attention_layer.f.residual_dropout\", \"scale (log\\u2082)\": 0.10115465792633789}, {\"type\": \"grad_x\", \"op\": \"4.mlp_layer.f.ln\", \"scale (log\\u2082)\": 0.195275775377811}, {\"type\": \"grad_x\", \"op\": \"4.mlp_layer.f.linear_1\", \"scale (log\\u2082)\": 0.08121051049453956}, {\"type\": \"grad_x\", \"op\": \"4.mlp_layer.f.act\", \"scale (log\\u2082)\": -0.42103056926124877}, {\"type\": \"grad_x\", \"op\": \"4.mlp_layer.f.linear_2\", \"scale (log\\u2082)\": -0.42706505605047823}, {\"type\": \"grad_x\", \"op\": \"4.mlp_layer.f.dropout\", \"scale (log\\u2082)\": 0.07375455648367142}, {\"type\": \"grad_x\", \"op\": \"5.attention_layer.f.ln\", \"scale (log\\u2082)\": -0.0638175935086259}, {\"type\": \"grad_x\", \"op\": \"5.attention_layer.f.w_qkv\", \"scale (log\\u2082)\": -0.1561586700186234}, {\"type\": \"grad_x\", \"op\": \"5.attention_layer.f.attn_dropout\", \"scale (log\\u2082)\": 1.0907268973469186}, {\"type\": \"grad_x\", \"op\": \"5.attention_layer.f.w_o\", \"scale (log\\u2082)\": 0.10210241988124079}, {\"type\": \"grad_x\", \"op\": \"5.attention_layer.f.residual_dropout\", \"scale (log\\u2082)\": 0.10576691027250983}, {\"type\": \"grad_x\", \"op\": \"5.mlp_layer.f.ln\", \"scale (log\\u2082)\": 0.21970679729321624}, {\"type\": \"grad_x\", \"op\": \"5.mlp_layer.f.linear_1\", \"scale (log\\u2082)\": 0.08557359947980071}, {\"type\": \"grad_x\", \"op\": \"5.mlp_layer.f.act\", \"scale (log\\u2082)\": -0.41666617671356876}, {\"type\": \"grad_x\", \"op\": \"5.mlp_layer.f.linear_2\", \"scale (log\\u2082)\": -0.42493584430859915}, {\"type\": \"grad_x\", \"op\": \"5.mlp_layer.f.dropout\", \"scale (log\\u2082)\": 0.07799049126907061}, {\"type\": \"grad_x\", \"op\": \"lm_head\", \"scale (log\\u2082)\": -0.04211223179540383}], \"data-aba164b708626377b1c2d166d9cbb395\": [{\"type\": \"w\", \"op\": \"input_embeddings\", \"scale (log\\u2082)\": -0.00033824353085225604}, {\"type\": \"w\", \"op\": \"ln\", \"scale (log\\u2082)\": null}, {\"type\": \"w\", \"op\": \"0.attention_layer.f.ln\", \"scale (log\\u2082)\": null}, {\"type\": \"w\", \"op\": \"0.attention_layer.f.w_qkv\", \"scale (log\\u2082)\": -0.0025817044018203368}, {\"type\": \"w\", \"op\": \"0.attention_layer.f.w_o\", \"scale (log\\u2082)\": 0.0006537299035510112}, {\"type\": \"w\", \"op\": \"0.mlp_layer.f.ln\", \"scale (log\\u2082)\": null}, {\"type\": \"w\", \"op\": \"0.mlp_layer.f.linear_1\", \"scale (log\\u2082)\": -0.0016657533399381365}, {\"type\": \"w\", \"op\": \"0.mlp_layer.f.linear_2\", \"scale (log\\u2082)\": -0.0020405547049505693}, {\"type\": \"w\", \"op\": \"1.attention_layer.f.ln\", \"scale (log\\u2082)\": null}, {\"type\": \"w\", \"op\": \"1.attention_layer.f.w_qkv\", \"scale (log\\u2082)\": -0.002166199098458749}, {\"type\": \"w\", \"op\": \"1.attention_layer.f.w_o\", \"scale (log\\u2082)\": -0.002302103708364473}, {\"type\": \"w\", \"op\": \"1.mlp_layer.f.ln\", \"scale (log\\u2082)\": null}, {\"type\": \"w\", \"op\": \"1.mlp_layer.f.linear_1\", \"scale (log\\u2082)\": 0.002637692943776065}, {\"type\": \"w\", \"op\": \"1.mlp_layer.f.linear_2\", \"scale (log\\u2082)\": -0.0003526935333157916}, {\"type\": \"w\", \"op\": \"2.attention_layer.f.ln\", \"scale (log\\u2082)\": null}, {\"type\": \"w\", \"op\": \"2.attention_layer.f.w_qkv\", \"scale (log\\u2082)\": 0.0004799237445196671}, {\"type\": \"w\", \"op\": \"2.attention_layer.f.w_o\", \"scale (log\\u2082)\": -0.0028922056354971676}, {\"type\": \"w\", \"op\": \"2.mlp_layer.f.ln\", \"scale (log\\u2082)\": null}, {\"type\": \"w\", \"op\": \"2.mlp_layer.f.linear_1\", \"scale (log\\u2082)\": 0.0021883665136781423}, {\"type\": \"w\", \"op\": \"2.mlp_layer.f.linear_2\", \"scale (log\\u2082)\": -0.0006716625859951024}, {\"type\": \"w\", \"op\": \"3.attention_layer.f.ln\", \"scale (log\\u2082)\": null}, {\"type\": \"w\", \"op\": \"3.attention_layer.f.w_qkv\", \"scale (log\\u2082)\": -0.0017132761721515418}, {\"type\": \"w\", \"op\": \"3.attention_layer.f.w_o\", \"scale (log\\u2082)\": 1.5134393893010827e-05}, {\"type\": \"w\", \"op\": \"3.mlp_layer.f.ln\", \"scale (log\\u2082)\": null}, {\"type\": \"w\", \"op\": \"3.mlp_layer.f.linear_1\", \"scale (log\\u2082)\": 0.0022046800090691028}, {\"type\": \"w\", \"op\": \"3.mlp_layer.f.linear_2\", \"scale (log\\u2082)\": -0.0024917714649996116}, {\"type\": \"w\", \"op\": \"4.attention_layer.f.ln\", \"scale (log\\u2082)\": null}, {\"type\": \"w\", \"op\": \"4.attention_layer.f.w_qkv\", \"scale (log\\u2082)\": 0.0009376885835217384}, {\"type\": \"w\", \"op\": \"4.attention_layer.f.w_o\", \"scale (log\\u2082)\": -0.002493580405606981}, {\"type\": \"w\", \"op\": \"4.mlp_layer.f.ln\", \"scale (log\\u2082)\": null}, {\"type\": \"w\", \"op\": \"4.mlp_layer.f.linear_1\", \"scale (log\\u2082)\": 0.0013124903398776795}, {\"type\": \"w\", \"op\": \"4.mlp_layer.f.linear_2\", \"scale (log\\u2082)\": -0.00038477649666198127}, {\"type\": \"w\", \"op\": \"5.attention_layer.f.ln\", \"scale (log\\u2082)\": null}, {\"type\": \"w\", \"op\": \"5.attention_layer.f.w_qkv\", \"scale (log\\u2082)\": -0.0004068823316003299}, {\"type\": \"w\", \"op\": \"5.attention_layer.f.w_o\", \"scale (log\\u2082)\": -0.0011181484417249004}, {\"type\": \"w\", \"op\": \"5.mlp_layer.f.ln\", \"scale (log\\u2082)\": null}, {\"type\": \"w\", \"op\": \"5.mlp_layer.f.linear_1\", \"scale (log\\u2082)\": -0.0005142337705289129}, {\"type\": \"w\", \"op\": \"5.mlp_layer.f.linear_2\", \"scale (log\\u2082)\": -0.0007045269432580428}, {\"type\": \"w\", \"op\": \"lm_head\", \"scale (log\\u2082)\": 0.0010982070795064894}, {\"type\": \"grad_w\", \"op\": \"ln\", \"scale (log\\u2082)\": 0.09729145861844779}, {\"type\": \"grad_w\", \"op\": \"0.attention_layer.f.ln\", \"scale (log\\u2082)\": -0.08097448845415708}, {\"type\": \"grad_w\", \"op\": \"0.attention_layer.f.w_qkv\", \"scale (log\\u2082)\": -0.8730317009746936}, {\"type\": \"grad_w\", \"op\": \"0.attention_layer.f.w_o\", \"scale (log\\u2082)\": -0.08557554013652283}, {\"type\": \"grad_w\", \"op\": \"0.mlp_layer.f.ln\", \"scale (log\\u2082)\": 0.144586832409429}, {\"type\": \"grad_w\", \"op\": \"0.mlp_layer.f.linear_1\", \"scale (log\\u2082)\": -0.4166090151105555}, {\"type\": \"grad_w\", \"op\": \"0.mlp_layer.f.linear_2\", \"scale (log\\u2082)\": -0.413607656375734}, {\"type\": \"grad_w\", \"op\": \"1.attention_layer.f.ln\", \"scale (log\\u2082)\": 0.043724544986902765}, {\"type\": \"grad_w\", \"op\": \"1.attention_layer.f.w_qkv\", \"scale (log\\u2082)\": -0.853814513951056}, {\"type\": \"grad_w\", \"op\": \"1.attention_layer.f.w_o\", \"scale (log\\u2082)\": -0.07017905142996378}, {\"type\": \"grad_w\", \"op\": \"1.mlp_layer.f.ln\", \"scale (log\\u2082)\": 0.23377661696207738}, {\"type\": \"grad_w\", \"op\": \"1.mlp_layer.f.linear_1\", \"scale (log\\u2082)\": -0.3985758073388665}, {\"type\": \"grad_w\", \"op\": \"1.mlp_layer.f.linear_2\", \"scale (log\\u2082)\": -0.40332727299114907}, {\"type\": \"grad_w\", \"op\": \"2.attention_layer.f.ln\", \"scale (log\\u2082)\": -0.10660004706074698}, {\"type\": \"grad_w\", \"op\": \"2.attention_layer.f.w_qkv\", \"scale (log\\u2082)\": -0.854261541807377}, {\"type\": \"grad_w\", \"op\": \"2.attention_layer.f.w_o\", \"scale (log\\u2082)\": -0.06991871393998036}, {\"type\": \"grad_w\", \"op\": \"2.mlp_layer.f.ln\", \"scale (log\\u2082)\": 0.18153173133558087}, {\"type\": \"grad_w\", \"op\": \"2.mlp_layer.f.linear_1\", \"scale (log\\u2082)\": -0.38805384589374325}, {\"type\": \"grad_w\", \"op\": \"2.mlp_layer.f.linear_2\", \"scale (log\\u2082)\": -0.3898982750238579}, {\"type\": \"grad_w\", \"op\": \"3.attention_layer.f.ln\", \"scale (log\\u2082)\": -0.15893566163708153}, {\"type\": \"grad_w\", \"op\": \"3.attention_layer.f.w_qkv\", \"scale (log\\u2082)\": -0.852948985597407}, {\"type\": \"grad_w\", \"op\": \"3.attention_layer.f.w_o\", \"scale (log\\u2082)\": -0.08178804455295201}, {\"type\": \"grad_w\", \"op\": \"3.mlp_layer.f.ln\", \"scale (log\\u2082)\": 0.01875642366713573}, {\"type\": \"grad_w\", \"op\": \"3.mlp_layer.f.linear_1\", \"scale (log\\u2082)\": -0.3984712984795948}, {\"type\": \"grad_w\", \"op\": \"3.mlp_layer.f.linear_2\", \"scale (log\\u2082)\": -0.39300727035997907}, {\"type\": \"grad_w\", \"op\": \"4.attention_layer.f.ln\", \"scale (log\\u2082)\": -0.11510183529956854}, {\"type\": \"grad_w\", \"op\": \"4.attention_layer.f.w_qkv\", \"scale (log\\u2082)\": -0.8446874766754093}, {\"type\": \"grad_w\", \"op\": \"4.attention_layer.f.w_o\", \"scale (log\\u2082)\": -0.06615953307682271}, {\"type\": \"grad_w\", \"op\": \"4.mlp_layer.f.ln\", \"scale (log\\u2082)\": 0.05119317224020472}, {\"type\": \"grad_w\", \"op\": \"4.mlp_layer.f.linear_1\", \"scale (log\\u2082)\": -0.3960415058879076}, {\"type\": \"grad_w\", \"op\": \"4.mlp_layer.f.linear_2\", \"scale (log\\u2082)\": -0.4147528387493812}, {\"type\": \"grad_w\", \"op\": \"5.attention_layer.f.ln\", \"scale (log\\u2082)\": -0.04336869603067049}, {\"type\": \"grad_w\", \"op\": \"5.attention_layer.f.w_qkv\", \"scale (log\\u2082)\": -0.857391505305957}, {\"type\": \"grad_w\", \"op\": \"5.attention_layer.f.w_o\", \"scale (log\\u2082)\": -0.06947451550683177}, {\"type\": \"grad_w\", \"op\": \"5.mlp_layer.f.ln\", \"scale (log\\u2082)\": 0.0922185494887558}, {\"type\": \"grad_w\", \"op\": \"5.mlp_layer.f.linear_1\", \"scale (log\\u2082)\": -0.3912684842988959}, {\"type\": \"grad_w\", \"op\": \"5.mlp_layer.f.linear_2\", \"scale (log\\u2082)\": -0.42321559349243987}, {\"type\": \"grad_w\", \"op\": \"lm_head\", \"scale (log\\u2082)\": 0.001341356862927274}], \"empty\": [{}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.LayerChart(...)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = UnitScaledNanoGPTModel(config)\n",
    "analyse_full_model(model, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Success! 🥂 🎊 🍾 We've managed to keep every tensor in the model at unit scale for the forward and backward pass. All that was required was to re-implement standard layers with the right scaling factors.\n",
    "\n",
    "Of course, this only gives us the right scaling *at initialisation*. Scales inevitably drift throughout training, but we've given ourselves headroom by starting in the ideal place. The results in our paper show that this is sufficient to enable accurate, out-the-box training of many models, up to the size of BERT Large (we haven't tested anything larger—yet!)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "#### Regular scaling\n",
    "\n",
    "To show that this really does work, let's train both our models.\n",
    "\n",
    "As in Karpathy's original NanoGPT, we'll train on the small Shakespeare dataset for a few minutes and see if we can get something that captures the rough style. Starting with the regular (non-unit-scaled) model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-20T16:50:19.666598Z",
     "iopub.status.busy": "2023-03-20T16:50:19.666407Z",
     "iopub.status.idle": "2023-03-20T16:50:21.201896Z",
     "shell.execute_reply": "2023-03-20T16:50:21.201169Z",
     "shell.execute_reply.started": "2023-03-20T16:50:19.666579Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: hide\n",
    "from optimum.graphcore.generation_utils import IPUGenerationMixin\n",
    "from optimum.graphcore.modeling_utils import (\n",
    "    PipelineMixin,\n",
    "    outline_attribute,\n",
    "    register,\n",
    "    tied_weight_model,\n",
    ")\n",
    "\n",
    "@tied_weight_model(NanoGPTModel)\n",
    "@register(NanoGPTModel)\n",
    "class PipelinedNanoGPTModel(NanoGPTModel, PipelineMixin, IPUGenerationMixin):\n",
    "    def parallelize(self):\n",
    "        self._hooks = [outline_attribute(self.ln, \"LayerNorm\")]\n",
    "\n",
    "    def deparallelize(self):\n",
    "        pass\n",
    "\n",
    "@tied_weight_model(UnitScaledNanoGPTModel)\n",
    "@register(UnitScaledNanoGPTModel)\n",
    "class PipelinedUnitScaledNanoGPTModel(\n",
    "    UnitScaledNanoGPTModel, PipelineMixin, IPUGenerationMixin\n",
    "):\n",
    "    def parallelize(self):\n",
    "        self._hooks = [outline_attribute(self.ln, \"UnitScaledLayerNorm\")]\n",
    "\n",
    "    def deparallelize(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-20T16:50:21.203155Z",
     "iopub.status.busy": "2023-03-20T16:50:21.202853Z",
     "iopub.status.idle": "2023-03-20T16:50:21.217343Z",
     "shell.execute_reply": "2023-03-20T16:50:21.216591Z",
     "shell.execute_reply.started": "2023-03-20T16:50:21.203135Z"
    }
   },
   "outputs": [],
   "source": [
    "import poptorch_experimental_addons as pea\n",
    "\n",
    "def scaled(X, alpha=1.0, beta=1.0):\n",
    "  # Forward: Y = X * alpha\n",
    "  # Backward: grad_X = grad_Y * beta\n",
    "  return pea.autograd_proxy(X * alpha, X * beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-20T16:50:21.218413Z",
     "iopub.status.busy": "2023-03-20T16:50:21.218216Z",
     "iopub.status.idle": "2023-03-20T16:50:21.222363Z",
     "shell.execute_reply": "2023-03-20T16:50:21.221759Z",
     "shell.execute_reply.started": "2023-03-20T16:50:21.218393Z"
    }
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "from datasets.load import load_dataset\n",
    "from optimum.graphcore import (\n",
    "    IPUConfig,\n",
    "    IPUTrainer,\n",
    "    IPUTrainingArguments,\n",
    "    pipeline,\n",
    "    pipelines,\n",
    ")\n",
    "from transformers import AutoTokenizer, DataCollatorForLanguageModeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-20T16:50:21.223272Z",
     "iopub.status.busy": "2023-03-20T16:50:21.223099Z",
     "iopub.status.idle": "2023-03-20T16:50:24.910139Z",
     "shell.execute_reply": "2023-03-20T16:50:24.909293Z",
     "shell.execute_reply.started": "2023-03-20T16:50:21.223254Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb62757fd41841a987bf607bbd44048d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/3.73k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "800041a84f2842e7bf8c61e54634162b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/1.90k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8ed95e88e4c4cb68e5cd3b3677c2268",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/6.10k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset tiny_shakespeare/default to /root/.cache/huggingface/datasets/tiny_shakespeare/default/1.0.0/b5b13969f09fe8707337f6cb296314fbe06960bd9a868dca39e713e163d27b5e...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5641dbd634c14409a414147d2a756dd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/435k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a408b52f875148d69146fc6c72686e2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "512cc71b918e484ba64ca7360d93fecf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b4b70d4127c46bab506be62f24e8335",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset tiny_shakespeare downloaded and prepared to /root/.cache/huggingface/datasets/tiny_shakespeare/default/1.0.0/b5b13969f09fe8707337f6cb296314fbe06960bd9a868dca39e713e163d27b5e. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "836d8ff2551b4efeb7732d85f15623f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7233cf32fce4ce19641be0854df9a59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/2.59k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3001d32512254074b6c3eb3b19aab0c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/698 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a50c821beb5439095964c8a4427afd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/2.50k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb5345367d3545e1bb5d30d442e6a9d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "663501814f68476ba5268ed84acd70f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24beb4b31107498f8694fc561c6e8827",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"tiny_shakespeare\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/byt5-small\")\n",
    "\n",
    "config = NanoGPTConfig(vocab_size=len(tokenizer), eos_token_id=tokenizer.eos_token_id)\n",
    "batch_sz = 16\n",
    "seq_len = 128\n",
    "ipu_config = IPUConfig(\n",
    "    gradient_accumulation_steps=5 * 64 // batch_sz,\n",
    "    layers_per_ipu=[config.num_hidden_layers],\n",
    "    executable_cache_dir=\"./exe_cache\",\n",
    ")\n",
    "\n",
    "def split_and_tokenize(data, seq_len, batch_sz):\n",
    "    tokens = tokenizer(data[\"text\"])\n",
    "    seqs = [\n",
    "        tokens[\"input_ids\"][0][i : i + seq_len]\n",
    "        for i in range(0, len(tokens[\"input_ids\"][0]), seq_len)\n",
    "    ]\n",
    "    seqs = seqs[: int(len(seqs) / batch_sz) * batch_sz]  # make divisible by batch size\n",
    "    return {\"input_ids\": seqs}\n",
    "\n",
    "\n",
    "prep_data = partial(split_and_tokenize, seq_len=seq_len, batch_sz=batch_sz)\n",
    "tokenized_dataset = dataset.map(\n",
    "    prep_data, batched=True, remove_columns=dataset[\"train\"].column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-20T16:50:24.911341Z",
     "iopub.status.busy": "2023-03-20T16:50:24.911156Z",
     "iopub.status.idle": "2023-03-20T16:56:51.483766Z",
     "shell.execute_reply": "2023-03-20T16:56:51.483017Z",
     "shell.execute_reply.started": "2023-03-20T16:50:24.911322Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "Compiling Model...\n",
      "Graph compilation: 100%|██████████| 100/100 [01:24<00:00]\n",
      "Compiled/Loaded model in 89.9241143650006 secs\n",
      "***** Running training *****\n",
      "  Num examples = 7840\n",
      "  Num Epochs = 42\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 320\n",
      "  Gradient Accumulation steps = 20\n",
      "  Total optimization steps = 1000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a5298094ebc407886b838ea52327b6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.5586, 'learning_rate': 0.0002, 'epoch': 0.83}\n",
      "{'loss': 2.804, 'learning_rate': 0.0004, 'epoch': 1.67}\n",
      "{'loss': 2.4454, 'learning_rate': 0.0006, 'epoch': 2.5}\n",
      "{'loss': 2.2854, 'learning_rate': 0.0008, 'epoch': 3.33}\n",
      "{'loss': 2.1626, 'learning_rate': 0.001, 'epoch': 4.17}\n",
      "{'loss': 2.0473, 'learning_rate': 0.0009777777777777777, 'epoch': 5.0}\n",
      "{'loss': 1.9735, 'learning_rate': 0.0009555555555555556, 'epoch': 5.83}\n",
      "{'loss': 1.8992, 'learning_rate': 0.0009333333333333333, 'epoch': 6.67}\n",
      "{'loss': 1.8237, 'learning_rate': 0.0009111111111111111, 'epoch': 7.5}\n",
      "{'loss': 1.8095, 'learning_rate': 0.0008888888888888888, 'epoch': 8.33}\n",
      "{'loss': 1.7489, 'learning_rate': 0.0008666666666666667, 'epoch': 9.17}\n",
      "{'loss': 1.7393, 'learning_rate': 0.0008444444444444444, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compiling Model...\n",
      "\n",
      "Graph compilation:   0%|          | 0/100 [00:00<?]\u001b[A\n",
      "Graph compilation:   3%|▎         | 3/100 [00:00<00:15]\u001b[A\n",
      "Graph compilation:   4%|▍         | 4/100 [00:00<00:19]\u001b[A\n",
      "Graph compilation:   7%|▋         | 7/100 [00:01<00:23]\u001b[A\n",
      "Graph compilation:  17%|█▋        | 17/100 [00:01<00:06]\u001b[A\n",
      "Graph compilation:  21%|██        | 21/100 [00:02<00:06]\u001b[A\n",
      "Graph compilation:  24%|██▍       | 24/100 [00:02<00:05]\u001b[A\n",
      "Graph compilation:  27%|██▋       | 27/100 [00:04<00:17]\u001b[A\n",
      "Graph compilation:  31%|███       | 31/100 [00:04<00:11]\u001b[A\n",
      "Graph compilation:  33%|███▎      | 33/100 [00:05<00:13]\u001b[A\n",
      "Graph compilation:  35%|███▌      | 35/100 [00:07<00:23]\u001b[A\n",
      "Graph compilation:  41%|████      | 41/100 [00:08<00:15]\u001b[A\n",
      "Graph compilation:  45%|████▌     | 45/100 [00:08<00:10]\u001b[A\n",
      "Graph compilation:  49%|████▉     | 49/100 [00:08<00:08]\u001b[A\n",
      "Graph compilation:  51%|█████     | 51/100 [00:08<00:07]\u001b[A\n",
      "Graph compilation:  53%|█████▎    | 53/100 [00:09<00:08]\u001b[A\n",
      "Graph compilation:  55%|█████▌    | 55/100 [00:10<00:10]\u001b[A\n",
      "Graph compilation:  57%|█████▋    | 57/100 [00:10<00:10]\u001b[A\n",
      "Graph compilation:  61%|██████    | 61/100 [00:11<00:08]\u001b[A\n",
      "Graph compilation:  65%|██████▌   | 65/100 [00:12<00:09]\u001b[A\n",
      "Graph compilation:  75%|███████▌  | 75/100 [00:13<00:03]\u001b[A\n",
      "Graph compilation:  76%|███████▌  | 76/100 [00:13<00:03]\u001b[A\n",
      "Graph compilation:  82%|████████▏ | 82/100 [00:14<00:02]\u001b[A\n",
      "Graph compilation:  83%|████████▎ | 83/100 [00:14<00:02]\u001b[A\n",
      "Graph compilation:  85%|████████▌ | 85/100 [00:15<00:02]\u001b[A\n",
      "Graph compilation:  86%|████████▌ | 86/100 [00:15<00:03]\u001b[A\n",
      "Graph compilation:  88%|████████▊ | 88/100 [00:17<00:05]\u001b[A\n",
      "Graph compilation:  89%|████████▉ | 89/100 [00:18<00:04]\u001b[A\n",
      "Graph compilation:  94%|█████████▍| 94/100 [00:18<00:01]\u001b[A\n",
      "Graph compilation:  96%|█████████▌| 96/100 [00:19<00:00]\u001b[A\n",
      "Graph compilation: 100%|██████████| 100/100 [00:21<00:00][A\n",
      "Compiled/Loaded model in 24.455767086000378 secs\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 432\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66eadf7e4f5943d8ba04e0e6121927c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7431640625, 'eval_runtime': 0.5885, 'eval_samples_per_second': 734.109, 'eval_steps_per_second': 45.882, 'epoch': 10.42}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Graph compilation:   0%|          | 0/100 [00:00<?]\u001b[A\n",
      "Graph compilation: 100%|██████████| 100/100 [00:01<00:00]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7048, 'learning_rate': 0.0008222222222222222, 'epoch': 10.83}\n",
      "{'loss': 1.6518, 'learning_rate': 0.0008, 'epoch': 11.67}\n",
      "{'loss': 1.6482, 'learning_rate': 0.0007777777777777778, 'epoch': 12.5}\n",
      "{'loss': 1.6165, 'learning_rate': 0.0007555555555555555, 'epoch': 13.33}\n",
      "{'loss': 1.6036, 'learning_rate': 0.0007333333333333333, 'epoch': 14.17}\n",
      "{'loss': 1.5772, 'learning_rate': 0.0007111111111111111, 'epoch': 15.0}\n",
      "{'loss': 1.5662, 'learning_rate': 0.000688888888888889, 'epoch': 15.83}\n",
      "{'loss': 1.5594, 'learning_rate': 0.0006666666666666666, 'epoch': 16.67}\n",
      "{'loss': 1.521, 'learning_rate': 0.0006444444444444444, 'epoch': 17.5}\n",
      "{'loss': 1.5226, 'learning_rate': 0.0006222222222222223, 'epoch': 18.33}\n",
      "{'loss': 1.5203, 'learning_rate': 0.0006, 'epoch': 19.17}\n",
      "{'loss': 1.5033, 'learning_rate': 0.0005777777777777778, 'epoch': 20.0}\n",
      "{'loss': 1.491, 'learning_rate': 0.0005555555555555556, 'epoch': 20.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compiling Model...\n",
      "\n",
      "Graph compilation:   0%|          | 0/100 [00:00<?]\u001b[A\n",
      "Graph compilation: 100%|██████████| 100/100 [00:00<00:00]\u001b[A\n",
      "Compiled/Loaded model in 3.15173802299978 secs\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 432\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe12be781d454b78b6eac841386abee2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to out/checkpoint-500\n",
      "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1428: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Configuration saved in out/checkpoint-500/ipu_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5712890625, 'eval_runtime': 0.4452, 'eval_samples_per_second': 970.327, 'eval_steps_per_second': 60.645, 'epoch': 20.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Graph compilation:   0%|          | 0/100 [00:00<?]\u001b[A\n",
      "Graph compilation:   3%|▎         | 3/100 [00:19<10:23]\u001b[A\n",
      "Graph compilation:   4%|▍         | 4/100 [00:23<09:04]\u001b[A\n",
      "Graph compilation:   7%|▋         | 7/100 [00:26<04:38]\u001b[A\n",
      "Graph compilation:  13%|█▎        | 13/100 [00:26<01:39]\u001b[A\n",
      "Graph compilation:  16%|█▌        | 16/100 [00:27<01:11]\u001b[A\n",
      "Graph compilation:  21%|██        | 21/100 [00:27<00:42]\u001b[A\n",
      "Graph compilation:  23%|██▎       | 23/100 [00:30<00:54]\u001b[A\n",
      "Graph compilation:  25%|██▌       | 25/100 [00:43<02:27]\u001b[A\n",
      "Graph compilation:  26%|██▌       | 26/100 [00:43<02:08]\u001b[A\n",
      "Graph compilation:  27%|██▋       | 27/100 [00:44<01:49]\u001b[A\n",
      "Graph compilation:  28%|██▊       | 28/100 [00:44<01:36]\u001b[A\n",
      "Graph compilation:  29%|██▉       | 29/100 [00:44<01:17]\u001b[A\n",
      "Graph compilation:  30%|███       | 30/100 [00:45<01:12]\u001b[A\n",
      "Graph compilation:  31%|███       | 31/100 [00:46<00:58]\u001b[A\n",
      "Graph compilation:  32%|███▏      | 32/100 [00:46<00:44]\u001b[A\n",
      "Graph compilation:  33%|███▎      | 33/100 [00:46<00:33]\u001b[A\n",
      "Graph compilation:  35%|███▌      | 35/100 [00:48<00:49]\u001b[A\n",
      "Graph compilation:  36%|███▌      | 36/100 [00:48<00:43]\u001b[A\n",
      "Graph compilation:  37%|███▋      | 37/100 [00:49<00:36]\u001b[A\n",
      "Graph compilation:  41%|████      | 41/100 [00:51<00:36]\u001b[A\n",
      "Graph compilation:  45%|████▌     | 45/100 [00:51<00:19]\u001b[A\n",
      "Graph compilation:  49%|████▉     | 49/100 [00:52<00:14]\u001b[A\n",
      "Graph compilation:  50%|█████     | 50/100 [00:53<00:16]\u001b[A\n",
      "Graph compilation:  52%|█████▏    | 52/100 [00:54<00:17]\u001b[A\n",
      "Graph compilation:  55%|█████▌    | 55/100 [00:56<00:20]\u001b[A\n",
      "Graph compilation:  57%|█████▋    | 57/100 [00:56<00:18]\u001b[A\n",
      "Graph compilation:  61%|██████    | 61/100 [00:57<00:14]\u001b[A\n",
      "Graph compilation:  65%|██████▌   | 65/100 [01:00<00:17]\u001b[A\n",
      "Graph compilation:  67%|██████▋   | 67/100 [01:00<00:13]\u001b[A\n",
      "Graph compilation:  73%|███████▎  | 73/100 [01:01<00:06]\u001b[A\n",
      "Graph compilation:  75%|███████▌  | 75/100 [01:02<00:08]\u001b[A\n",
      "Graph compilation:  76%|███████▌  | 76/100 [01:03<00:08]\u001b[A\n",
      "Graph compilation:  79%|███████▉  | 79/100 [01:03<00:05]\u001b[A\n",
      "Graph compilation:  82%|████████▏ | 82/100 [01:03<00:03]\u001b[A\n",
      "Graph compilation:  83%|████████▎ | 83/100 [01:04<00:05]\u001b[A\n",
      "Graph compilation:  85%|████████▌ | 85/100 [01:06<00:05]\u001b[A\n",
      "Graph compilation:  86%|████████▌ | 86/100 [01:07<00:08]\u001b[A\n",
      "Graph compilation:  87%|████████▋ | 87/100 [01:08<00:07]\u001b[A\n",
      "Graph compilation:  88%|████████▊ | 88/100 [01:12<00:15]\u001b[A\n",
      "Graph compilation:  89%|████████▉ | 89/100 [01:12<00:12]\u001b[A\n",
      "Graph compilation:  91%|█████████ | 91/100 [01:12<00:06]\u001b[A\n",
      "Graph compilation:  93%|█████████▎| 93/100 [01:13<00:03]\u001b[A\n",
      "Graph compilation:  94%|█████████▍| 94/100 [01:15<00:05]\u001b[A\n",
      "Graph compilation:  96%|█████████▌| 96/100 [01:16<00:02]\u001b[A\n",
      "Graph compilation:  97%|█████████▋| 97/100 [01:21<00:04]\u001b[A\n",
      "Graph compilation: 100%|██████████| 100/100 [01:21<00:00][A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4705, 'learning_rate': 0.0005333333333333334, 'epoch': 21.67}\n",
      "{'loss': 1.4702, 'learning_rate': 0.0005111111111111111, 'epoch': 22.5}\n",
      "{'loss': 1.4462, 'learning_rate': 0.0004888888888888889, 'epoch': 23.33}\n",
      "{'loss': 1.4544, 'learning_rate': 0.00046666666666666666, 'epoch': 24.17}\n",
      "{'loss': 1.4625, 'learning_rate': 0.0004444444444444444, 'epoch': 25.0}\n",
      "{'loss': 1.4357, 'learning_rate': 0.0004222222222222222, 'epoch': 25.83}\n",
      "{'loss': 1.4208, 'learning_rate': 0.0004, 'epoch': 26.67}\n",
      "{'loss': 1.4187, 'learning_rate': 0.00037777777777777777, 'epoch': 27.5}\n",
      "{'loss': 1.4232, 'learning_rate': 0.00035555555555555557, 'epoch': 28.33}\n",
      "{'loss': 1.385, 'learning_rate': 0.0003333333333333333, 'epoch': 29.17}\n",
      "{'loss': 1.4109, 'learning_rate': 0.0003111111111111111, 'epoch': 30.0}\n",
      "{'loss': 1.4041, 'learning_rate': 0.0002888888888888889, 'epoch': 30.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compiling Model...\n",
      "\n",
      "Graph compilation:   0%|          | 0/100 [00:00<?]\u001b[A\n",
      "Graph compilation:   3%|▎         | 3/100 [00:00<00:15]\u001b[A\n",
      "Graph compilation:   4%|▍         | 4/100 [00:00<00:19]\u001b[A\n",
      "Graph compilation:   7%|▋         | 7/100 [00:01<00:22]\u001b[A\n",
      "Graph compilation:  17%|█▋        | 17/100 [00:01<00:06]\u001b[A\n",
      "Graph compilation:  21%|██        | 21/100 [00:02<00:05]\u001b[A\n",
      "Graph compilation:  24%|██▍       | 24/100 [00:02<00:05]\u001b[A\n",
      "Graph compilation:  27%|██▋       | 27/100 [00:04<00:17]\u001b[A\n",
      "Graph compilation:  31%|███       | 31/100 [00:04<00:12]\u001b[A\n",
      "Graph compilation:  33%|███▎      | 33/100 [00:05<00:13]\u001b[A\n",
      "Graph compilation:  35%|███▌      | 35/100 [00:07<00:24]\u001b[A\n",
      "Graph compilation:  41%|████      | 41/100 [00:08<00:16]\u001b[A\n",
      "Graph compilation:  45%|████▌     | 45/100 [00:08<00:10]\u001b[A\n",
      "Graph compilation:  49%|████▉     | 49/100 [00:08<00:08]\u001b[A\n",
      "Graph compilation:  51%|█████     | 51/100 [00:09<00:07]\u001b[A\n",
      "Graph compilation:  53%|█████▎    | 53/100 [00:09<00:08]\u001b[A\n",
      "Graph compilation:  55%|█████▌    | 55/100 [00:10<00:10]\u001b[A\n",
      "Graph compilation:  57%|█████▋    | 57/100 [00:10<00:10]\u001b[A\n",
      "Graph compilation:  61%|██████    | 61/100 [00:11<00:08]\u001b[A\n",
      "Graph compilation:  65%|██████▌   | 65/100 [00:13<00:09]\u001b[A\n",
      "Graph compilation:  75%|███████▌  | 75/100 [00:13<00:03]\u001b[A\n",
      "Graph compilation:  76%|███████▌  | 76/100 [00:13<00:03]\u001b[A\n",
      "Graph compilation:  82%|████████▏ | 82/100 [00:14<00:02]\u001b[A\n",
      "Graph compilation:  83%|████████▎ | 83/100 [00:14<00:02]\u001b[A\n",
      "Graph compilation:  85%|████████▌ | 85/100 [00:15<00:02]\u001b[A\n",
      "Graph compilation:  86%|████████▌ | 86/100 [00:15<00:02]\u001b[A\n",
      "Graph compilation:  88%|████████▊ | 88/100 [00:18<00:05]\u001b[A\n",
      "Graph compilation:  89%|████████▉ | 89/100 [00:18<00:04]\u001b[A\n",
      "Graph compilation:  94%|█████████▍| 94/100 [00:19<00:01]\u001b[A\n",
      "Graph compilation:  96%|█████████▌| 96/100 [00:19<00:00]\u001b[A\n",
      "Graph compilation: 100%|██████████| 100/100 [00:21<00:00][A\n",
      "Compiled/Loaded model in 24.625312311000016 secs\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 432\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26c37e3cd60f4e9894c9601315e749b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.525390625, 'eval_runtime': 0.5841, 'eval_samples_per_second': 739.651, 'eval_steps_per_second': 46.228, 'epoch': 31.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Graph compilation:   0%|          | 0/100 [00:00<?]\u001b[A\n",
      "Graph compilation: 100%|██████████| 100/100 [00:01<00:00]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3877, 'learning_rate': 0.0002666666666666667, 'epoch': 31.67}\n",
      "{'loss': 1.379, 'learning_rate': 0.00024444444444444443, 'epoch': 32.5}\n",
      "{'loss': 1.3865, 'learning_rate': 0.0002222222222222222, 'epoch': 33.33}\n",
      "{'loss': 1.3761, 'learning_rate': 0.0002, 'epoch': 34.17}\n",
      "{'loss': 1.3676, 'learning_rate': 0.00017777777777777779, 'epoch': 35.0}\n",
      "{'loss': 1.3646, 'learning_rate': 0.00015555555555555556, 'epoch': 35.83}\n",
      "{'loss': 1.3451, 'learning_rate': 0.00013333333333333334, 'epoch': 36.67}\n",
      "{'loss': 1.3484, 'learning_rate': 0.0001111111111111111, 'epoch': 37.5}\n",
      "{'loss': 1.3531, 'learning_rate': 8.888888888888889e-05, 'epoch': 38.33}\n",
      "{'loss': 1.3556, 'learning_rate': 6.666666666666667e-05, 'epoch': 39.17}\n",
      "{'loss': 1.3431, 'learning_rate': 4.4444444444444447e-05, 'epoch': 40.0}\n",
      "{'loss': 1.3423, 'learning_rate': 2.2222222222222223e-05, 'epoch': 40.83}\n",
      "{'loss': 1.3501, 'learning_rate': 0.0, 'epoch': 41.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compiling Model...\n",
      "\n",
      "Graph compilation:   0%|          | 0/100 [00:00<?]\u001b[A\n",
      "Graph compilation: 100%|██████████| 100/100 [00:00<00:00]\u001b[A\n",
      "Compiled/Loaded model in 3.2015101830002095 secs\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 432\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84325a7850074f37896c9d08e6574b1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to out/checkpoint-1000\n",
      "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1428: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Configuration saved in out/checkpoint-1000/ipu_config.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Saving model checkpoint to trained_model/\n",
      "Configuration saved in trained_model/ipu_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5146484375, 'eval_runtime': 0.44, 'eval_samples_per_second': 981.813, 'eval_steps_per_second': 61.363, 'epoch': 41.67}\n",
      "{'train_runtime': 296.0934, 'train_samples_per_second': 1080.74, 'train_steps_per_second': 3.377, 'train_loss': 1.653693359375, 'epoch': 41.67}\n"
     ]
    }
   ],
   "source": [
    "train_args = IPUTrainingArguments(\n",
    "    output_dir=\"out\",\n",
    "    per_device_train_batch_size=batch_sz,\n",
    "    per_device_eval_batch_size=batch_sz,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=250,\n",
    "    logging_steps=20,\n",
    "    max_steps=1000,\n",
    "    weight_decay=0.1,\n",
    "    warmup_steps=100,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    learning_rate=1e-3,\n",
    "    loss_scaling=2**6,\n",
    ")\n",
    "\n",
    "model = NanoGPTModel(config)\n",
    "\n",
    "trainer = IPUTrainer(\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    "    model=model,\n",
    "    args=train_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    ipu_config=ipu_config,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(\"trained_model/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we use loss scaling here, though this model is sufficiently small that it can get away without loss scaling. This certainly doesn't hold for larger models though.\n",
    "\n",
    "To really get a sense of how the model's doing, we ought to get it to write some Shakespeare. Here's two attempts at *The Tempest*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-20T16:56:51.485138Z",
     "iopub.status.busy": "2023-03-20T16:56:51.484940Z",
     "iopub.status.idle": "2023-03-20T16:57:19.549434Z",
     "shell.execute_reply": "2023-03-20T16:57:19.548780Z",
     "shell.execute_reply.started": "2023-03-20T16:56:51.485119Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IPUConfig {\n",
      "  \"auto_loss_scaling\": false,\n",
      "  \"device_iterations\": 1,\n",
      "  \"embedding_serialization_factor\": 1,\n",
      "  \"enable_half_partials\": true,\n",
      "  \"executable_cache_dir\": \"./exe_cache\",\n",
      "  \"execute_encoder_on_cpu_for_generation\": false,\n",
      "  \"gradient_accumulation_steps\": 20,\n",
      "  \"inference_device_iterations\": 1,\n",
      "  \"inference_replication_factor\": 1,\n",
      "  \"ipus_per_replica\": 1,\n",
      "  \"layers_per_ipu\": [\n",
      "    6\n",
      "  ],\n",
      "  \"matmul_proportion\": 0.2,\n",
      "  \"optimizer_state_offchip\": true,\n",
      "  \"optimum_version\": \"1.6.1\",\n",
      "  \"output_mode\": \"final\",\n",
      "  \"recompute_checkpoint_every_layer\": false,\n",
      "  \"replicated_tensor_sharding\": false,\n",
      "  \"replication_factor\": 1,\n",
      "  \"seed\": null,\n",
      "  \"transformers_version\": \"4.25.1\"\n",
      "}\n",
      "\n",
      "Graph compilation: 100%|██████████| 100/100 [00:23<00:00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Completion [1] =====\n",
      "\n",
      "PROSPERO:\n",
      "Our revels now are ended. These our actors,\n",
      "As I foretold you, were all spirits of his life;\n",
      "And then to you are to him so little so.\n",
      "\n",
      "LEONTES:\n",
      "What is the body in my brother, and then?\n",
      "\n",
      "LEONTES:\n",
      "I have been like a substance and great them?\n",
      "\n",
      "POMPEY:\n",
      "No, my lord, I thank your friends the news?\n",
      "\n",
      "POMPEY:\n",
      "The worship is for a more so leave, and the first,\n",
      "Which should I have sworn with my service\n",
      "Than the land his son of yourself.\n",
      "\n",
      "KING RICHARD III:\n",
      "As I am devil to the ground of my liege,\n",
      "And when I \n",
      "\n",
      "===== Completion [2] =====\n",
      "\n",
      "PROSPERO:\n",
      "Our revels now are ended. These our actors,\n",
      "As I foretold you, were all spirits of your part,\n",
      "To the people of your state and so much as live,\n",
      "And she the bloody and strike the other state\n",
      "To be the friends of the house of York,\n",
      "And so a king, and like to the bear the fire\n",
      "And many shall be as the bloody of the state\n",
      "To be a subjects of a bawd and life\n",
      "That would be with for the root of the country's face,\n",
      "And then the world is of the senators.\n",
      "\n",
      "PAULINA:\n",
      "I thank you,\n",
      "I would have slain the ground \n"
     ]
    }
   ],
   "source": [
    "pipelines.check_model_type = lambda self, supported_models: ...\n",
    "\n",
    "final_model = NanoGPTModel.from_pretrained(\"trained_model/\")\n",
    "\n",
    "TEST_INPUT = \"\"\"PROSPERO:\n",
    "Our revels now are ended. These our actors,\n",
    "As I foretold you, were all spirits\"\"\"\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    ipu_config=ipu_config.to_dict(),  # TODO: feature request -> no to_dict()\n",
    "    model=final_model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=512,\n",
    "    do_sample=True,\n",
    ")\n",
    "\n",
    "outputs = pipe(TEST_INPUT, num_return_sequences=2, temperature=0.4)\n",
    "for i, output in enumerate(outputs):\n",
    "    print(f\"\\n===== Completion [{i+1}] =====\\n\")\n",
    "    print(output[\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's not going to win any literary awards, but pretty good for a few minutes of training. Now let's see if our unit-scaled model can do the same..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unit scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All we need to do is swap in our new model, remove the loss scaling (of course!), and increase the learning rate (as our weights are now larger due to unit-initialisation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-20T16:57:19.550523Z",
     "iopub.status.busy": "2023-03-20T16:57:19.550340Z",
     "iopub.status.idle": "2023-03-20T16:57:19.555105Z",
     "shell.execute_reply": "2023-03-20T16:57:19.554563Z",
     "shell.execute_reply.started": "2023-03-20T16:57:19.550503Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: hide?\n",
    "from types import MethodType\n",
    "\n",
    "class _IPUConfig(IPUConfig):\n",
    "    def to_options(self, *args, **kwargs):\n",
    "        options = super().to_options(*args, **kwargs)\n",
    "        options._popart.setPatterns(dict(AutogradProxyOpPattern=True))\n",
    "        return options\n",
    "\n",
    "    def for_pod_type(self, *args, **kwargs):\n",
    "        config = super().for_pod_type(*args, **kwargs)\n",
    "        config.__class__ = _IPUConfig\n",
    "        return config\n",
    "\n",
    "ipu_config.__class__ = _IPUConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-20T16:57:19.555923Z",
     "iopub.status.busy": "2023-03-20T16:57:19.555720Z",
     "iopub.status.idle": "2023-03-20T16:57:19.977214Z",
     "shell.execute_reply": "2023-03-20T16:57:19.976357Z",
     "shell.execute_reply.started": "2023-03-20T16:57:19.555903Z"
    }
   },
   "outputs": [],
   "source": [
    "model = UnitScaledNanoGPTModel(config)\n",
    "\n",
    "train_args.loss_scaling = 1.0\n",
    "train_args.learning_rate = 2e-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🤞 The moment of truth ... let's train our unit-scaled model 🤞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-20T16:57:19.978280Z",
     "iopub.status.busy": "2023-03-20T16:57:19.978081Z",
     "iopub.status.idle": "2023-03-20T17:03:44.172293Z",
     "shell.execute_reply": "2023-03-20T17:03:44.171628Z",
     "shell.execute_reply.started": "2023-03-20T16:57:19.978259Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "Compiling Model...\n",
      "Graph compilation: 100%|██████████| 100/100 [01:55<00:00]\n",
      "Compiled/Loaded model in 127.81229283099947 secs\n",
      "***** Running training *****\n",
      "  Num examples = 7840\n",
      "  Num Epochs = 42\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 320\n",
      "  Gradient Accumulation steps = 20\n",
      "  Total optimization steps = 1000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2dd6c54c82f44aaa43c5cf878a29abb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.7835, 'learning_rate': 0.004, 'epoch': 0.83}\n",
      "{'loss': 2.8256, 'learning_rate': 0.008, 'epoch': 1.67}\n",
      "{'loss': 2.4451, 'learning_rate': 0.012, 'epoch': 2.5}\n",
      "{'loss': 2.2794, 'learning_rate': 0.016, 'epoch': 3.33}\n",
      "{'loss': 2.15, 'learning_rate': 0.02, 'epoch': 4.17}\n",
      "{'loss': 2.056, 'learning_rate': 0.019555555555555555, 'epoch': 5.0}\n",
      "{'loss': 1.9731, 'learning_rate': 0.019111111111111113, 'epoch': 5.83}\n",
      "{'loss': 1.9058, 'learning_rate': 0.018666666666666668, 'epoch': 6.67}\n",
      "{'loss': 1.8505, 'learning_rate': 0.018222222222222223, 'epoch': 7.5}\n",
      "{'loss': 1.7972, 'learning_rate': 0.017777777777777778, 'epoch': 8.33}\n",
      "{'loss': 1.7667, 'learning_rate': 0.017333333333333336, 'epoch': 9.17}\n",
      "{'loss': 1.7384, 'learning_rate': 0.01688888888888889, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compiling Model...\n",
      "\n",
      "Graph compilation:   0%|          | 0/100 [00:00<?]\u001b[A\n",
      "Graph compilation:   3%|▎         | 3/100 [00:04<02:20]\u001b[A\n",
      "Graph compilation:   4%|▍         | 4/100 [00:04<01:45]\u001b[A\n",
      "Graph compilation:   7%|▋         | 7/100 [00:05<00:58]\u001b[A\n",
      "Graph compilation:  22%|██▏       | 22/100 [00:05<00:09]\u001b[A\n",
      "Graph compilation:  28%|██▊       | 28/100 [00:08<00:15]\u001b[A\n",
      "Graph compilation:  32%|███▏      | 32/100 [00:09<00:18]\u001b[A\n",
      "Graph compilation:  35%|███▌      | 35/100 [00:12<00:23]\u001b[A\n",
      "Graph compilation:  37%|███▋      | 37/100 [00:14<00:29]\u001b[A\n",
      "Graph compilation:  39%|███▉      | 39/100 [00:14<00:25]\u001b[A\n",
      "Graph compilation:  41%|████      | 41/100 [00:15<00:28]\u001b[A\n",
      "Graph compilation:  45%|████▌     | 45/100 [00:16<00:17]\u001b[A\n",
      "Graph compilation:  49%|████▉     | 49/100 [00:16<00:12]\u001b[A\n",
      "Graph compilation:  51%|█████     | 51/100 [00:16<00:10]\u001b[A\n",
      "Graph compilation:  53%|█████▎    | 53/100 [00:17<00:11]\u001b[A\n",
      "Graph compilation:  55%|█████▌    | 55/100 [00:18<00:13]\u001b[A\n",
      "Graph compilation:  57%|█████▋    | 57/100 [00:18<00:12]\u001b[A\n",
      "Graph compilation:  61%|██████    | 61/100 [00:19<00:08]\u001b[A\n",
      "Graph compilation:  65%|██████▌   | 65/100 [00:20<00:09]\u001b[A\n",
      "Graph compilation:  73%|███████▎  | 73/100 [00:21<00:03]\u001b[A\n",
      "Graph compilation:  75%|███████▌  | 75/100 [00:21<00:03]\u001b[A\n",
      "Graph compilation:  77%|███████▋  | 77/100 [00:21<00:03]\u001b[A\n",
      "Graph compilation:  82%|████████▏ | 82/100 [00:22<00:02]\u001b[A\n",
      "Graph compilation:  84%|████████▍ | 84/100 [00:22<00:02]\u001b[A\n",
      "Graph compilation:  86%|████████▌ | 86/100 [00:24<00:03]\u001b[A\n",
      "Graph compilation:  88%|████████▊ | 88/100 [00:26<00:06]\u001b[A\n",
      "Graph compilation:  89%|████████▉ | 89/100 [00:27<00:05]\u001b[A\n",
      "Graph compilation:  94%|█████████▍| 94/100 [00:28<00:02]\u001b[A\n",
      "Graph compilation:  96%|█████████▌| 96/100 [00:28<00:01]\u001b[A\n",
      "Graph compilation: 100%|██████████| 100/100 [00:30<00:00][A\n",
      "Compiled/Loaded model in 40.916471270999864 secs\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 432\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adb5e6e4905b4b119164f78587a80d80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7333984375, 'eval_runtime': 0.8964, 'eval_samples_per_second': 481.929, 'eval_steps_per_second': 30.121, 'epoch': 10.42}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Graph compilation:   0%|          | 0/100 [00:00<?]\u001b[A\n",
      "Graph compilation: 100%|██████████| 100/100 [00:02<00:00]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7074, 'learning_rate': 0.016444444444444446, 'epoch': 10.83}\n",
      "{'loss': 1.6881, 'learning_rate': 0.016, 'epoch': 11.67}\n",
      "{'loss': 1.6322, 'learning_rate': 0.015555555555555557, 'epoch': 12.5}\n",
      "{'loss': 1.6246, 'learning_rate': 0.015111111111111112, 'epoch': 13.33}\n",
      "{'loss': 1.6069, 'learning_rate': 0.014666666666666666, 'epoch': 14.17}\n",
      "{'loss': 1.5772, 'learning_rate': 0.014222222222222223, 'epoch': 15.0}\n",
      "{'loss': 1.5472, 'learning_rate': 0.013777777777777778, 'epoch': 15.83}\n",
      "{'loss': 1.5531, 'learning_rate': 0.013333333333333332, 'epoch': 16.67}\n",
      "{'loss': 1.5266, 'learning_rate': 0.01288888888888889, 'epoch': 17.5}\n",
      "{'loss': 1.5199, 'learning_rate': 0.012444444444444445, 'epoch': 18.33}\n",
      "{'loss': 1.5043, 'learning_rate': 0.012, 'epoch': 19.17}\n",
      "{'loss': 1.4862, 'learning_rate': 0.011555555555555555, 'epoch': 20.0}\n",
      "{'loss': 1.4942, 'learning_rate': 0.011111111111111112, 'epoch': 20.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compiling Model...\n",
      "\n",
      "Graph compilation:   0%|          | 0/100 [00:00<?]\u001b[A\n",
      "Graph compilation: 100%|██████████| 100/100 [00:01<00:00]\u001b[A\n",
      "Compiled/Loaded model in 10.19603246600036 secs\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 432\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2892d51d50e4e968f8cdae022aad0fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to out/checkpoint-500\n",
      "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1428: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Configuration saved in out/checkpoint-500/ipu_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5400390625, 'eval_runtime': 0.6393, 'eval_samples_per_second': 675.789, 'eval_steps_per_second': 42.237, 'epoch': 20.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Graph compilation:   0%|          | 0/100 [00:00<?]\u001b[A\n",
      "Graph compilation: 100%|██████████| 100/100 [00:02<00:00]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4525, 'learning_rate': 0.010666666666666666, 'epoch': 21.67}\n",
      "{'loss': 1.455, 'learning_rate': 0.010222222222222221, 'epoch': 22.5}\n",
      "{'loss': 1.4438, 'learning_rate': 0.009777777777777778, 'epoch': 23.33}\n",
      "{'loss': 1.4358, 'learning_rate': 0.009333333333333334, 'epoch': 24.17}\n",
      "{'loss': 1.4157, 'learning_rate': 0.008888888888888889, 'epoch': 25.0}\n",
      "{'loss': 1.4046, 'learning_rate': 0.008444444444444445, 'epoch': 25.83}\n",
      "{'loss': 1.3781, 'learning_rate': 0.008, 'epoch': 26.67}\n",
      "{'loss': 1.3857, 'learning_rate': 0.007555555555555556, 'epoch': 27.5}\n",
      "{'loss': 1.3919, 'learning_rate': 0.0071111111111111115, 'epoch': 28.33}\n",
      "{'loss': 1.3537, 'learning_rate': 0.006666666666666666, 'epoch': 29.17}\n",
      "{'loss': 1.3511, 'learning_rate': 0.006222222222222223, 'epoch': 30.0}\n",
      "{'loss': 1.3359, 'learning_rate': 0.0057777777777777775, 'epoch': 30.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compiling Model...\n",
      "\n",
      "Graph compilation:   0%|          | 0/100 [00:00<?]\u001b[A\n",
      "Graph compilation: 100%|██████████| 100/100 [00:01<00:00]\u001b[A\n",
      "Compiled/Loaded model in 10.092855619999682 secs\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 432\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab834c234a2548ca8bc67f1ac4b8874a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.4873046875, 'eval_runtime': 0.6397, 'eval_samples_per_second': 675.286, 'eval_steps_per_second': 42.205, 'epoch': 31.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Graph compilation:   0%|          | 0/100 [00:00<?]\u001b[A\n",
      "Graph compilation: 100%|██████████| 100/100 [00:02<00:00]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3398, 'learning_rate': 0.005333333333333333, 'epoch': 31.67}\n",
      "{'loss': 1.3367, 'learning_rate': 0.004888888888888889, 'epoch': 32.5}\n",
      "{'loss': 1.327, 'learning_rate': 0.0044444444444444444, 'epoch': 33.33}\n",
      "{'loss': 1.322, 'learning_rate': 0.004, 'epoch': 34.17}\n",
      "{'loss': 1.2932, 'learning_rate': 0.0035555555555555557, 'epoch': 35.0}\n",
      "{'loss': 1.2955, 'learning_rate': 0.0031111111111111114, 'epoch': 35.83}\n",
      "{'loss': 1.2822, 'learning_rate': 0.0026666666666666666, 'epoch': 36.67}\n",
      "{'loss': 1.2897, 'learning_rate': 0.0022222222222222222, 'epoch': 37.5}\n",
      "{'loss': 1.2786, 'learning_rate': 0.0017777777777777779, 'epoch': 38.33}\n",
      "{'loss': 1.2674, 'learning_rate': 0.0013333333333333333, 'epoch': 39.17}\n",
      "{'loss': 1.2678, 'learning_rate': 0.0008888888888888889, 'epoch': 40.0}\n",
      "{'loss': 1.2452, 'learning_rate': 0.00044444444444444447, 'epoch': 40.83}\n",
      "{'loss': 1.2518, 'learning_rate': 0.0, 'epoch': 41.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compiling Model...\n",
      "\n",
      "Graph compilation:   0%|          | 0/100 [00:00<?]\u001b[A\n",
      "Graph compilation: 100%|██████████| 100/100 [00:01<00:00]\u001b[A\n",
      "Compiled/Loaded model in 10.114462663000268 secs\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 432\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7de902dbc4094344bf4c95048735c936",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to out/checkpoint-1000\n",
      "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1428: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Configuration saved in out/checkpoint-1000/ipu_config.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Saving model checkpoint to trained_model/\n",
      "Configuration saved in trained_model/ipu_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.466796875, 'eval_runtime': 0.6367, 'eval_samples_per_second': 678.511, 'eval_steps_per_second': 42.407, 'epoch': 41.67}\n",
      "{'train_runtime': 255.7952, 'train_samples_per_second': 1251.001, 'train_steps_per_second': 3.909, 'train_loss': 1.632796875, 'epoch': 41.67}\n"
     ]
    }
   ],
   "source": [
    "unit_scale_trainer = IPUTrainer(\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    "    model=model,\n",
    "    args=train_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    ipu_config=ipu_config,\n",
    ")\n",
    "\n",
    "unit_scale_trainer.train()\n",
    "unit_scale_trainer.save_model(\"trained_model/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get a similar evaluation loss as the regular model—a success! In fact after sweeping the learning rate, unit scaling appears to be slightly better here.\n",
    "\n",
    "Let's celebrate with some unit-scaled Shakespeare..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-20T17:03:44.173474Z",
     "iopub.status.busy": "2023-03-20T17:03:44.173288Z",
     "iopub.status.idle": "2023-03-20T17:04:33.736394Z",
     "shell.execute_reply": "2023-03-20T17:04:33.735641Z",
     "shell.execute_reply.started": "2023-03-20T17:03:44.173454Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IPUConfig {\n",
      "  \"auto_loss_scaling\": false,\n",
      "  \"device_iterations\": 1,\n",
      "  \"embedding_serialization_factor\": 1,\n",
      "  \"enable_half_partials\": true,\n",
      "  \"executable_cache_dir\": \"./exe_cache\",\n",
      "  \"execute_encoder_on_cpu_for_generation\": false,\n",
      "  \"gradient_accumulation_steps\": 20,\n",
      "  \"inference_device_iterations\": 1,\n",
      "  \"inference_replication_factor\": 1,\n",
      "  \"ipus_per_replica\": 1,\n",
      "  \"layers_per_ipu\": [\n",
      "    6\n",
      "  ],\n",
      "  \"matmul_proportion\": 0.2,\n",
      "  \"optimizer_state_offchip\": true,\n",
      "  \"optimum_version\": \"1.6.1\",\n",
      "  \"output_mode\": \"final\",\n",
      "  \"recompute_checkpoint_every_layer\": false,\n",
      "  \"replicated_tensor_sharding\": false,\n",
      "  \"replication_factor\": 1,\n",
      "  \"seed\": null,\n",
      "  \"transformers_version\": \"4.25.1\"\n",
      "}\n",
      "\n",
      "Graph compilation: 100%|██████████| 100/100 [00:37<00:00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Completion [1] =====\n",
      "\n",
      "PROSPERO:\n",
      "Our revels now are ended. These our actors,\n",
      "As I foretold you, were all spirits of the\n",
      "provost: the strength of your honours and strength\n",
      "To meet the world in the violence of your father's country's life\n",
      "Than the ground of the court-wings of all the death.\n",
      "\n",
      "KING HENRY VI:\n",
      "My lord, the last is the father for the court?\n",
      "\n",
      "KING RICHARD III:\n",
      "No more than I see thee to be a peace\n",
      "And more than my brother's love in the gods\n",
      "And the root of my counsel: therefore I mean not\n",
      "The sister of the charge of the \n",
      "\n",
      "===== Completion [2] =====\n",
      "\n",
      "PROSPERO:\n",
      "Our revels now are ended. These our actors,\n",
      "As I foretold you, were all spirits for the head\n",
      "To see him that live and dear at his country's head,\n",
      "The sea of his crown, sir, the last of the ground\n",
      "Of our highness and the sun of his presence\n",
      "Of the singer of the steed service of her sense,\n",
      "That she was not her of your grace was for his son.\n",
      "\n",
      "KING RICHARD III:\n",
      "What is the done? what thou art thou dost deserve?\n",
      "\n",
      "DUKE OF AUMERLE:\n",
      "Pray, the good man! what says he lightning how\n",
      "That he was before his des\n"
     ]
    }
   ],
   "source": [
    "pipelines.check_model_type = lambda self, supported_models: ...\n",
    "\n",
    "final_model = UnitScaledNanoGPTModel.from_pretrained(\"trained_model/\")\n",
    "\n",
    "\n",
    "TEST_INPUT = \"\"\"PROSPERO:\n",
    "Our revels now are ended. These our actors,\n",
    "As I foretold you, were all spirits\"\"\"\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    ipu_config=ipu_config.to_dict(),  # TODO: feature request -> no to_dict()\n",
    "    model=final_model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=512,\n",
    "    do_sample=True,\n",
    ")\n",
    "\n",
    "outputs = pipe(TEST_INPUT, num_return_sequences=2, temperature=0.4)\n",
    "for i, output in enumerate(outputs):\n",
    "    print(f\"\\n===== Completion [{i+1}] =====\\n\")\n",
    "    print(output[\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some neat verse, from a neatly-scaled model. From which we can only conclude:\n",
    "\n",
    "*Enter: unit scaling*\n",
    "\n",
    "*Exeunt: loss scaling, automatic loss scaling, etc.*\n",
    "\n",
    "FIN\n",
    "\n",
    "---\n",
    "\n",
    "We hope that practitioners will consider using unit scaling for future projects, particularly those having difficulties with loss scaling or automatic mixed precision. With FP8 on the horizon, these issues are likely to become more prevalent. We hope unit scaling can help.\n",
    "\n",
    "If you're interested in using unit scaling yourself, or have questions, please do reach out 🙏☎️ We're keen to hear from anyone that has a problem unit scaling might help solve.\n",
    "\n",
    "The definitions provided here are the closest we have to an \"official\" PyTorch implementation, but if there's demand for a library tell us and we'll make one!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
